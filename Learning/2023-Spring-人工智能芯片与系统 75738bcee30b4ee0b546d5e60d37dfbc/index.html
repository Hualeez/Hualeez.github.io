<!DOCTYPE html>
<html lang="en">

<!-- Head tag (contains Google-Analytics、Baidu-Tongji)-->
<head>
  <!-- Google Analytics -->
  
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=UA-xxxxxx-xx"></script>
    <script type="text/javascript">
      window.dataLayer = window.dataLayer || [];

      function gtag() {
        dataLayer.push(arguments);
      }
      gtag('js', new Date());

      gtag('config', 'UA-xxxxxx-xx');
    </script>
  

  <!-- Baidu Tongji -->
  
    <script type="text/javascript">
      // Originial
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  <!-- Baidu Push -->
  
    <script>
      (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
          bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
          bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
      })();
    </script>
  

  <meta charset="utf-8"/>
  <meta http-equiv="X-UA-Compatible" content="IE=edge"/>

  <meta name="google-site-verification" content="lxDfCplOZbIzjhG34NuQBgu2gdyRlAtMB4utP5AgEBc"/>
  <meta name="baidu-site-verification" content="PpzM9WxOJU"/>

  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="description" content="这里是Hualingz，一个乐观主义者"/>
  <meta name="keyword" content="Hualingz,hualeez,hualingz,cyc"/>
  <link rel="shortcut icon" href="/img/avatar/fin_32.png"/>

  <!-- Place this tag in your head or just before your close body tag. -->
  <script async="async" defer="defer" src="https://buttons.github.io/buttons.js"></script>

  
    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css"/>

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/beantech.min.css"/>

    <!-- Pygments Highlight CSS -->
    <link rel="stylesheet" href="/css/highlight.css"/>
    <link rel="stylesheet" href="/css/widget.css"/>
    <link rel="stylesheet" href="/css/rocket.css"/>
    <link rel="stylesheet" href="/css/signature.css"/>
    <link rel="stylesheet" href="/css/catalog.css"/>
    <link rel="stylesheet" href="/css/livemylife.css"/>

    
      <!-- wave start -->
      <link rel="stylesheet" href="/css/wave.css"/>
      <!-- wave end -->
    

    
      <!-- top start (article top hot config) -->
      <link rel="stylesheet" href="/css/top.css"/>
      <!-- top end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/scroll.css"/>
      <!-- ThemeColor end -->
    

    
      <!-- viewer start (Picture preview) -->
      <link rel="stylesheet" href="/css/viewer.min.css"/>
      <!-- viewer end -->
    

    
      <!-- Search start -->
      <link rel="stylesheet" href="/css/search.css"/>
      <!-- Search end -->
    

    
      <!-- ThemeColor start -->
      <link rel="stylesheet" href="/css/themecolor.css"/>
      <!-- ThemeColor end -->
    

    

    
      <!-- gitalk start -->
      <!-- <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css"> -->
      <link rel="stylesheet" href="/css/gitalk.css"/>
      <!-- gitalk end -->
    
  

  <!-- Custom Fonts -->
  <!-- <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
  <!-- Hux change font-awesome CDN to qiniu -->
  <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.5.0/css/font-awesome.min.css" type="text/css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">

  <!-- Hux Delete, sad but pending in China <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'> <link
  href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/ css'> -->

  <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
  <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
  <!--[if lt IE 9]> <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script> <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script> <![endif]-->

  <!-- ga & ba script hoook -->
  <link rel="canonical" href="http://Hualingz.cn/Learning/2023-Spring-人工智能芯片与系统 75738bcee30b4ee0b546d5e60d37dfbc/">
  <title>
    
      AIChip - Hualingz_Channel
    
  </title>
<meta name="generator" content="Hexo 5.4.2"></head>


<!-- hack iOS CSS :active style -->

	<body ontouchstart="" class="body--light body--dark">


		<!-- ThemeColor -->
		
		<!-- ThemeColor -->
<style type="text/css">
  .body--light {
    --light-mode: none;
    --dark-mode: block;
  }
  .body--dark {
    --light-mode: block;
    --dark-mode: none;
  }
  i.mdui-icon.material-icons.light-mode {
    display: var(--light-mode);
  }
  i.mdui-icon.material-icons.dark-mode {
    display: var(--dark-mode);
  }
</style>
<div class="toggle" onclick="document.body.classList.toggle('body--dark')">
  <i class="mdui-icon material-icons light-mode"></i>
  <i class="mdui-icon material-icons dark-mode"></i>
</div>
<script>
  //getCookieValue
  function getCookieValue(a) {
    var b = document.cookie.match('(^|[^;]+)\\s*' + a + '\\s*=\\s*([^;]+)');
    return b
      ? b.pop()
      : '';
  }
  let themeMode = 'dark';
  if (getCookieValue('sb-color-mode') && (getCookieValue('sb-color-mode') !== themeMode)) {
    let dbody = document.body.classList;
    themeMode === 'dark' ? dbody.remove('body--dark') : dbody.add('body--dark');
  }

  //setCookieValue
  var toggleBtn = document.querySelector(".toggle");
  toggleBtn.addEventListener("click", function () {
    var e = document.body.classList.contains("body--dark");
    var cookieString = e
      ? "dark"
      : "light";
    var exp = new Date();
    exp.setTime(exp.getTime() + 3 * 24 * 60 * 60 * 1000); //3天过期
    document.cookie = "sb-color-mode=" + cookieString + ";expires=" + exp.toGMTString() + ";path=/";
  });
</script>

		

		<!-- Gitter -->
		
		<!-- Gitter -->
<!-- Docs:https://gitter.im/?utm_source=left-menu-logo -->
<script>
  ((window.gitter = {}).chat = {}).options = {
    room: 'your-community/your-room'
  };
</script>
<script src="https://sidecar.gitter.im/dist/sidecar.v1.js" async defer></script>

		

		<!-- Navigation (contains search)-->
		<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
  <div class="container-fluid">
    <!-- Brand and toggle get grouped for better mobile display -->
    <div class="navbar-header page-scroll">
      <button type="button" class="navbar-toggle">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Hualingz_Channel</a>
    </div>

    <!-- Collect the nav links, forms, and other content for toggling -->
    <!-- Known Issue, found by Hux: <nav>'s height woule be hold on by its content. so, when navbar scale out, the <nav> will cover tags. also mask any touch event of tags, unfortunately. -->
    <div id="huxblog_navbar">
      <div class="navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          <li>
            <a href="/">首页</a>
          </li>

          
          
          
          
          <li>
            <a href="/about/">
              
              关于
              
              
            </a>
          </li>
          
          
          
          
          
          <li>
            <a href="/categories/">
              
              分类
              
              
            </a>
          </li>
          
          
          
          <li>
            <a href="/tags/">
              
              标签
              
              
            </a>
          </li>
          
          

          
          <li>
            <a class="popup-trigger">
              <span class="search-icon"></span>搜索</a>
          </li>
          

          <!-- LangSelect -->
          
          
          
          
          
        </ul>
      </div>
    </div>
    <!-- /.navbar-collapse -->
  </div>
  <!-- /.container -->
</nav>
<!-- progress -->
<div id="progress">
  <div class="line" style="width: 0%;"></div>
</div>

<script>
  // Drop Bootstarp low-performance Navbar Use customize navbar with high-quality material design animation in high-perf jank-free CSS3 implementation
  var $body = document.body;
  var $toggle = document.querySelector('.navbar-toggle');
  var $navbar = document.querySelector('#huxblog_navbar');
  var $collapse = document.querySelector('.navbar-collapse');

  $toggle.addEventListener('click', handleMagic)

  function handleMagic(e) {
    if ($navbar.className.indexOf('in') > 0) {
      // CLOSE
      $navbar.className = " ";
      // wait until animation end.
      setTimeout(function() {
        // prevent frequently toggle
        if ($navbar.className.indexOf('in') < 0) {
          $collapse.style.height = "0px"
        }
      }, 400)
    } else {
      // OPEN
      $collapse.style.height = "auto"
      $navbar.className += " in";
    }
  }
</script>


		<!-- Post Header (contains intro-header、signature、wordcount、busuanzi、waveoverlay) -->
		<!-- Modified by Yu-Hsuan Yen -->
<!-- Post Header -->

  <style type="text/css">
    .body--light {
      /* intro-header */
      --intro-header-background-image-url-home: url('/img/header_img/newhome_bg.jpg');
      --intro-header-background-image-url-post: url('');
      --intro-header-background-image-url-page: url('/img/header_img/archive_bg2.jpg');
    }
    .body--dark {
      --intro-header-background-image-url-home: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/newhome_bg.jpg');
      --intro-header-background-image-url-post: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('');
      --intro-header-background-image-url-page: linear-gradient(rgba(0, 0, 0, 0.1), rgba(0, 0, 0, 0.2)), url('/img/header_img/archive_bg2.jpg');
    }

    header.intro-header {
       /*post*/
        background-image: var(--intro-header-background-image-url-post);
        /* background-image: url(''); */
      
    }

    
      #signature {/*signature*/
        background-image: url('/img/signature/vincent-white.png');
      }
    
  </style>





<header class="intro-header">
  <!-- Signature -->
  <div id="signature">
    <div class="container">
      <div class="row">
        <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
          
          <div class="post-heading">
            <div class="tags">
              
              <a class="tag" href="/tags/#浙江大学" title="浙江大学">浙江大学</a>
              
              <a class="tag" href="/tags/#AI chip" title="AI chip">AI chip</a>
              
              <a class="tag" href="/tags/#AI芯片系统" title="AI芯片系统">AI芯片系统</a>
              
            </div>
            <h1>AIChip</h1>
            <h2 class="subheading">Hualingz</h2>
            <span class="meta">
              Posted by Hualingz on
              2023-07-21
            </span>


            
            <!-- WordCount start -->
            <div class="blank_box"></div>
            <span class="meta">
              Estimated Reading Time <span class="post-count">96</span> Minutes
            </span>
            <div class="blank_box"></div>
            <span class="meta">
              Words <span class="post-count">24.6k</span> In Total
            </span>
            <div class="blank_box"></div>
            <!-- WordCount end -->
            
            
            <!-- 不蒜子统计 start -->
            <span class="meta" id="busuanzi_container_page_pv">
              Viewed <span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span> Times
            </span>
            <!-- 不蒜子统计 end -->
            


          </div>
          
        </div>
      </div>
    </div>
  </div>

  
  <!-- waveoverlay start -->
  <div class="preview-overlay">
    <svg class="preview-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto">
      <defs>
        <path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"></path>
      </defs>
      <g class="preview-parallax">
        <use xlink:href="#gentle-wave" x="48" y="0" fill=var(--gentle-wave1)></use>
        <use xlink:href="#gentle-wave" x="48" y="3" fill=var(--gentle-wave2)></use>
        <use xlink:href="#gentle-wave" x="48" y="5" fill=var(--gentle-wave3)></use>
        <use xlink:href="#gentle-wave" x="48" y="7" fill=var(--gentle-wave)></use>
      </g>
    </svg>
  </div>
  <!-- waveoverlay end -->
  

</header>



		<!-- Main Content (Post contains
	Pager、
	tip、
	socialshare、
	gitalk、gitment、disqus-comment、
	Catalog、
	Sidebar、
	Featured-Tags、
	Friends Blog、
	anchorjs、
	) -->
		<!-- Modify by Yu-Hsuan Yen -->
<!-- Post Content -->
<article>
  <div class="container">
    <div class="row">
      <!-- Post Container -->
      <div class="col-lg-8 col-lg-offset-1 col-md-10 col-md-offset-1 post-container">

        <h1 id="lecture-1-intro">Lecture 1 Intro</h1>
<ul>
<li>ISA架构：指令集架构</li>
<li>Microarchitecture：ISA的实现</li>
</ul>
<p>为了实现最好的效率和表现，我们必须take the expanded view，Co-design
across the hierarchy，在既定目标范围内尽可能定制化</p>
<h3 id="一些定理"><strong>一些定理</strong></h3>
<h3 id="amdhal-law"><strong>Amdhal Law</strong></h3>
<p>A formula which gives the theoretical speedup in latency of the
execution of a task at fixed workload that can be expected of a system
whose resources are improved.</p>
<ul>
<li>f：Parallelizable fraction of a
program，代码的<strong>并行部分比例</strong>，1-f是<strong>串行部分</strong></li>
<li>N：并行处理器的个数</li>
<li>speed up = <span class="math inline">\(\frac{1}{1-f+\frac{f}{N}}\)</span></li>
</ul>
<p>最大的加速限制是<span class="math inline">\({1-f}\)</span></p>
<p>Parallel portion (f) is usually not perfectly parallel</p>
<ol type="1">
<li>Synchronization overhead (e.g., updates to shared
data)有overhead，共享数据的更新</li>
<li>Load imbalance overhead (imperfect parallelization)</li>
<li>Resource sharing overhead (contention among N processors)</li>
</ol>
<h3 id="roofline-model"><strong>Roofline Model</strong></h3>
<p>Theoretical performance bound of your application running on your
machine.</p>
<ul>
<li>计算机制:延迟有限到吞吐量有限
<ul>
<li>原来的面向延迟的性能模型不起作用</li>
</ul></li>
<li>处理器方面的限制
<ul>
<li>在计算和内存方面显示固有的硬件限制(或限制)，内存带宽等硬件限制</li>
</ul></li>
<li>计算内核的角度
<ul>
<li>显示在给定处理器上运行的给定计算内核的优化优先级</li>
</ul></li>
</ul>
<p><strong>Arithmetic Intensity</strong></p>
<ul>
<li><p>算力：也称为计算平台的<strong>性能上限</strong>，指的是一个计算平台倾尽全力每秒钟所能完成的浮点运算数。单位是
<code>FLOPS</code> or <code>FLOP/s</code></p></li>
<li><p>带宽
：也即计算平台的<strong>带宽上限</strong>，指的是一个计算平台倾尽全力每秒所能完成的内存交换量。单位是<code>Byte/s</code></p></li>
<li><p>Arithmetic Intensity = Total Flops/Total Memory Bytes</p></li>
<li><p>AI越大,越好,希望是读数据一次后尽量多做计算</p></li>
<li><p>这个值越高，读每一个bytes需要的算力越多，越可能是compute
bound，低的话可能是内存bound</p>
<ul>
<li>Large AI ：Compute-bound</li>
<li>Small AI ： Memory-bound</li>
<li>Flops：每秒浮点运算</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p><strong>Attainable Flop/s = min( peak Flop/s, AI * peak GB/s
)</strong></p></li>
<li><p><strong>这个定理告诉我们，应用的表现要在一定的bound里面，才是正常的</strong></p></li>
<li><p>Peak Flop会根据计算平台变化.</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%201.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<p>Example：计算程序的AI</p>
<figure class="highlight inform7"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#pragma omp parallel for</span><br><span class="line">for(i=0;i&lt;N;i++)&#123;</span><br><span class="line">  Z<span class="comment">[i]</span> = X<span class="comment">[i]</span> + alpha*Y<span class="comment">[i]</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Type</strong>: double（8bytes）</li>
<li><strong>Memory</strong>: 24
Bytes/iteration，数组取2次，存入1次，共3次，每次8bytes</li>
<li><strong>Compute</strong>: 2 flops/iteration，浮点计算有两次</li>
<li><strong>Arithmetic Intensity</strong>: 0.083 flops/byte</li>
</ul>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#pragma omp parallel for</span></span><br><span class="line"><span class="keyword">for</span>(k=<span class="number">1</span>;k&lt;dim+<span class="number">1</span>;k++)&#123;</span><br><span class="line"><span class="keyword">for</span>(j=<span class="number">1</span>;j&lt;dim+<span class="number">1</span>;j++)&#123;</span><br><span class="line"><span class="keyword">for</span>(i=<span class="number">1</span>;i&lt;dim+<span class="number">1</span>;i++)&#123;</span><br><span class="line">  <span class="type">int</span> ijk = i + j*jStride + k*kStride;</span><br><span class="line">  <span class="built_in">new</span>[ijk] = <span class="number">-6.0</span>*<span class="built_in">old</span>[ijk        ]</span><br><span class="line">                + <span class="built_in">old</span>[ijk<span class="number">-1</span>      ]</span><br><span class="line">                + <span class="built_in">old</span>[ijk+<span class="number">1</span>      ]</span><br><span class="line">                + <span class="built_in">old</span>[ijk-jStride]</span><br><span class="line">                + <span class="built_in">old</span>[ijk+jStride]</span><br><span class="line">                + <span class="built_in">old</span>[ijk-kStride]</span><br><span class="line">                + <span class="built_in">old</span>[ijk+kStride];</span><br><span class="line">&#125;&#125;&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>Type</strong>: short（2bytes）</li>
<li><strong>Memory</strong>: 16 Bytes/iteration，数组存取共8次</li>
<li><strong>Compute</strong>: 7
flops/iteration，浮点计算7次，整型计算不能计入</li>
<li><strong>Arithmetic Intensity</strong>: 0.4375 flops/byte</li>
</ul>
<h3 id="littles-lawlwbuffer-size-throughputlatency">**Little‘s
Law：L=W(buffer size = throughput*latency)**</h3>
<p>A theorem by <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/John_Little_(academic)">John
Little</a> which states that the long-term average number <em>L</em> of
customers in a stationary system is equal to the long-term average
effective arrival rate <em>λ</em> multiplied by the average time
<em>W</em> that a customer spends in the system. 返回值需要时间。</p>
<ul>
<li>直观的例子：只有一个窗口，Service时间6min，顾客来的速度1person/min，我们需要多少个柜台给顾客
<ul>
<li>Answer：6 slots</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%202.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li>那么Latency就是上述中的Service
time，Throughput就是顾客来的速度，Buffersize就是我们需要的柜台数量
<ul>
<li><code>Buffer size = Latency * Throughput</code></li>
</ul></li>
</ul>
<h1 id="lecture-2-neuman-isa-cpu">Lecture 2 Neuman &amp; ISA &amp;
CPU</h1>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%203.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="von-neuman-model">Von Neuman Model</h2>
<p>冯诺依曼模型，是一个执行模式，为了建造一个计算机，我们需要一个执行模型这就是冯诺依曼模型。冯诺依曼结构有5个基本的组成部分。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%204.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Memory</li>
<li>Processing unit</li>
<li>Input</li>
<li>Output</li>
<li>Control unit</li>
</ul>
<h3 id="memory">Memory</h3>
<p>Memory中存储Program和Data</p>
<p>Memory contains
bits：bit就是一个位，8个位组成一个byte字节，bit也可以组成word字（8，16，32bits的都有）</p>
<p><strong>Address
space地址空间</strong>：内存中唯一可识别位置的总数</p>
<ul>
<li>在MIPS中，地址空间为32-bit的</li>
<li>在x86-64中，地址空间是48-bit的</li>
</ul>
<p><strong>Addressability寻址能力</strong>：每个位置(地址)存储多少位</p>
<ul>
<li>现在的cpu都是一个byte一个byte寻址的，即8-bit
addressable/byte-addressable
<ul>
<li>如这里的一个地址上是8个bit，那么这个就是byte-addressable的</li>
<li>并且其地址空间<code>Address space = 2^8</code></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%205.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li>也有的是一个word一个word寻址的，但是很少有一个bit一个bit寻址的</li>
</ul>
<h3 id="processing-unitpu">Processing Unit（PU）</h3>
<p>真正做计算的部分</p>
<p>PU由ALU和Regs组成：ALU执行计算，Regs寄存器组用来短时存储</p>
<ul>
<li>ALU：将各种算术和逻辑操作组合到一个单元中
<ul>
<li>一个周期只做一次运算</li>
<li>输入A、B以及Func信号，输出结果</li>
</ul></li>
<li>Regs：引入寄存器的初衷是因为Memory虽然大但是很慢，在CPU中我们需要更快的存储。
<ul>
<li>一般来说一个Reg存放一个word</li>
<li>MIPS由32个通用寄存器，<code>寄存器长度 = word length = 32bits</code></li>
<li>寄存器的存放值由ABI（Application Binary Interface）规定</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%206.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="input-and-output">Input and Output</h3>
<p>输入和输出使信息能够进出计算机。</p>
<ul>
<li>Input：Keyboard、Mouse、Scanner、Disks、Network…</li>
<li>Output：Monitor、Printer、Disks、Network…</li>
</ul>
<h3 id="control-unit">Control Unit</h3>
<p>直觉：控制单元就像管弦乐队的指挥</p>
<ul>
<li>执行(程序中的每条指令)的逐步过程。</li>
<li>通过包含指令的指令寄存器(IR)跟踪正在处理的指令。</li>
<li>通过程序计数器(PC)或指令指针(IP)(另一个包含要处理的(下一个)指令地址的寄存器)跟踪下一个要处理的指令。</li>
</ul>
<p><strong>冯诺依曼结构的最大特点：</strong></p>
<ol type="1">
<li>Stored Program</li>
<li>Sequential instruction processing</li>
</ol>
<h2 id="instruction-set-architectureisa">Instruction Set
Architecture(ISA)</h2>
<p>我们能够直接操作的东西，就是一个指令集</p>
<p>直接的理解instruction set：</p>
<ul>
<li>Instructions are words in the language of a computer</li>
<li>ISA是vocabulary</li>
</ul>
<p>ISA是软件命令和硬件执行之间的接口。ISA是介于Program和Micoarchitecture之间的位置。</p>
<p>ISA规定了三个组成部分</p>
<ul>
<li>Memory Organization
<ul>
<li><p>Address Space（MIPS：2^32bits）</p></li>
<li><p>Addressability（MIPS：8 bits）</p></li>
<li><p>Word- or Byte-addressable</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%207.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%208.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul></li>
<li>Register set
<ul>
<li>MIPS：32个regs</li>
</ul></li>
<li>Instruction set
<ul>
<li><p>Opcodes</p>
<ul>
<li>Opcode：指定了指令做什么</li>
<li>Operands：指定了指令的操作对象（操作数）</li>
</ul>
<p>opcode的类型，以MIPS为例子：</p>
<ul>
<li>Operate：如ALU的计算</li>
<li>Data movement：如Memory的Read和Write</li>
<li>Control：改变操作序列，如循环、判断跳转等</li>
</ul></li>
<li><p>Data types</p>
<p>早期RISC机器：只有计算整型</p>
<p>AIchip：能够计算tensor张量</p></li>
<li><p>Addressing modes</p>
<p>寻址模式是一种指定操作数位置的机制，在MIPS中有5种</p>
<ul>
<li>Immediate or literal：操作数Operand位于指令的某些位中</li>
<li>Register：操作数是一个寄存器</li>
<li>Three <strong>memory addressing</strong> modes
<ul>
<li>PC-relative：和PC相关的寻址模式</li>
<li>Pseudo-direct addressing：伪直接寻址</li>
<li>Base+offset：Base寄存器+offset</li>
</ul></li>
</ul>
<p>不同的寻址模式的优点：</p>
<p>简洁，上层map更容易，应用容易实现，Sparse matrix
accesses，支持更好地将高级编程构造映射到硬件，减少指令数量和代码大小。受益多种应用：基于指针的访问(间接)、稀疏矩阵访问、数组索引</p>
<p>不同的寻址模式的缺点：</p>
<p>在硬件层面实现困难，编译器和微架构的工作更复杂</p></li>
</ul></li>
</ul>
<h3 id="operate-instructions">Operate Instructions</h3>
<ul>
<li>以MIPS为例Add：<code>add a,b,c</code>
，a是目的操作数，bc是源操作数</li>
<li>R-type：add、and、xor</li>
<li>I-type：R-type的immediate版本</li>
<li>F-type：Float的操作</li>
</ul>
<h3 id="datamovement-instructions">DataMovement Instructions</h3>
<ul>
<li>Motivation：需要从Memory中获取操作数，操作数需要进入reg中</li>
<li>Load：从Memory到Regs</li>
<li>Store：从Regs到Memory</li>
</ul>
<h3 id="control-flow-instructions">Control Flow Instructions</h3>
<ul>
<li>control flow inst允许程序的执行顺序不是顺序的</li>
<li>Branch、Jump等指令</li>
</ul>
<p><strong>复杂指令集和简单指令集：</strong></p>
<ul>
<li>复杂指令集是一条指令做很多的工作，如计算FFT、matrix
multiplication，高效性
<ul>
<li>优势：编译器简单、Denser Encoding（smaller code
size），更好的内存利用率</li>
<li>劣势：Larger chunks of work，难以复用，更复杂的硬件</li>
</ul></li>
<li>简单指令集是一条指令做很少的工作，如我们以前写的RISCV指令集，灵活性</li>
</ul>
<p><strong>寄存器数量</strong></p>
<p>寄存器的数量影响了，编码寄存器的位数，寄存在fast storage的数据，reg
file的size、access time、power consumption</p>
<ul>
<li>寄存器数量多：
<ul>
<li><ul>
<li>使得更好的Register Allocation，更少的saves和restores</li>
</ul></li>
<li><ul>
<li>更大的指令size，即用来编码寄存器的位数要多</li>
</ul></li>
<li><ul>
<li>更大的register file size</li>
</ul></li>
</ul></li>
</ul>
<h2 id="instructionprocessingcycle">Instruction（Processing）Cycle</h2>
<p>指令周期是计算机取指令到执行完毕的时间，CPU周期/机器周期是把一条指令的执行过程进行划分程阶段，一个阶段的时间是一个CPU周期，时钟周期是一个阶段里面最小的操作的时间，是最小的单位。</p>
<h3 id="microarchitecture">Microarchitecture</h3>
<p>下层实现ISA的架构</p>
<p>五级流水线：IF、ID、EXE、MEM、WB</p>
<p><strong>指令周期</strong>：指令周期是一条指令被执行的一系列的步骤或阶段，<strong>一个指令周期就是把流水线的5个阶段走一遍的时间</strong>。</p>
<h3 id="single-cycle-cpu单周期cpu">Single-Cycle CPU：单周期CPU</h3>
<p>就是一条指令走5个阶段，执行完了再读下一条，这是<strong>串行的指令</strong></p>
<ul>
<li>CPI：Cycles per instruction is strictly 1</li>
</ul>
<p>单周期CPU即一个时钟周期完成一条指令</p>
<ul>
<li>AS到AS‘在1个cycle中
<ul>
<li>AS：Architecture State</li>
</ul></li>
</ul>
<p>每条指令需要一个时钟周期来执行，只使用组合逻辑来实现指令执行。</p>
<p>[C{CD~H5)X@72{@]YV{<a href="2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/CCDH5">{IVP.png</a>X72YVIVP.png)</p>
<h3 id="单周期的执行步骤">单周期的执行步骤</h3>
<ul>
<li>给出一个指令和 AS（Architectural State）</li>
<li>ISA抽象地指定新的 AS’ 应该是什么
<ul>
<li>它定义了一个抽象的有限状态机：根据现在的状态由逻辑实现下个状态</li>
</ul></li>
<li>Microarchitecture微架构实现了从AS到AS‘的转换
<ul>
<li>单周期：<span class="math inline">\(AS\rightarrow
AS&#39;\)</span>在一个周期内</li>
</ul></li>
</ul>
<p><strong>DataPath and Control Logic</strong></p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%209.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>单周期的微架构执行一条指令需要一个时钟周期，即<code>CPI=1</code></li>
<li>每条指令所花费的时间是由执行最慢指令所花费的时间决定的
<ul>
<li>尽管许多指令执行起来不需要那么长时间</li>
<li>因为单周期无法跳过一些阶段，因此必须走完所有指令都要走的部分</li>
</ul></li>
<li>微架构的时钟周期时间是由完成最慢指令所需的时间决定的
<ul>
<li>设计的关键路径由最慢指令的处理时间决定</li>
<li>和上面的一样</li>
</ul></li>
</ul>
<h3 id="multi-cycle-cpu多周期cpu">Multi-Cycle CPU：多周期CPU</h3>
<ul>
<li>多周期CPU，让每一条指令只做它需要的阶段，有的是MEM和WB不需要的就不走，其目标是让每条指令只占用它真正需要的周期。一个时钟周期完成一个阶段.但是可以降低时钟周期
<ul>
<li>Decrease clock cycle time</li>
<li>每条指令需要多少时钟周期就需要多少时钟周期</li>
</ul></li>
<li>AS→AS+MS1→AS+MS2 →……→AS’</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2010.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>局限性：
<ul>
<li>并行度太差，每个时刻只有一个阶段的资源在使用，利用率低</li>
</ul></li>
<li>优点：
<ul>
<li>关键路径设计：能否独立于任何指令的最坏情况处理时间而不断减小关键路径</li>
<li>Bread and
butter(常见情况)设计：可以优化执行重要任务所需的状态数占用大量执行时间的指令</li>
<li>平衡设计：不需要提供超出实际需要的能力或资源
<ul>
<li>一条多次需要资源X的指令不需要执行多个X</li>
<li>导致更高效的硬件：可以重复使用一条指令需要多次的硬件组件</li>
</ul></li>
</ul></li>
<li>Performance Analysis
<ul>
<li><code>单条指令执行时间 = CPI * clock cycle time</code></li>
<li><code>整个程序的执行时间 = 指令数 * 平均API * clock cycle time</code></li>
<li>单周期微架构：
<ul>
<li><code>CPI = 1</code></li>
<li>Clock cycle time = long</li>
</ul></li>
<li>多周期微架构：
<ul>
<li><code>CPI = different for each instruction</code>
<ul>
<li><code>平均CPI → hopefully small</code></li>
</ul></li>
<li><code>Clock cycle time = short</code></li>
</ul></li>
</ul></li>
</ul>
<h3 id="pipeline-cpu流水线cpu">Pipeline CPU：流水线CPU</h3>
<ul>
<li>Pipeline
CPU：更大的吞吐量！一条指令过了IF，下一条指令就可以进来了，要判断hazard的条件。</li>
<li>关键思想：当一条指令在其处理阶段使用了某些资源时，在该指令不需要的空闲资源上处理其他指令</li>
<li>目标：最大化吞吐率，用最小化的资源</li>
<li>核心思想：
<ul>
<li>指令处理周期划分为不同的阶段</li>
<li>保证资源都在工作</li>
<li>每一个阶段都在执行不同的指令</li>
</ul></li>
<li>理想中的Pipeline
<ul>
<li>identical operations：重复同样的一个操作</li>
<li>independent operations：指令之间不能有依赖性</li>
<li>Uniformly partitionable suboperations：分为均匀的suboperation</li>
<li>Speed up的比例是流水线的级数</li>
</ul></li>
<li>Pipeline处理指令
<ul>
<li>实际中Speed up会小，是因为阶段之间的分布不均匀</li>
<li>最完美的情况是每一个阶段都有指令执行</li>
<li>但是这些中间部件在ISA中是显示不出来的</li>
</ul></li>
<li>Pipeline的Control信号：
<ul>
<li>选择Ⅰ：只解析一次，并且不断向后传递，对于控制信息不多的情况比较好</li>
<li>选择Ⅱ：每个阶段都解析</li>
</ul></li>
</ul>
<h1 id="lecture-3-pipeline-hazard-record-buffer">Lecture 3 Pipeline
Hazard &amp; Record Buffer</h1>
<h2 id="pipeline-hazard">Pipeline Hazard</h2>
<p>Hazard：Hazard是阻止Pipeline中的指令执行其下一个计划Pipeline
stage的条件。</p>
<p>Structural Hazard：缺少硬件资源执行，竞争硬件资源</p>
<p>Data Hazard：分三种，对寄存器有依赖</p>
<p>Control Hazard：if、else等条件指令造成的影响</p>
<h2 id="structural-hazard">Structural Hazard</h2>
<ul>
<li>原因：当两个或多个指令试图在同一个周期中使用相同的硬件资源时发生。</li>
<li>后果：产生了一个stall，一条指令不能执行</li>
<li>解决方式：可以通过复制硬件资源来克服
<ul>
<li>Multiple accesses to the register file：需要两个读口一个写口</li>
<li>Multiple accesses to the memory</li>
<li>Fully pipeline the functional unit</li>
</ul></li>
<li>有关寄存器的Structural Hazard
<ul>
<li>如下图，我们的REG需要2个Read和1个Write来能保证避免Structural Hazard
<ul>
<li>因为ALU需要从REG中读2个值</li>
<li>硬件设计需要满足最坏的情况</li>
</ul></li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2011.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>有关内存的Structural Hazard
<ul>
<li>解决方案：将内存分为Instruction和data</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2012.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<h3 id="data-hazard">Data Hazard</h3>
<p>Data <strong>Dependences</strong></p>
<ul>
<li>Flow dependence：read after write—true data dependence</li>
<li>Output dependence：write after write</li>
<li>Anti dependence：write after read （？）</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2013.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>反依赖和输出依赖：它们更容易处理。只在最后一个阶段并按程序顺序写入目标</li>
</ul>
<p><strong>Flow Dependence/RAW：</strong></p>
<ul>
<li>硬件pipeline stall：侦测并且等待reg的值被处理</li>
<li>软件pipeline
stall：在软件层面检测并消除依赖，不需要硬件检测依赖</li>
<li>Data Forward/Bypass：检测和转发/绕过数据到相关指令</li>
</ul>
<p>一条指令写一个寄存器($s0)，下一条指令读这个寄存器=&gt;读后写(RAW)依赖。</p>
<ul>
<li>and、or都会产生错误的值</li>
<li>sub可以获得正确的值（先写后读的情况下）</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2014.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>“fix”流依赖的最简单方法是暂停pipeline。</p>
<ul>
<li>Pipeline stall，称为pipeline bubble或简称bubble。pipeline
stall会做：
<ul>
<li>先前的指示：继续在pipeline中进行。</li>
<li>下面的指令：在pipeline中被一个或多个时钟周期停止，直到等待寄存器准备就绪。</li>
<li>新指令：在stall期间未获取。</li>
</ul></li>
</ul>
<p>例子：假设<code>i</code>写了<code>ra</code>，<code>j</code>读了<code>ra</code></p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2015.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>如何在硬件中实现？</strong></p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2016.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>pipeline stalling：
<ul>
<li>禁用PC和IF/ID锁存，确保有stall时后续的指令会停在原来的阶段</li>
<li>将一个bubble推到下一阶段：bubble =
1并禁用控制信号Wreg和Wmem；向前push一个nop到ID/EX。</li>
</ul></li>
<li>Stalls are supported by
<ul>
<li>一个使能EN来控制Fetch和Decode的pipeline registers</li>
<li>以及同步重置/清除(CLR)输入到EXE
pipeline寄存器或与每个pipeline寄存器相关联的INV位，表示内容无效</li>
</ul></li>
</ul>
<p><strong>软件的解决方法：</strong></p>
<p>侦测冲突，并且使用软件增加足够的nop指令来使得reg准备就绪。或者可以移动指令位置，来避免冲突。</p>
<ul>
<li>在编译器级别重新排序/重新安排指令</li>
<li>插入nops的个数不是固定的，要根据上下文来确定</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2017.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>使用软件插入，更易于理解，硬件设计更加简单。</li>
</ul>
<p><strong>软件VS硬件</strong></p>
<ul>
<li>软件是静态调度，编译器必须把指令排序，硬件顺序执行他们</li>
<li>在实时的过程中，软件不能知道依赖关系的硬件状态，不知道每一个指令的latency，也不知道if
else的逻辑。</li>
<li>编译器不知道哪些信息使得静态调度变得困难？在运行时确定的任何东西比如可变长的操作延迟、内存地址、分支方向</li>
</ul>
<p><strong>DataForward</strong></p>
<p>一旦结果值可用，就将结果值转发给相关指令</p>
<p>有点像Dataflow的方式</p>
<ul>
<li>数据值提供给相关指令，只要data是available的</li>
<li>指令在其所有操作数available时执行</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2018.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>Forward可以有两种情况：</p>
<ul>
<li>将MEM阶段的值返回给EXE</li>
<li>将WB阶段的值返回给EXE</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2019.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>我们应该在什么时候从MEM阶段或WB阶段Forward到EXE？</p>
<ul>
<li><p>如果该阶段将写入目标寄存器，并且目标寄存器与EXE阶段的源寄存器匹配。</p></li>
<li><p>Forward优先级：</p>
<ul>
<li>例子：写s0，写s0，读s0，那么此时的EXE需要的是第二次s0的值，即MEM阶段的值</li>
<li>优先MEM，后WB</li>
</ul>
<p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span>  ((rsE != <span class="number">0</span>) AND (rsE == WriteRegM) AND RegWriteM) then </span><br><span class="line">  ForwardAE = <span class="number">10</span>  # forward from Memory stage</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> ((rsE != <span class="number">0</span>) AND (rsE == WriteRegW) AND RegWriteW) then  </span><br><span class="line">  ForwardAE = <span class="number">01</span>  # forward from Writeback stage</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">	ForwardAE = <span class="number">00</span>  # no forwarding</span><br></pre></td></tr></table></figure></p></li>
<li><p>Forward不是都可以行得通的</p>
<ul>
<li>由于pipeline设计和指令延迟</li>
<li>lw指令直到Memory阶段结束才完成读取数据，导致了其结果不能转发到下一条指令的Execute阶段</li>
</ul></li>
</ul>
<h3 id="control-hazard">Control Hazard</h3>
<p>if else这种分支判断的指令都有control hazard。取决于Instruction
Pointer / Program Counter</p>
<p>解决方案：分支预测</p>
<h2 id="reorder-buffer">Reorder Buffer</h2>
<h3 id="pipeline-cpuideal-vs-realistic">Pipeline CPU：Ideal vs
Realistic</h3>
<ul>
<li>Ideal pipeline CPU
<ul>
<li>一个pipeline</li>
<li>固定延迟</li>
<li>依赖关系在编译器时已知</li>
<li>不支持异常/中断</li>
</ul></li>
<li>Realistic pipeline CPU
<ul>
<li>具有不同延迟的多个pipeline</li>
<li>不可预测的延迟</li>
<li>编译时未知的依赖项</li>
<li>支持异常/中断</li>
</ul></li>
</ul>
<h3 id="for-multi-cycle-execution">For Multi-cycle Execution</h3>
<ul>
<li>多周期执行的问题：不同指令的执行的时间不同。</li>
<li>解决方案：对不同的执行周期的指令，用不同周期的执行单元执行。要有多个不同的功能单元，它们需要不同的循环次数
<ul>
<li>可以让独立的指令在之前的长延迟指令完成执行之前在不同的功能单元上开始执行</li>
<li>比如我们有两个pipeline：一个是4stage的，一个11stage的，分别对应了1轮8轮EXE的两种指令，那么执行时就应该是如下图所示的，ADD不需要等DIV执行完再执行（没有冲突时），可以直接使用另一pipeline执行</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2020.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>但是很明显，上面的执行步骤有很明显的问题：WriteBack的顺序会出现错误，<strong>因此这是我们的Reorder
Buffer要做的事情！</strong></li>
</ul></li>
</ul>
<h3 id="for-exception-and-interrupt">For Exception and Interrupt</h3>
<p>程序执行中的“Unplanned”更改或中断</p>
<ul>
<li>Exception：由于程序执行中的内部问题internal problems</li>
<li>Interrupts：由于需要由处理器处理的外部事件external problems that
need to be handled by processor</li>
</ul>
<p>Exception和interrupts的处理步骤：</p>
<ul>
<li>停止当前的程序</li>
<li>保存状态，context switch</li>
<li>handling the exception/interrupts，可以交付给handler进行
<ul>
<li>When to handle？
<ul>
<li>Exception：当检测到就要handle</li>
<li>Interrupts：when convenient（除非interrupt的优先级很高）</li>
</ul></li>
</ul></li>
<li>如果有返回，返回程序执行，context switch</li>
</ul>
<p><strong>Precise Exception/Interrupts</strong></p>
<p>当异常/中断准备好处理时，体系结构状态arch
state应该是一致的(精确的)，我们要记录保存下这个状态。</p>
<p><strong>精确异常：</strong>简单地说就是 eptr
的指向就是真正引起异常的指令之所在</p>
<ul>
<li>所有以前的指示都应该完全retire</li>
<li>以后的指令不应retire</li>
<li>retire=commit=完成指令并且更新arch state</li>
</ul>
<p><strong>当最后一条要retire的指令被检测到exception时：</strong></p>
<ul>
<li>Ensure arch state is precise</li>
<li>将所有之前在pipeline的instructions进行flush</li>
<li><strong>保存此时的PC和register</strong></li>
<li>重定向Fetch引擎到适当的异常处理例程</li>
</ul>
<p><strong>为什么要做Precise Exception？Goal？</strong></p>
<ol type="1">
<li>保持冯·诺依曼模型的语义</li>
<li>辅助软件调试</li>
<li>允许(轻松)从异常中恢复</li>
<li>使traps进入软件(例如，软件实现opcodes）</li>
</ol>
<p><strong>在单周期和多周期的CPU中如何做的呢？</strong></p>
<ul>
<li>single cycle
<ul>
<li>指令边界=周期边界</li>
</ul></li>
<li>multi cycle
<ul>
<li>在控制FSM中添加导致异常或中断处理程序的特殊状态</li>
<li>仅在获取下一条指令之前的精确状态切换到处理程序</li>
</ul></li>
</ul>
<h3 id="for-false-dependenceswawwar">For False
Dependences（WAW&amp;WAR）</h3>
<p>False Dependences就是WAW和WAR的情况的统称</p>
<h2 id="保留顺序语义">保留顺序语义</h2>
<p>说会上文，我们说了For Multi-cycle
Execution，使用多pipeline的方式会使得WB的顺序错误，我们需要让这个写的顺序正确。我们有一种选择就是让所有的指令都走最长的latency，但是那样子效率不高</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2021.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="reorder-bufferrob">Reorder Buffer（ROB）</h3>
<p>核心思想：<strong>Complete instructions out-of-order，but reorder
them before making result visible to arch state（Commit）</strong></p>
<ul>
<li>指令的Complete和Commit是不同的</li>
<li>当指令被Decoded，它会在Reorder Buffer中顺序保留一个自己的entry</li>
<li>当指令Complete了（执行单元出来），它会将结果写到Reorder
Buffer中对应的entry中</li>
<li>当ROB中最早进入的Instruction已经Complete了，并且没有exception，这条entry的对应值写回Register
files或者Memory【问：这条entry怎么了？清空？还是整体移动
答：环形的结构清空后一直继续顺序写，清空的不用管】</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2022.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="rob的entry">ROB的entry</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2023.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ol type="1">
<li>需要正确给指令重新排序到原始程序一样的顺序</li>
<li>如果指示可以retire没有任何问题，根据指令的结果更新arch state</li>
<li>精确处理异常/中断，如果异常/中断需要在指令retire之前处理</li>
<li>使用有效位来跟踪结果的就绪情况，并确定指令是否已完成执行</li>
</ol>
<p>那么使用ROB后，我们的写回阶段时间点就有了变化：</p>
<ul>
<li>指令complete时，结果写回首先写到ROB中</li>
<li>在commit时，ROB会把结果写回reg file/memory</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2024.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>那么问题来了：如果后续指令需要用到前面指令的值，而这个结果值还在ROB怎么办？</p>
<ul>
<li>One Option：stall</li>
<li>Better：从ROB中读取值</li>
</ul>
<p>一个寄存器的值能在Reg files中，reorder
buffer中，那么我们使用间接访问的方法，来简化ROB的访问</p>
<ul>
<li>首先访问Reg file，若reg
file中为invalid，那么寄存器会有对应的ROB的ID，我们去对应的entry获取这个寄存器值即可</li>
<li>访问ROB</li>
</ul>
<p>Reorder Buffer：For False Dependencies</p>
<ul>
<li>输出依赖和反依赖都不是真正的依赖，为什么？
<ul>
<li>同一个寄存器指的是彼此没有任何关系的值</li>
<li>它们的存在是因为ISA中缺少寄存器ID（name）</li>
<li>寄存器太少了</li>
</ul></li>
<li>RB消除了反依赖和输出依赖，给人一种有大量寄存器的错觉，为什么？
<ul>
<li>因为实际上我们的reg有限，但是ROB的entry很多，相当于reg和entry是同一个东西。只是被Rename了。
<ul>
<li>寄存器ID→ROB entry ID</li>
<li>架构寄存器ID→物理寄存器ID</li>
<li>重命名后，ROB条目ID用于引用寄存器</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2025.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<h2 id="in-order-pipeline-with-reorder-buffer">In-Order Pipeline with
Reorder Buffer</h2>
<p>引入了Reorder Buffer我们的Pipeline就可以如下执行：</p>
<ul>
<li>In-order dispatch/execution，out-of-order completion，in-order
retirement</li>
<li>Decode
(D)：访问regfile/ROB，在ROB中分配条目，检查指令是否可以执行，如果可以，则分派指令</li>
<li>Execution(E)：指令可以乱序完成</li>
<li>Completion(R)：将结果写入重排序缓冲区ROB</li>
<li>Retire/Commit(W)：检查例外情况；如果没有，将结果写入架构寄存器文件或内存。否则，flush
pipeline并且开始异常处理</li>
</ul>
<p>Advantages：</p>
<ul>
<li>概念上简单，支持精确的异常</li>
<li>可以消除false dependences</li>
</ul>
<p>Disadvantages：</p>
<ul>
<li>需要访问Reorder缓冲区以获得结果尚未写入寄存器文件</li>
<li>间接→增加延迟和复杂性</li>
</ul>
<h1 id="lecture-4-tomasulo-simd">Lecture 4 Tomasulo &amp; SIMD</h1>
<h2 id="tomasulo算法">Tomasulo算法</h2>
<p>有了这个算法指令可以乱序执行。旨在指令之间的并行。</p>
<h3 id="in-order-dispatch">In-order Dispatch</h3>
<p>正常来说，前面的指令stall了，后面的指令也会跟着stall掉</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_10.56.21.png" alt="截屏2023-03-23 10.56.21.png">
<figcaption aria-hidden="true">截屏2023-03-23 10.56.21.png</figcaption>
</figure>
<p>那么Tomasulo算法就是为了让这两条蓝色的指令不受STALL的影响</p>
<p>Solution：out-of-order dispatch</p>
<h2 id="reservation-station">Reservation Station</h2>
<p>Key idea：</p>
<ul>
<li>在<strong>ID和EXE阶段</strong>之间插入了RS，让有冲突的指令不要堵住后面的，让有冲突的指令在RS中等着。</li>
<li>Rest areas for dependent instructions：Reservation Station</li>
<li>本质就是一个指令buffer</li>
</ul>
<p>Tips：</p>
<ul>
<li>进入RS是要顺序的，出RS进入执行可以是乱序的</li>
<li>出ROB写会要顺序的，进入写ROB可以乱序的</li>
<li>两头顺序，中间乱序</li>
</ul>
<p>Function：</p>
<ul>
<li>监控在RS区域内的指令的source的值是否available</li>
<li>“fires”（dispatch）那些source值（一般最多是rs1和rs2）都已经available的指令</li>
<li>dispatch出来都是符合data-flow的顺序的</li>
</ul>
<h2 id="tomaulo算法实现">Tomaulo算法实现</h2>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_10.56.45.png" alt="截屏2023-03-23 10.56.45.png">
<figcaption aria-hidden="true">截屏2023-03-23 10.56.45.png</figcaption>
</figure>
<h3 id="enabling-ooo-execution">Enabling OoO Execution</h3>
<ol type="1">
<li><p>需要将value的“consumer”和“producer”连接起来</p>
<p>使用Register
renaming的方式：用tag将register相互连接，比如目的寄存器的两个源寄存器也被作为了目的寄存器时</p></li>
<li><p>Buffer所有的指令，即所有的指令都要进RS，让RS dispatch</p>
<p>使用RS和renaming的技术</p></li>
<li><p>指令要跟踪readiness of source</p>
<ul>
<li>当一个tag的值已经从EXE出来了，要broadcast，消除对应的依赖。</li>
<li>source可以是直接valid的，也可以是不valid的要依赖tag，此时若tag已经complete了，要更新依赖</li>
</ul></li>
<li><p>当所有的source都是valid，我们的指令可以分发执行</p></li>
</ol>
<h3 id="register-rename">Register Rename</h3>
<ul>
<li><p>Register Rename Table</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.02.23.png" alt="截屏2023-03-23 11.02.23.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.02.23.png</figcaption>
</figure>
<ul>
<li>Valid的话，该寄存器的值是value；否则为tag里面对应的值</li>
</ul></li>
<li><p>Reservation Station</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.04.01.png" alt="截屏2023-03-23 11.04.01.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.04.01.png</figcaption>
</figure>
<ul>
<li>实际上还有一个标记是否是否为空的标记，来执行清除和写入判断</li>
</ul></li>
</ul>
<h3 id="执行步骤">执行步骤</h3>
<p>RS在<strong>ID阶段</strong>check，check完后会自动选择一个两个都是Valid的指令执行，若有多个可执行的执行则会自动选择一个执行，因为执行单元对应只有一个。那么实际上执行完后就会WB，WB之后是要清除RS中内容的，否则会一直竞争。</p>
<p><strong>下面的例子没有考虑竞争和清除！！！</strong></p>
<ul>
<li><p>初始状态</p></li>
<li><p>Cycle 1</p>
<ul>
<li>第一条指令Fetch</li>
</ul></li>
<li><p>Cycle 2</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.13.05.png" alt="截屏2023-03-23 11.13.05.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.13.05.png</figcaption>
</figure>
<ul>
<li>乘法指令没有依赖一直跑到ID，但是ID阶段要去RS
<ul>
<li>step1：检查RS是否有空间：x</li>
<li>step2：访问reg table
<ul>
<li>R1是Valid的，Value为1</li>
<li>R2是Valid的，Value为2</li>
</ul></li>
<li>step3：将R1和R2的值写入RS</li>
<li>step4：重命名R3为x，告诉我们寄存器文件里的R3不是最新的，而是RS中的x</li>
</ul></li>
</ul></li>
<li><p>Cycle 3</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.22.08.png" alt="截屏2023-03-23 11.22.08.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.22.08.png</figcaption>
</figure>
<ul>
<li>第二条指令进入了ID阶段
<ul>
<li>step1：因为是Add指令，因此去Add的RS中找空间：a</li>
<li>step2：Add里有两个操作数，R3和R4
<ul>
<li>R3是Valid=0的，Tag=x</li>
<li>R4是Valid=1的，Value = 4</li>
<li>由于R3的Valid是0，因此要等Mul的结果R3</li>
</ul></li>
<li>step3：写入Add的RS中</li>
<li>step4：R5的tag是a，valid是0，寄存器中的R5不是最新值了</li>
</ul></li>
</ul></li>
<li><p>Cycle 4</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.21.27.png" alt="截屏2023-03-23 11.21.27.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.21.27.png</figcaption>
</figure>
<ul>
<li>第一条指令执行，第二条指令被堵住了，第三条指令进入了ID</li>
<li>第三条是Add，检查RS的空余，有b</li>
<li>R2和R6都是Valid的，把目标寄存器R7重命名为b</li>
<li>写入b的两个source（R2和R6），都是valid</li>
<li>意味着这条指令可以继续执行</li>
</ul></li>
<li><p>Cycle 5</p></li>
<li><p>Cycle 6</p></li>
<li><p>Cycle 7</p>
<ul>
<li>第六条指令进入ID阶段，是加法RS，空余d</li>
<li>R5此时已经是a了，但是我们还是要重命名为d</li>
<li>RS中d对应的两个source reg是老的R5对应的a和R11对应的y</li>
</ul></li>
<li><p>Cycle 8</p>
<ul>
<li>此时的第一条指令已经Complete了（EXE执行完）
<ul>
<li>RS的输出为tag=x和value=2</li>
<li>检查谁和x有依赖，全部替换为valid和value
<ul>
<li>这里在Reg Table里有依赖</li>
<li>Add RS中的a有依赖</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.30.33.png" alt="截屏2023-03-23 11.30.33.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.30.33.png</figcaption>
</figure></li>
<li>问：如何判断是否执行，这里的第二条指令两个source已经valid了</li>
</ul></li>
<li>此时的第三条指令也Complete了
<ul>
<li>输出为tag=b和value=8</li>
<li>检查b的依赖进行替换</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_11.33.33.png" alt="截屏2023-03-23 11.33.33.png">
<figcaption aria-hidden="true">截屏2023-03-23 11.33.33.png</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="question">Question</h3>
<ul>
<li>关键路径问题
<ul>
<li>最慢的阶段是：Tag Broadcast、value capture、instruction wake up</li>
</ul></li>
<li>如何减少这个关键路径的问题：
<ul>
<li>更新可以分为好几个周期，因为不更新也不会影响正确性，原来有依赖的指令不会提前开始运行</li>
</ul></li>
<li>Dataflow Graph
<ul>
<li><p>使用了Tomasulo算法后就有类似的图结构了</p></li>
<li><p>让指令之间能并行的都执行了</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-03-23_12.05.12.png" alt="截屏2023-03-23 12.05.12.png">
<figcaption aria-hidden="true">截屏2023-03-23 12.05.12.png</figcaption>
</figure></li>
</ul></li>
</ul>
<h2 id="out-of-order-execution-with-precise-exception">Out-of-Order
Execution with Precise Exception</h2>
<ul>
<li>Idea：使用ROB去reorder instructions在committing到AS之前</li>
<li>一条指令更新RAT（Register Table）当其complete了EXE
<ul>
<li>这些也称为Front Register File，不是有编制的，不影响架构的状态</li>
</ul></li>
<li>一条指令更新分离的Architectural Register File当其retire
<ul>
<li>Architectural Register就是我们常见的r0-r31 ，有编制的</li>
<li>Architectural Register总是按照program order来更新</li>
</ul></li>
</ul>
<h1 id="lecture-5-superscalarsimdmultithread">Lecture 5
Superscalar&amp;SIMD&amp;Multithread</h1>
<h3 id="recalltomasula-algorithm">Recall：Tomasula Algorithm</h3>
<ul>
<li>ID：若RS中有空的entry，那么我们要rename dst register
<ul>
<li>为这条指令占有一个RS Entry</li>
<li>对每一个RS Entry中的source register：若valid bit在Register
Table中为1，那么RS.source.v=1，RS.source.value=source
register；若为0，那么RS.source.v=0，RS.source.tag=source
register.tag</li>
<li>对于每一个Register Table中的dst register：Rename to the tag of the
corresponding RS entry。重命名为对应的tag</li>
</ul></li>
<li>RS：当在RS中时，每一条指令
<ul>
<li>更新：检查common data bus（CDB）看自己的source的tag，when tag
seen，grab value for the source and keep it in RS</li>
<li>Issue：当两个operands
available，那么这条指令就可以准备dispatch了</li>
</ul></li>
<li>EXE：在FU中执行指令，产生其boardcast tag和value</li>
<li>WB：需要竞争总线，因为可能有很多的EXE的输出
<ul>
<li>竞争总线，选择一个EXE输出的Boardcast Tag和Value</li>
<li>将广播标签及其广播值放入CDB（总线）</li>
<li>更新连接到CDB的寄存器文件Register File
Table，如果寄存器文件中的标记与广播标记匹配，则将广播值写入寄存器(并设置有效位)</li>
<li>更新连接到CDB的RS，如果广播标记与RS条目中任何源的标记匹配，则写入向源广播值并设置源的有效位</li>
</ul></li>
</ul>
<h2 id="superscalar">Superscalar</h2>
<p>超标量，本质还是在提升指令执行的并行度</p>
<ul>
<li>Idea：Fetch，decode，execute，retire <strong>multiple
instructions</strong> per cycle
<ul>
<li>N-wide superscalar可以N instructions per cycle</li>
</ul></li>
<li>Issues：
<ul>
<li>需要更多的硬件资源去做</li>
<li>指令之间的冲突关系更多了。因为同时有多条指令在执行会产生很多的冲突。</li>
</ul></li>
<li>Superscalar execution和out=of-order execution是正交的概念，互不影响
<ul>
<li>[in-order，out-of-order]×[scalar，superscalar]</li>
</ul></li>
</ul>
<p><strong>一个Example</strong></p>
<p>Idea：Multiple copies of data-path：Can fetch/decode/execute multiple
instructions per cycle</p>
<p>Issue：冲突更加复杂</p>
<p>比如我们要做一个可以处理两条指令同时执行的超标量流水线CPU，那么此时我们要从Instruction
Memory中拿出2个指令，Register
file的读口、写口也要翻倍，ALU的数量一定要翻倍（最重要的执行阶段），同样的WB阶段的读口和写口也要翻倍。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2026.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>上图的理想的IPC=2，但是实际中我们的superscalar有更多的dependence，难以达到这个IPC。</p>
<p><strong>有Dependence的Superscalar Performance</strong></p>
<p>由于依赖的存在有的指令不能在同一波执行，有的波次之间也需要stall</p>
<ul>
<li>Advantage
<ul>
<li>Higher instruction throughput</li>
<li>Higher IPC：instructions per cycle</li>
</ul></li>
<li>Disadvantage
<ul>
<li>Higher Complexity for dependence checking
<ul>
<li>需要check在一个pipeline stage中</li>
<li>在OoO的processor中寄存器的重命名更复杂</li>
<li>Potential lengthens critical path delay，clock cycle time</li>
</ul></li>
<li>硬件资源需求更多。</li>
</ul></li>
</ul>
<h2 id="vector-ins">Vector Ins</h2>
<p>SISD：单指令单数据，一个指令只对一个数据操作</p>
<p>SIMD(<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/55327037">https://zhuanlan.zhihu.com/p/55327037</a>)：单指令多数据，一个指令带着很多的数据，即可以批量对多个数据进行这个指令操作</p>
<ul>
<li>Array processor</li>
<li>Vector processor</li>
</ul>
<p>MISD：多指令单数据</p>
<p>MIMD：多指令对多数据执行</p>
<ul>
<li>multiprocessor</li>
<li>multithread processor</li>
</ul>
<h2 id="sisd">SISD</h2>
<p>在指令或数据流中没有并行的。</p>
<p>SISD的例子：传统的单处理器机器，例如我们信赖的RISC-V pipeline</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2027.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="simd">SIMD</h2>
<p>一条指令可以对多个数据进行操作，能够有更高的并行性</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2028.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>比如可以进行向量的加法。若用SISD进行100维向量的加法，需要100次，若SIMD一次可以操作100个数据，则只需要一次的加法操作</p>
<ul>
<li>Intuition of SIMD Capability
<ul>
<li>计算<span class="math inline">\(A[6:0]+B[6:0]\)</span>
<ul>
<li>scalar：一个cyle进行一次加法</li>
<li>SIMD：一个周期多个操作，比如4个操作</li>
</ul></li>
<li>Scalar：最少需要7个周期</li>
<li>SIMD：最少需要2个周期</li>
</ul></li>
<li>SIMD in Intel CPU：
<ul>
<li>256 bit AVX2：8个32-bit的float</li>
<li>512 bit AVX512：16个32-bit的float</li>
</ul></li>
</ul>
<p>Vector Processor Limitations</p>
<ul>
<li>内存（带宽）可能会是一个瓶颈</li>
</ul>
<h2 id="mimd">MIMD</h2>
<p>MIMD计算机利用多个处理器实现异步和独立的并行性。</p>
<p>在任何时候，不同的处理器都可能对不同的数据块执行不同的指令。</p>
<p>多用于Multiprocessor和Multithreaded processor</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2029.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="misd">MISD</h2>
<p>MISD计算机利用多个指令流对单个数据流。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2030.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="support-vector-insns">Support Vector Insns</h2>
<ul>
<li>Vecoter Register File：端口数量增加</li>
<li>V ALU：ALU数量增加</li>
<li>V Memory：端口数量增加</li>
</ul>
<h3 id="programmer-visible-architectural-states">Programmer Visible
Architectural States</h3>
<ul>
<li>Memory
<ul>
<li>Array of Storage locations indexed by an address</li>
<li>Memory Bank
Design：与原来的不同给一个地址出一个数不同，现在给一个地址，出来4个数（一般是连续的）</li>
</ul></li>
<li>Register</li>
</ul>
<h2 id="roofline-model-for-simd-cpu">Roofline Model for SIMD CPU</h2>
<p>本质上来说看的是理论的算力。<strong>加了SIMD，对算力是有提升的</strong>。</p>
<p>在Roofline Model中可以体现出来。Throughput会是普通pipeline的几倍</p>
<h2 id="fine-grained-multithreading">Fine-Grained Multithreading</h2>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/caishunzhe/p/12817245.html">https://www.cnblogs.com/caishunzhe/p/12817245.html</a></p>
<ul>
<li>FMT(fine-grained multithreading)又叫交叉多线程或指令交错多线程
<ul>
<li>每个时钟周期都进行线程的切换，多个线程交替执行，同一个周期只从一个线程发射指令到功能部件</li>
<li>理论上，FMT通过有效的调度可以完全隐藏存储延时，即在存储操作完成之前不从同一个线程取指</li>
</ul></li>
</ul>
<p>一个物理核一般支持两个逻辑核</p>
<ul>
<li>Idea：
<ul>
<li>硬件有多个线程的context（PC+Registers）。每一个cycle，取指令引擎从不同的线程去fetch</li>
<li>让ALU的利用率更高</li>
<li>每一个时钟周期都进行线程的切换，以确保没有两条指令来自同一个thread</li>
</ul></li>
</ul>
<p>为什么：因为Thread之间没有依赖，Thread内部有依赖，每轮时钟周期从不同的Thread中取指令可以避免dependence。</p>
<ul>
<li>Advantage
<ul>
<li>在线程中不需要处理控制和数据依赖的逻辑</li>
</ul></li>
<li>Disadvantage
<ul>
<li>单线程性能下降</li>
<li>用于保持线程上下文的额外逻辑</li>
<li>如果没有足够的线程覆盖整个管道，不会重叠延迟</li>
</ul></li>
<li>idea：
<ul>
<li>每个周期切换到另一个线程，这样就不会有来自线程的两条指令同时在pipeline中</li>
</ul></li>
<li>Advantage
<ul>
<li>通过与其他线程的有用工作重叠延迟来容忍控制和数据依赖延迟</li>
<li>通过利用多线程提高pipeline利用率</li>
<li>一个thread之内不好的并行，可以通过thread之间的并行，来提升并行度</li>
<li>提升了利用率</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2031.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="advantages">Advantages</h3>
<ul>
<li>不需要额外的Dependency检测</li>
<li>不需要branch预测逻辑</li>
<li>Otherwise-bubble周期用于从不同线程执行有用的指令</li>
<li>提升系统的吞吐率、延迟容忍、利用率</li>
</ul>
<h3 id="disadvantages">Disadvantages</h3>
<ul>
<li>更多的硬件复杂度：更多的context（PCs、Register Files）</li>
<li>减少了单线程的表现</li>
<li>缓存和内存中线程之间的资源争用</li>
<li>线程之间仍然保留一些依赖项检查逻辑(加载/存储)</li>
</ul>
<h2 id="multi-core">Multi-Core</h2>
<ul>
<li>idea：把多个核放在同一个模具die上。</li>
<li>Technology scaling 使得更多的晶体管可以放到同一个模具区域上。</li>
<li>还可以对专用于多处理器的芯片区域做什么?
<ul>
<li>拥有更大更强大的核心</li>
<li>内存层次结构中有更大的缓存</li>
<li>同步多线程</li>
<li>在芯片上集成平台组件(例如，网络接口，内存控制器)</li>
</ul></li>
</ul>
<h2 id="why-multi-core">Why Multi-Core</h2>
<ul>
<li>可选的替代方案：更大、更强大的单核
<ul>
<li>更大的超标量问题宽度、更大的指令窗口、更多的执行单元、更大的跟踪缓存、更大的分支预测器等</li>
<li>提高单线程性能，对程序员和编译器更透明</li>
<li>非常难以设计(可扩展的算法，以提高单线程性能难以捉摸)</li>
<li>耗电-许多乱序执行结构在扩展时消耗大量的功率/面积。</li>
<li>收益递减</li>
<li>对内存受限的应用程序性能没有显著帮助(可伸缩算法对此难以捉摸)。</li>
</ul></li>
<li>MultiCore的优势
<ul>
<li>更简单的核心，更节能，更低的复杂性，更容易设计和复制，更高的频率(更短的线，更小的结构)</li>
<li>在多程序工作负载上提高系统吞吐量，减少上下文切换</li>
<li>并行应用程序中更高的系统吞吐量</li>
</ul></li>
<li>MulitiCore的缺点
<ul>
<li>需要并行任务/线程来提高性能(并行编程)</li>
<li>资源共享会降低单线程性能</li>
<li>共享硬件资源需要管理</li>
<li>引脚数量限制数据供应增加的需求</li>
</ul></li>
</ul>
<h1 id="lecture-6-memory">Lecture 6 Memory</h1>
<p>理想的计算平台</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2032.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="memory-overview">Memory Overview</h2>
<p>我们的目标：带宽大、容量大、延迟低。一般而言三者不能兼得，而且要考虑cost</p>
<ul>
<li>容量大，导致，更长的寻址定位时间，即延迟高</li>
<li>延迟低，速度快，导致，cost高：SRAM、DRAM、SSD、Disk、Tape</li>
<li>带宽大，导致，cost高：更多的bank、port、channel，higher frequency or
faster technology</li>
</ul>
<p><strong>各种存储元件的比较</strong>：</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2033.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Flip-Flops
<ul>
<li>Very Fast，Parallel Access</li>
<li>Very Expensive，一个bit需要几十个晶体管</li>
</ul></li>
<li>SRAM
<ul>
<li>相对快，only one data word at a time</li>
<li>贵，一个bit需要6+个晶体管</li>
</ul></li>
<li>DRAM
<ul>
<li>Slower，one data word at a time，reading destroys
content（refresh），需要一些特定的进程，存在数据出错的情况</li>
<li>cheap，一个bit需要一个晶体管和一个电容器</li>
</ul></li>
<li>Flash Memory
<ul>
<li>Much Slower，access takes long time，很慢，non-volatile非易失性</li>
<li>Very cheap，一个晶体管可以存储16bit</li>
</ul></li>
</ul>
<h2 id="sram">SRAM</h2>
<p>Cerebras‘s Wafer Scale Engine（2019）</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2034.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>SRAM和DRAM都是内存，其访问方式如右图</p>
<ul>
<li>Goal：buffering data on chip，减少外部的访问</li>
<li>Advantage：随机访问有高性能</li>
<li>Disadvantage：low capacity（MBs）</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2035.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="memory-arrays">Memory Arrays</h3>
<p>输入N位地址，输出M位带宽的数据（一个word）</p>
<ul>
<li>Goal：高效地进行大数据的存取</li>
</ul>
<p>使用了Memory Arrays，取都是取一行one row出来的，即M位的数据</p>
<ul>
<li>一个m位值可以在每个唯一的n位地址上读或写</li>
<li>所有值都可以访问，但一次只能访问m位</li>
<li>访问限制允许更紧凑的组织</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2036.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>比如一个N地址位M数据位的Array</p>
<ul>
<li>有<span class="math inline">\(2^N\)</span>个行，和M个列</li>
<li>Depth：深度就是行数，即word的个数</li>
<li>Width：带宽就是一个word的位数</li>
<li>Array Size = Depth*Width</li>
</ul>
<h3 id="bit-line-word-line">Bit Line &amp; Word Line</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2037.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>SRAM和DRAM的内存阵列组织形式是一样的，只是bit的存储方式不同。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2038.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Where to Use SRAM？
<ul>
<li>Cache in CPU</li>
<li>Shared Memory in GPU</li>
<li>On-Chip buffer in AI accelerator</li>
</ul></li>
<li>How to Use SRAM
<ul>
<li>Multiple small separate SRAMs：低延迟和高吞吐率</li>
<li>Banked Design：wide access ports</li>
</ul></li>
</ul>
<h3 id="memory-banking">Memory Banking</h3>
<ul>
<li>Memory被分为了很多个bank，每个bank可以独立访问</li>
<li>每一个bank每个周期都可以访问一次</li>
<li>可以同时进行N次协同的访问，如果有N个bank</li>
</ul>
<p>即，把同一个地址输入到N个Banks中，每一个bank返回一个word/bit，最后进行选择，从而同一周期获得更多的数据，下面是一个bank，有row
buffer</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2039.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>Read Sequence</strong></p>
<ol type="1">
<li><p>Address decode：解码地址，获取word-lines</p></li>
<li><p>drive row select：选择读取一个Row</p></li>
<li><p>selected bit-cells drive
bit-lines，选择好的bits被送上bit-lines，一整行进行读取</p></li>
<li><p>Differential sensing and column select，选择column</p></li>
<li><p>pre-charge all bit-lines</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2040.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ol>
<h2 id="hbmdram">HBM&amp;DRAM</h2>
<ul>
<li>Motivation and Goals
<ul>
<li>应用视图Application Perspective</li>
<li>性能视图Performance Perspective</li>
<li>可靠性视图Reliability Perspective</li>
</ul></li>
</ul>
<h3 id="main-memory-system">Main Memory System</h3>
<p>main memory是计算系统中最重要的组成部分</p>
<p>main memory系统必须scale：size、technology、cost、efficiency</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2041.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>Application Perspective：</strong></p>
<p>目前都是内存存在瓶颈：Memory带宽、容量</p>
<ul>
<li>Important workloads</li>
<li>需要更快和更高效的大数据处理方式</li>
<li>Data is increasing</li>
</ul>
<p><strong>Performance Perspective：</strong></p>
<p>从性能角度出发，DRAM也是很重要的。Cache的访问速度慢，miss
rate高。</p>
<p>CPU的周期被bound在cache，在等内存回数据</p>
<p><strong>Energy Perspective：</strong></p>
<p>内存访问的功耗很大</p>
<p>增加计算，减少内存访问是很好的趋势。我们需要减少对内存的访问。</p>
<p>Memory对功耗的要求很严格，大部分的功耗都在data movement上。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2042.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>Reliability Perspective：</strong></p>
<p>容量越大，出错的概率更大。</p>
<h3 id="dram">DRAM</h3>
<p>我们的目标是大容量、bandwidth，没有latency</p>
<ul>
<li>Capacity：18年增长了128x</li>
<li>Bandwidth：20x</li>
<li>Latency：1.3x</li>
</ul>
<p>Dynamic Random Access Memory</p>
<ul>
<li>电容器充电状态表示存储值</li>
<li>电容器充电和放电表示了1和0</li>
<li>1个电容器和1个晶体管</li>
<li>DRAM cell loses charge over time，电荷会流失</li>
<li>DRAM cell needs to be refreshed，需要recharge</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2043.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="bank">Bank</h3>
<p>Goal：需要更大的内存阵列</p>
<p>Challenge：大内存意味着更慢的访问，我们能否在不降低访问速度的情况下进行大内存的访问？</p>
<p>Idea：将内存划分为更小的阵列，并将阵列连接到输入/输出总线</p>
<h3 id="architecture-of-dram">Architecture of DRAM</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2044.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li><p>有很多的Channel。</p>
<ul>
<li>Channel是并行的，但是DIMM是串行的，每次只能访问一个DIMM</li>
<li>Memory Channel是内存接口</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2045.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>一个DIMM有正反两面：每面都是一个RANK</p>
<ul>
<li>一个芯片的容量一般是固定的，芯片越多容量越大</li>
<li>两个RANK是串行的</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2046.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>每一个RANK上面有Chips</p>
<ul>
<li>比如8G有8个chip，实际会更多，有ECC校验的芯片</li>
<li>chip是并行的</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2047.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>每一个chip中有很多的Bank</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2048.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>Bank的拆解</p>
<ul>
<li>Row-Buffer，需要提高hit概率，从而提高带宽，因此DRAM要有顺序访问，提高hit</li>
<li>下图的一个col是1bit，不是1B?col大小是可以变的?</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2049.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<p>由于每个 rank 由许多 chip 组成，1 个 chip 仅负责部分的资料读取，chip
接收到位址讯号后，将位址丢入内部的 row/column 解码器找出相对应的 bank
位址（每家每款产品的内部 bank
组合有可能不同，因此对应也会有所不同），接著开启 row 线，同 1 排 row
的内部资料就会流到 row buffer 内部，row buffer 判断讯号为 0 或是 1
之后就输出资料。</p>
<h3 id="transferring-a-cache-block">Transferring a cache block</h3>
<p>假设一个cache block为64Bytes</p>
<p>每次从8个chip上表面读一个Byte，concat获得8B</p>
<p>一共需要8个周期</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-06_11.53.11.png" alt="截屏2023-04-06 11.53.11.png">
<figcaption aria-hidden="true">截屏2023-04-06 11.53.11.png</figcaption>
</figure>
<h3 id="address-bits-of-memory">Address Bits of Memory</h3>
<p><strong>SRAM</strong></p>
<ul>
<li>SRAM总是与计算单元存在于同一芯片中。</li>
<li>由于容量小，地址位数量相对较少。</li>
</ul>
<p><strong>DRAM</strong></p>
<p>DRAM的数据访问不是一个Address就足够，需要多个地址，要channel、DIMM、rank、chip等等的地址。</p>
<ul>
<li>DRAM与计算单元是分开的芯片，因此由于物理限制，pin码可能成为瓶颈。</li>
<li>由于内存容量大，直接映射的地址位数量多</li>
<li>而数据是暂存在Row Buffer的因此顺序访问的效率很高</li>
</ul>
<h2 id="ssd">SSD</h2>
<h2 id="disk">Disk</h2>
<h1 id="lecture-7-graphics-processing-units">Lecture 7 Graphics
Processing Units</h1>
<h2 id="gpu">GPU</h2>
<h3 id="keymessages">KeyMessages</h3>
<ul>
<li>编程模式是英伟达成功的关键，而不是GPU本身。</li>
<li>GPU的内存带宽和计算能力都比CPU高一个数量级。</li>
<li>只有当任务具有足够的计算强度时，才会将任务卸载给GPU。</li>
<li>AI任务需要计算密集型的加速器，例如GPU和AI处理器。</li>
</ul>
<p>GPU：本质是many-core的，与multi-core不同。multi-core是CPU，many-core是GPU个数更多。</p>
<p>核之间并行，核内也有很多的thread在跑（SMT）</p>
<ul>
<li>我们为什么用GPU？
<ul>
<li>更大的算力</li>
</ul></li>
</ul>
<h3 id="cpu-vs-gpu">CPU vs GPU</h3>
<ul>
<li>CPU
<ul>
<li>Few complex cores
<ul>
<li>控制复杂，能够进行更多的工作</li>
</ul></li>
<li>Larger cache for low memory latency</li>
<li>Large and slow memory</li>
</ul></li>
<li>GPU
<ul>
<li>Lots of simple cores
<ul>
<li>执行简单的、并行度高的计算，不擅长精巧的计算</li>
</ul></li>
<li>small cache for low memory latency</li>
<li>small and fast memory</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2050.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>CPU和GPU之间的关系
<ul>
<li>数据通过PCI Bus或者PCIe Bus进行传输</li>
<li>GPU是CPU的下游计算端，控制流都在CPU中，GPU干的是简单的活</li>
</ul></li>
</ul>
<p>More Cores，更多的核，带来了更多的问题：如何管理这些核？</p>
<h3 id="programming-model">Programming Model</h3>
<ul>
<li>CPU-GPU Co-Processing
<ul>
<li>CPU：串行的代码和不是很并行的代码</li>
<li>GPU：处理并行的代码</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2051.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>CPU会有驱动代码调用GPU。</p>
<h3 id="gpus-are-simd-engines-underneath">GPUs are SIMD Engines
Underneath</h3>
<ul>
<li>指令pipeline操作是SIMD类似的pipeline</li>
<li>但是Programming是使用threads完成的，不是使用SIMD指令</li>
<li>Programming model VS Execution model
<ul>
<li>我们不能让用户改代码，我们需要改进硬件的执行方式</li>
<li>编程模型和执行模型</li>
</ul></li>
</ul>
<p>Programming Model</p>
<ul>
<li>编程者表达code的方式
<ul>
<li>Sequential，Data
Parallel（SIMD），Dataflow，Multi-Thread（MIMD）</li>
</ul></li>
</ul>
<p>Hardware Execution Model</p>
<ul>
<li>硬件如何执行代码的方式
<ul>
<li>OoO Execution、Vector Processor，Array
Processor，Multiprocessor</li>
</ul></li>
<li>Discussion：执行模式可以和编程模式很不同
<ul>
<li>编程的代码可以在很多的硬件上面都能跑</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2052.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="nvidia-geforce-gtx-285">NVIDIA GeForce GTX 285</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2053.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2054.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2055.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Groups of 32 threads share instruction
stream，每一个group为一个warp</li>
<li>32个warp可以同时interleaved</li>
<li>1024个thread context能够存储在一个core上</li>
<li>8个SIMD计算单元每一个core上</li>
<li>SM：Streaming Multi-processor</li>
</ul>
<h2 id="programming-model-1">Programming Model</h2>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i=<span class="number">0</span>;i&lt;N;i++)&#123;</span><br><span class="line">	C[i]=B[i]+A[i]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="sisd-1">SISD</h3>
<ul>
<li>Pipeline Process</li>
<li>OoO execution Processor</li>
<li>Superscalar</li>
</ul>
<p>一条一条执行，load、load、add、store，一直串行</p>
<h3 id="simd-1">SIMD</h3>
<p>以SIMD为2为例子，我们每次可以load两个B[i]，两次迭代就变成了一次。</p>
<p>即一次load把B[0]、B[1]读出来了，再一次load把A[0]、A[1]读出。</p>
<p>然后add，把C=B+A，这里的ABC都是二维向量，再进行store</p>
<p>这样，总体而言我们的速度就快了2倍。</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Vectorized Code</span><br><span class="line">VLD</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="multithreadspmd">Multithread：SPMD</h3>
<p>Single Program Multiple Data</p>
<p>每次的迭代都有一个thread执行，thread1执行第一轮迭代，thread2执行第二轮，这些迭代是独立的，因此可以并行。每个核可以在同一时间内处理一个指令流，那么就可以进行并行</p>
<ul>
<li>SPMD的核心思想：多个指令流执行同一个程序</li>
<li>每个程序/过程
<ul>
<li>1)处理不同的数据</li>
<li>2)在运行时可以执行不同的控制流路径
<ul>
<li>许多科学应用程序都以这种方式编程，并在MIMD硬件(多处理器)上运行。</li>
<li>现代图形处理器以类似的方式在SIMD硬件上编程</li>
</ul></li>
</ul></li>
</ul>
<p>Hardware is Free to schedule.</p>
<p>不同的设备可以适应编程模型.比如一个编程模型的grid,里面有8个block,一个GPU有2个SM,那么4周期执行完,一个GPU有4个SM,则两周期执行完.</p>
<h2 id="gpu-programming-example">GPU Programming Example</h2>
<p>The device executes CUDA kernels</p>
<ul>
<li>Grid：里面有很多的Block，对应硬件的device</li>
<li>Block中有很多的Thread，这些thread共享block的shared
memory，独享regs，对应硬件层面的SM</li>
<li>Thread对应硬件层面的core</li>
<li>Thread Block
<ul>
<li>一个thread block回去一个SM中，SM中可能对应很多的thread block</li>
</ul></li>
</ul>
<h3 id="traditional-program-structure">Traditional Program
Structure</h3>
<ul>
<li>Function prototype</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> <span class="title function_">serialFunction</span><span class="params">(...)</span>;</span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">kernel</span><span class="params">(...)</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>main()
<ul>
<li>Allocate memory space on the
device-<code>cudaMalloc(&amp;d_in,bytes)</code></li>
</ul></li>
<li>Language
<ul>
<li><code>cudaMalloc((void**)&amp;d_in,#bytes)</code></li>
<li></li>
</ul></li>
</ul>
<h3 id="sample-vector-add">Sample: Vector Add</h3>
<p>用cuda编程，一个thread执行一个加法，有n个thread，n个核，我们需要把thread变成group，n=16</p>
<ul>
<li>我们分为4个block，每一个block有4个thread，所以blockDim=4</li>
<li>每一个block中有thread id</li>
</ul>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">vecadd</span><span class="params">(<span class="type">float</span>* A,<span class="type">float</span>* B,<span class="type">float</span>* C,<span class="type">int</span> N)</span>&#123;</span><br><span class="line">	<span class="type">float</span> *A_d,*B_d,*C_d;</span><br><span class="line">	cudaMalloc((<span class="type">void</span>**) &amp;A_d,N*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">	cudaMalloc((<span class="type">void</span>**) &amp;B_d,N*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">	cudaMalloc((<span class="type">void</span>**) &amp;C_d,N*<span class="keyword">sizeof</span>(<span class="type">float</span>));</span><br><span class="line">	</span><br><span class="line">	cudaMemcpy(A_d,A,N*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice);</span><br><span class="line">	cudaMemcpy(B_d,B,N*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyHostToDevice);</span><br><span class="line">	</span><br><span class="line">	<span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> numThreadsPerBlock = <span class="number">512</span>;</span><br><span class="line">	<span class="type">const</span> <span class="type">unsigned</span> <span class="type">int</span> numBlocks = (N+numThreadsPerBlock<span class="number">-1</span>)/numThreadsPerBlock;</span><br><span class="line">	</span><br><span class="line">	vacadd_kernel&lt;&lt;&lt;numBlocks,numThreadsPerBlock&gt;&gt;&gt;(A_d,B_d,C_d,N);</span><br><span class="line">	</span><br><span class="line">	cudaMemcpy(C,C_d,N*<span class="keyword">sizeof</span>(<span class="type">float</span>),cudaMemcpyDeviceToHost);</span><br><span class="line">	</span><br><span class="line">	cudaFree(A_d);</span><br><span class="line">	cudaFree(B_d);</span><br><span class="line">	cudaFree(C_d);</span><br><span class="line">	</span><br><span class="line">	</span><br><span class="line">&#125;</span><br><span class="line">__global__ <span class="type">void</span> <span class="title function_">vecadd_kernel</span><span class="params">(<span class="type">float</span>* A,<span class="type">float</span>* B,<span class="type">float</span>* C,<span class="type">int</span> N)</span>&#123;</span><br><span class="line">	<span class="type">int</span> i=blockDim.x*blockIdx.x+threadIdx.x;</span><br><span class="line">	<span class="comment">//确定每一个thread需要做的工作</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(i&lt;N)&#123;</span><br><span class="line">		C[i]=A[i]+B[i];</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>我们要确保更多的thread投入使用，即<code>numBlocks = (N+numThreadsPerBlock-1)/numThreadsPerBlock;</code></li>
<li>同时要判断数组是否越界，即在GPU的工作中我们需要对i进行判断</li>
</ul>
<h3 id="gpu的架构">GPU的架构</h3>
<ul>
<li><p>Streaming Processor Array—device</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2056.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>Streaming Multiprocessors（SM）/Compute Unit—block</p>
<ul>
<li>Streaming Processor Array下有很多的SM</li>
<li>block也可以看成分为了很多的warp，warp里是多个core执行SIMD的任务</li>
</ul></li>
<li><p>Streaming Processors（SP）—core</p>
<ul>
<li>SM中很多的SP，即/cores</li>
</ul></li>
<li><p>SIMT就是将一些Core进行分组，对应了Warp</p></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2057.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>GPU的SIMD不会暴露给Programmer</strong></p>
<p><strong>GPU的Warp也不会暴露给Programmer</strong></p>
<p>SIMD：SIMD指令的单个顺序指令流，每个指令指定多个数据输入</p>
<ul>
<li>VLD，VST，VADD等</li>
</ul>
<p>SIMT：标量指令的多指令流，线程动态分组到warp</p>
<ul>
<li>包装完后每次进入SIMDpipeline都是一个warp进入，SIMDpipeline中有很多的scalar
pipeline</li>
<li>SIMD单元中有很多的core</li>
</ul>
<h1 id="lecture-8-gpu-optimization">Lecture 8 GPU optimization</h1>
<h3 id="recall">Recall</h3>
<ul>
<li>Comparison of Memories</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2058.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>FF vs SRAM vs DRAM vs SSD
<ul>
<li>FF～K：Very fast，parallel access，very expensive</li>
<li>SRAM～M：相对快，一次只能一个data word，expensive</li>
<li>DRAM～G：比较慢，一次也一个word，reading destroys
content（refresh），cheap</li>
<li>Flash Memory～T：很慢，访问时间很长，非易失性，很便宜</li>
</ul></li>
<li>SRAM Summary
<ul>
<li>SRAM
<ul>
<li>Goal：Buffering Data on chip to reduce external memory traffic</li>
<li>Advantage：可以支持随机访问有高性能</li>
<li>Disadvantage：Low capacity通常是几MB</li>
</ul></li>
<li>Where to use
<ul>
<li>Cache in CPU</li>
<li>Shared Memory in GPU</li>
<li>On-chip buffer in AI accelerator</li>
</ul></li>
<li>How to use
<ul>
<li>Multiple small separate SRAMs</li>
<li>Banked design：wide access ports</li>
</ul></li>
</ul></li>
</ul>
<h2 id="simt-warp">SIMT &amp; Warp</h2>
<p>SIMT是硬件层面上的，Warp是软件层面的</p>
<ul>
<li>thread是组成一个warp在里面调度的</li>
<li>我们希望一个warp访问顺序的内存地址，只用产生一个DRAM的操作，利用Row
Buffer提升性能。</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2059.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2060.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="warp"><strong>Warp</strong></h3>
<p>SM采用的SIMT(Single-Instruction,
Multiple-Thread，单指令多线程)架构，warp(线程束)是最基本的执行单元，一个warp包含32个并行thread，这些thread<strong>以不同数据资源执行相同的指令</strong>。</p>
<h2 id="optimization">Optimization</h2>
<h3 id="减少全局内存访问">减少全局内存访问</h3>
<ul>
<li>Multithreading</li>
<li>Memory Coalescing减少内存访问</li>
<li>Shared Memory</li>
</ul>
<h3 id="multithreading">Multithreading</h3>
<p>使用warp的方式</p>
<h3 id="memory-coalescing">Memory Coalescing</h3>
<p>【<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/300785893">https://zhuanlan.zhihu.com/p/300785893</a>】</p>
<p>DRAM对于顺序访问更加友好。因此我们<strong>希望Warp中所有的线程访问全局内存是连续的</strong>。当warp中的所有线程执行load指令时，硬件会检测它们访问的全局内存位置是否是连续的。如果是的话，硬件会将这些访问合并成一个对连续位置的访问。</p>
<ul>
<li>例如，对于warp的load指令，如果线程T0访问的全局内存位置是N、线程T1的位置是N+1、线程T2的位置是N+2，...
则在访问DRAMs时，所有这些访问都会被合并为单个请求。这种合并访问允许DRAMs
以 burst 的方式传递数据。</li>
</ul>
<p>【Example】</p>
<p>假设我们有两个行优先存储的二维矩阵M, N，要实现一个内核来计算MxN。</p>
<p>内核中的每个线程会访问矩阵M的一行（下面的矩阵A）和矩阵N的一列（下面的矩阵B）。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2061.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>注意:</p>
<ul>
<li><strong>Memory Coalescing
合并发生在不同线程的同一周期之间</strong>，而不是线程内部的不同迭代之间。</li>
<li>warp中的所有线程都执行相同的指令，它们在任何时候都在同时执行第k次迭代。因此，一个线程在其生命周期内是否读取整行数据并不重要。重要的是，wrap内的所有线程在每次内存访问时可以合并。</li>
</ul>
<p>对于M的访存：</p>
<ul>
<li>第k个迭代，Thread i 读取第i行的第k个数据</li>
<li>访问不会合并</li>
</ul>
<p>对于N的访存：</p>
<ul>
<li>第k个迭代，Thread i 读取第k行的第i个数据</li>
<li>访问可以合并</li>
</ul>
<h3 id="shared-memory">Shared Memory</h3>
<ul>
<li>shared memory：因为shared
mempory是片上的（<strong>Cache级别</strong>），所以比局部内存(local
memory)和全局内存(global memory)快很多，实际上，shared
memory的延迟要比没有缓存的全局内存延迟小100倍（如果线程之间没有bank
conflicts的话）。<strong>在同一个block的线程共享一块shared
memory</strong>。线程可以访问同一个block内的其他线程让shared
memory从全局内存加载的数据。这个功能（结合线程同步，thread
synchronization）有很多作用，比如实现用户管理的数据cache，高性能的并行协作算法（比如并行规约，parallel
reduction）等。</li>
<li>Bank：<strong>bank是一种划分方式。在cpu中，访存是访问某个地址，获得地址上的数据，但是在这里，是一次性访问banks数量的地址，获得这些地址上的所有数据，并逻辑映射到不同的bank上。类似内存读取的控制。</strong>
<ul>
<li>Shared Memory是一种banked memory</li>
<li>一般来说32个banks在nvidia GPU</li>
<li>连续的32位word被分配到连续的banks中
<ul>
<li>Bank = Address % 32</li>
</ul></li>
</ul></li>
<li>Shared Memory Banks Conflict
<ul>
<li>只在warp内部出现</li>
<li>为了实现内存高带宽的同时访问，<strong>shared
memory被划分成了可以同时访问的等大小内存块(banks)</strong>。因此，内存读写n个地址的行为则可以以b个独立的bank同时操作的方式进行，这样有效带宽就提高到了一个bank的b倍。
然而，如果多个线程请求的内存地址被映射到了同一个bank上，那么这些请求就变成了串行的(serialized)。</li>
</ul></li>
<li>Data Reuse
<ul>
<li>有些memory
locations会被反复访问，因此我们可以将其放入共享内存，下次使用直接从shared
memory中取即可。</li>
<li>原始的计算方法：</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2062.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2063.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>使用Data Reuse：Tiling平铺
<ul>
<li>为了利用数据重用，我们将输入划分为可以加载到共享内存中的块tiling</li>
<li>这里的高斯卷积是3*3的，因此我们的图像要padding，所以tiling的大小为(L_SIZE+2)^2。</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2064.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2065.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li>矩阵乘法例子
<ul>
<li>用普通的CPU进行矩阵计算如下，矩阵B的读取容易Row Buffer
Miss，因为不是Row读入的，A的话会多次Row Buffer Hit。</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2066.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>CPU使用tiled平铺的实现，将子矩阵的乘法放入RAMs更快进行计算</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2067.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li><p>GPU的Matrix-Matrix乘法计算需要计算的C矩阵分为小块，分别计算，每一个元素都可以并行计算</p>
<p><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">__global__ <span class="keyword">void</span> mm_kernel(float* A, float* B, float* C, <span class="keyword">unsigned</span> <span class="keyword">int</span> N) &#123;    </span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> row = blockIdx<span class="variable">.y</span>*blockDim<span class="variable">.y</span> + threadIdx<span class="variable">.y</span>;    </span><br><span class="line">	<span class="keyword">unsigned</span> <span class="keyword">int</span> col = blockIdx<span class="variable">.x</span>*blockDim<span class="variable">.x</span> + threadIdx<span class="variable">.x</span>;    </span><br><span class="line">	float sum = <span class="number">0</span><span class="variable">.0f</span>;    </span><br><span class="line">	<span class="keyword">for</span>(<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;        </span><br><span class="line">		sum += A[row*N + i]*B[i*N + col];    </span><br><span class="line">	&#125;    </span><br><span class="line">	C[row*N + col] = sum;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2068.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li><p>可以使用Reuse，因为并行计算过程中有些thread的需要读取的值是一样的，比如下图C绿色部分的计算都要B绿色部分的值</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2069.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>GPU用同样可以使用Tiled，进行平铺，把每一个小块进行乘法最后相加</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2070.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2071.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2072.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul></li>
<li>Synchronization Function
<ul>
<li><code>void __syncthreads()</code></li>
<li>同步block中的所有线程</li>
<li>一旦块中的所有线程都到达这一点，执行就会正常恢复</li>
<li>用来解决RAW、WAR、WAW hazard</li>
</ul></li>
</ul>
<h2 id="simt-efficiency">SIMT Efficiency</h2>
<h3 id="divergency">Divergency</h3>
<ul>
<li>每一个thread可能有不同的控制循环逻辑，因此尽管用的同一套指令，但是难以并行。</li>
<li>threads 能执行不同的控制流</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2073.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2074.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>GPU用的是SIMDpipeline节省control
logic，因此我们最好让相同控制逻辑的thread放在一个warp中
<ul>
<li>我们知道GPU是SIMT架构，warp是GPU调度的基本单元，也就是说一个warp中的threads执行同一条指令，并且每个thread会使用各自的data执行该指。</li>
<li>那么问题来了，遇到分支语句如if…else,for,while，如果这些线程遇到这些控制流语句时，如果进入不同的分支，同一时刻除了正在执行的分之外，其余分支都被阻塞了，十分影响性能。这类问题就是warp
divergence，如下图所示。为了获得最好的性能，就需要避免同一个warp存在不同的执行路径</li>
</ul></li>
<li>SIMT每个周期只能执行一条指令，我们怎么并行执行conditon branch指令
<ul>
<li>branch本身没有问题，我们只要保证同一个warp执行同一个指令就可以</li>
<li>那么假设我们的warp中部分的指令执行了A部分执行了B，那我们直接分步执行然后最后merge起来即可。</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2075.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2076.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>为什么Divergency-Free的SIMD利用率更高？因为相同的控制逻辑的thread
warp分在了一起，而第一种的thread是隔开的，一个warp加入都是32thread，那么第一个的SIMD使用率只有50%，第二个是100%。第二个同一个warp的thread的执行路径一致性强！</p>
<h3 id="atomic-operations">Atomic Operations</h3>
<p>CUDA提供了原子指令在shared memory和global memory上</p>
<ul>
<li>它们自动执行Read-modify-write操作</li>
</ul>
<p>Arithmetic Functions</p>
<ul>
<li><p>Add sub max min exch inc dec CAS</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-20_12.03.49.png" alt="截屏2023-04-20 12.03.49.png">
<figcaption aria-hidden="true">截屏2023-04-20 12.03.49.png</figcaption>
</figure></li>
</ul>
<p>Bitwise Functions</p>
<ul>
<li>And or xor</li>
</ul>
<p>比较讨厌有bank
conflict的情况，比如我们要访问0和2，访问2和2的比较，一个是并行的一个是串行的。这种有conflict的时候叫做Atomic
conflict</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2077.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>当多个线程需要更新相同的内存位置时，使用原子操作来防止数据竞争</p>
<h3 id="image-histogram">Image Histogram</h3>
<p>直方图在图像处理中有广泛的应用</p>
<ul>
<li>在直方图中投票之前可能需要进行一些计算</li>
</ul>
<h2 id="cpu和gpu的通信">CPU和GPU的通信</h2>
<h2 id="cuda-streams">CUDA Streams</h2>
<p>按顺序执行的操作序列</p>
<ol type="1">
<li>数据传输CPU-GPU</li>
<li>内核执行
<ol type="1">
<li>D个输入数据实例，B个blocks</li>
<li>#Streams: (D/ #Streams)数据实例，(B/ #Streams)块</li>
</ol></li>
<li>数据传输GPU-CPU</li>
</ol>
<p>通信和执行我们希望可以尽量快速执行，而不是完全并行。就是一边IO一边计算执行。</p>
<p>原来需要的时间为<span class="math inline">\(t_T+t_E\)</span></p>
<ul>
<li>当<span class="math inline">\(t_T\)</span>占主要时间，Stream执行下的时间<span class="math inline">\(t_T+t_E/ \#Stream\)</span></li>
<li>当<span class="math inline">\(t_E\)</span>占主要时间，Stream执行下的时间<span class="math inline">\(t_E+t_T/ \#Stream\)</span></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2078.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="gpu-limitation">GPU Limitation</h2>
<ol type="1">
<li>PCIe的口通信很慢</li>
</ol>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2079.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ol type="1">
<li>GPU并行编程，本质有一个overhead</li>
</ol>
<h1 id="lecture-9-cache">Lecture 9 Cache</h1>
<h2 id="recall-1">Recall</h2>
<h3 id="gpu-programming-model-cuda-programming-model">GPU Programming
Model &amp; CUDA Programming Model</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2059.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>Software：warp之内的thread需要分叉少、访问内存连续等特点，才能发挥优势</p>
<h3 id="data-movement-computation">Data Movement &amp; Computation</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_10.06.07.png" alt="截屏2023-04-27 10.06.07.png">
<figcaption aria-hidden="true">截屏2023-04-27 10.06.07.png</figcaption>
</figure>
<p>跨芯片访问时间会很多。我们的计算在同一个芯片内会快很多。</p>
<h3 id="dram-1">DRAM</h3>
<p>DRAM内存访问特性：顺序访问很快，因为有row buffer的存在，若row buffer
hit，给一个column地址就可以返回。若Row buffer miss，则需要先更新row
buffer，再进行column读取。row buffer hit带宽高，miss带宽低。</p>
<p>GPU如何解决延时高的问题？Multithreading、Memory Coalescing、Share
Memory</p>
<ul>
<li>Multithread：多warp有warp延时时，其他warp也可以跑。增加SM的利用率。</li>
<li>Memory
Coalescing：DRAM对于顺序访问更加友好。因此我们<strong>希望Warp中所有的线程访问全局内存是连续的</strong>。当warp中的所有线程执行load指令时，硬件会检测它们访问的全局内存位置是否是连续的。如果是的话，硬件会将这些访问合并成一个对连续位置的访问。不用Memory
Coalescing，一个warp需要访问#thread次内存，用了之后只需要一个warp一次。<strong>Memory
Coalescing
合并发生在不同线程的同一周期之间</strong>，而不是线程内部的不同迭代之间。</li>
<li>Share Memory：暂存一些全局数据</li>
</ul>
<p>Cache是因为DRAM的带宽低，延时高，会漏电，需要refresh</p>
<h2 id="memory-hierarchy-and-caches">Memory Hierarchy and Caches</h2>
<p>Cache对roofline
model的影响，cache的传输速度快，带宽大因此图像如下：</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_10.27.03.png" alt="截屏2023-04-27 10.27.03.png">
<figcaption aria-hidden="true">截屏2023-04-27 10.27.03.png</figcaption>
</figure>
<p>Ideal Memory</p>
<ul>
<li>Zero access time</li>
<li>infinite capacity</li>
<li>infinite bandwidth</li>
<li>zero cost</li>
</ul>
<p>Problem of Ideal Memory</p>
<ul>
<li>Ideal memory‘s requirements oppose each other
<ul>
<li>bigger is slower</li>
<li>faster is more expensive</li>
<li>higher bandwidth is more expensive</li>
</ul></li>
</ul>
<p>Why Cache？</p>
<ul>
<li>Challenge：DRAM latency is ～100 ns，slightly decreasing over
time</li>
<li>Our Goal：CPU wants both fast and large memory without modifying
user code
<ul>
<li>share memory快而大，但是需要改代码</li>
<li>cache<strong>快而大，且不需要改代码</strong></li>
</ul></li>
</ul>
<h3 id="memory-hierarchy-example">Memory Hierarchy Example</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_10.43.03.png" alt="截屏2023-04-27 10.43.03.png">
<figcaption aria-hidden="true">截屏2023-04-27 10.43.03.png</figcaption>
</figure>
<h3 id="why-cache-works">Why Cache Works？</h3>
<p><strong>Locality！！！局部性</strong></p>
<ul>
<li>空间局部性
<ul>
<li>如果刚刚做了一些事，那么很有可能你会做与之相关、相似的事情</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2080.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li>时间局部性
<ul>
<li>如果刚刚做了一些事，那么很有可能你将来马上还会做</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2081.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<h2 id="caching-in-a-pipeline-design">Caching in a pipeline Design</h2>
<ul>
<li><p>cache needs to be tightly integrated into the pipeline</p>
<ul>
<li>ideally，access in 1-cycle so that load-dependence operations do not
stall</li>
</ul></li>
<li><p>High frequency pipeline，不能将cache弄的很大</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.02.17.png" alt="截屏2023-04-27 11.02.17.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.02.17.png</figcaption>
</figure></li>
<li><p>Manual：程序员管理跨关卡的数据移动</p>
<ul>
<li>对于编写大量程序的程序员来说太痛苦了</li>
<li>“core”和“drum”存储器在20世纪50年代的对比在嵌入式处理器(片上刮擦板SRAM代替缓存)、gpu(称为“共享内存”)、ML加速器……</li>
</ul></li>
<li><p>Automatic：硬件管理跨级别的数据移动，对程序员透明</p>
<ul>
<li>c++程序员的生活更轻松一般程序员不需要了解缓存</li>
<li>你不需要知道缓存有多大以及它是如何工作的来编写一个“正确”的程序!(如果你想要一个“快速”的项目呢?)</li>
</ul></li>
</ul>
<h2 id="hierarchy-latency-analysis">Hierarchy Latency Analysis</h2>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2082.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>我们的访问i级cache的时间为ti，i级的hit rate为hi，miss
rate为mi，防问第i级cache成功的时间为Ti</p>
<p>我们的理想是Ti约等于ti，那么我们要求Miss
rate小，下一层cache的延迟小，才能保证这个理想。</p>
<ul>
<li>让<span class="math inline">\(mr_i\)</span>小
<ul>
<li><strong>增加cache的容量</strong>会使得<span class="math inline">\(mr_i\)</span>变小，但是会增大<span class="math inline">\(t_i\)</span></li>
</ul></li>
<li>让<span class="math inline">\(T_{i+1}\)</span>小</li>
</ul>
<h2 id="cache">Cache</h2>
<ul>
<li>任何“Memory”经常使用的结果/数据的结构
<ul>
<li>避免重复从头开始复制/获取结果/数据所需的长延迟操作</li>
<li>就是把经常使用的数据进行暂存</li>
</ul></li>
<li>在处理器设计上下文中最常见的是
<ul>
<li>一种自动管理的内存结构</li>
<li>例如，在快速SRAM中存储最频繁或最近访问的DRAM存储器位置，以避免重复支付DRAM访问延迟</li>
</ul></li>
</ul>
<h3 id="blocks">Blocks</h3>
<ul>
<li><code>main memory</code>逻辑上划分为固定大小的块(block)</li>
<li>缓存只能容纳有限数量的块,访问都是一个block访问的</li>
<li>每个block地址映射到缓存中的一个<code>potential position</code>，由用于索引标记和数据存储的地址中的索引位确定</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.27.39.png" alt="截屏2023-04-27 11.27.39.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.27.39.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.27.57.png" alt="截屏2023-04-27 11.27.57.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.27.57.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.29.21.png" alt="截屏2023-04-27 11.29.21.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.29.21.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.29.33.png" alt="截屏2023-04-27 11.29.33.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.29.33.png</figcaption>
</figure>
<h3 id="toy-example">Toy Example</h3>
<ul>
<li>256-byte memory(8 bit 地址)</li>
<li>64-byte cache，8-byte blocks</li>
<li>那么内存有32个block，cache有8个block</li>
</ul>
<h2 id="caching-basics">Caching Basics</h2>
<ul>
<li><p>Cache Block(line)：Unit of storage in the cache</p>
<ul>
<li>内存在逻辑上被划分为块，这些块映射到缓存中的潜在位置</li>
</ul></li>
<li><p>Hit：if in cache，use cache data instead of accessing
memory</p></li>
<li><p>Miss：if not in cache，bring block into cache</p></li>
<li><p>为了高的hit rate 我们需要设计时考虑：</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2083.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<h3 id="placement">Placement</h3>
<p>如何将内存的地址空间映射到cache？</p>
<ul>
<li><p>在缓存的哪个位置可以放置一个给定的“主内存块”</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.39.51.png" alt="截屏2023-04-27 11.39.51.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.39.51.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2084.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<p>有三种方式：</p>
<ul>
<li>Direct-Mapped
<ul>
<li><p>一个内存块去一个对应的cache block</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.52.05.png" alt="截屏2023-04-27 11.52.05.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.52.05.png</figcaption>
</figure></li>
<li><p>根据index找到对应的cache
block，查看tag是否一致确定是否hit</p></li>
<li><p>访问方式会导致很多的miss</p></li>
</ul></li>
<li>Fully-associative
<ul>
<li><p>一个内存块可以去任意一个cache
block，只要空着就可以进去或者满了替换</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.55.51.png" alt="截屏2023-04-27 11.55.51.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.55.51.png</figcaption>
</figure></li>
<li><p>直接对比全部的tag看是否hit，miss的话直接找一个空的cache
block或者替换block</p></li>
<li><p>复杂度很高</p></li>
</ul></li>
<li>Set- associative
<ul>
<li><p>在N-way组相连中，一个内存块可以去N个cache
block中的任意一个</p></li>
<li><p>根据index找N个cache block</p></li>
<li><p>对比N个里面的tag查看是否hit，miss的话找空位或者可替换位进行替换</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.56.22.png" alt="截屏2023-04-27 11.56.22.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.56.22.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-04-27_11.58.59.png" alt="截屏2023-04-27 11.58.59.png">
<figcaption aria-hidden="true">截屏2023-04-27 11.58.59.png</figcaption>
</figure></li>
</ul></li>
</ul>
<h1 id="lecture-10-cache-coherence">Lecture 10 Cache Coherence</h1>
<h2 id="basics">Basics</h2>
<ul>
<li>Cache
block：cache中存储的单元，memory是被划分为了很多的block可以映射到cache’中去</li>
<li>为了更高的cache hit rate，我们需要考虑
<ul>
<li>Placement：在哪一个/如何将memory中的block映射到cache中去</li>
<li>Replacement：cache满了之后怎么将数据移出</li>
<li>Granularity of management：block是大是小？</li>
<li>Write Policy：我们写操作要做什么</li>
<li>Instructions/Data：我们如何分离处理他们</li>
</ul></li>
</ul>
<h2 id="cache-block替换策略">Cache Block替换策略</h2>
<ol type="1">
<li>随机替换</li>
<li>FIFO</li>
<li>LRU</li>
<li>Hybrid replacement policies</li>
<li>Optimal replacement policy？</li>
</ol>
<h3 id="lru">LRU</h3>
<p>我们需要记录访问N-way中的Block的访问顺序，实时更新</p>
<p>我们其实需要的不是完美的LRU，我们可以允许一些误差。</p>
<p>我们可以使用近似的LRU</p>
<p>N路组相连，我们每一个block需要<span class="math inline">\(log_2N\)</span>个bit来track，那么一个组就需要<span class="math inline">\(Nlog_2N\)</span>个bit</p>
<h3 id="plru">PLRU</h3>
<p>LRU的近似算法</p>
<ul>
<li>Not MRU，最不常用的替换</li>
<li>Victim-NextVictim Replacement</li>
<li>PLRU</li>
</ul>
<p>Pseudo LRU for 8-way set- associated cache：</p>
<ul>
<li><p>假设我们一个set里面有8个cache
block（L0-L7），7个bit给rule（B0-B6）</p></li>
<li><p>PLRU根据Rule bit选择一个block</p></li>
<li><p>更新规则，只用更新Rule bit</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-11_10.23.59.png" alt="截屏2023-05-11 10.23.59.png">
<figcaption aria-hidden="true">截屏2023-05-11 10.23.59.png</figcaption>
</figure></li>
</ul>
<h3 id="random">Random</h3>
<p>LRU一般来说比Random好，但是也有random更好的情况</p>
<p>比如我们一直访问没有hit的同一个set的cache block，就会一直miss</p>
<p>实际使用的情况中，我们使用混合的方式。主要以LRU为主，附带Random</p>
<h3 id="optimal-replacement-policy">Optimal Replacement Policy</h3>
<h4 id="beladys-opt">Belady‘s OPT</h4>
<ul>
<li>基于假设：我们已经知道了我们未来要使用的block</li>
<li>替换将来最长未使用</li>
</ul>
<h2 id="write-policy">Write Policy</h2>
<h3 id="cache-policieshandling-memory-write">Cache Policies：Handling
Memory Write</h3>
<ul>
<li>Where should you write the result of a store？One policy for each
step：
<ul>
<li>step1：<strong>store instruction - cache</strong>，either policy
works
<ul>
<li>Write-Allocate Policy（default）
<ul>
<li>Allocate the cache line put it in cache</li>
<li>Issue：Read an entire cache block from memory</li>
<li>把memory先写到cache里面</li>
</ul></li>
<li>Write-No-Allocate Policy（PCIe/IO）
<ul>
<li>Write it directly to memory without allocation in cache</li>
<li>没有cache的分配</li>
</ul></li>
</ul></li>
<li>step2：<strong>cache- memory</strong>
<ul>
<li>Write-Back
<ul>
<li><strong>不写回memory，直到cache block要被替换出去</strong></li>
<li>好处：我们的cache可以预先处理多次修改，再只用一次访问memory写回</li>
<li>缺点：需要额外的dirty bit来检查是否memory和cache一致</li>
</ul></li>
<li>Write-Through
<ul>
<li>更新cache的同时，将memory同步更新</li>
<li>好处：简单，不会出错，Cache数据都是最新的</li>
<li>缺点：需要大量访问内存</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<h2 id="instructions-vs-data-caches">Instructions vs Data Caches</h2>
<ul>
<li>Core question：Separate or Unified</li>
<li>统一的利弊：
<ul>
<li>+缓存空间的动态共享，不会出现静态分区(即单独的I和D缓存)可能出现的过度配置</li>
<li>-指令和数据可以相互碰撞(也就是说，两者都没有保证空间)</li>
<li>-I和D在管道中的不同位置被访问。为了快速访问，我们应该把统一缓存放在哪里?</li>
</ul></li>
<li>现代CPU
<ul>
<li>一级cache往往是split的</li>
<li>高级的cache是unified的</li>
</ul></li>
</ul>
<h2 id="cache-performance">Cache Performance</h2>
<h3 id="cache-size">Cache Size：</h3>
<ul>
<li>我们希望是越大越好，因为有更好的时间局部性，但是越大不是完全越好的。</li>
<li>too large：局部性强，影响了hit和miss rate
<ul>
<li>访问更慢</li>
</ul></li>
<li>too small：
<ul>
<li>局部性不强</li>
</ul></li>
</ul>
<h3 id="block-size">Block Size：</h3>
<ul>
<li>too small：
<ul>
<li>空间局部性不够，有很大的一部分被tag使用</li>
</ul></li>
<li>too large：
<ul>
<li>block的数量少了</li>
<li>cache的利用率低</li>
</ul></li>
<li>太小不好，太大不好，在中间最好</li>
</ul>
<h3 id="associativity">Associativity</h3>
<ul>
<li>Large Associativity</li>
<li>Small Associativity</li>
</ul>
<h2 id="cache-miss">Cache Miss</h2>
<h3 id="compulsory-miss">Compulsory miss</h3>
<p>第一次访问就会miss，我们的解决方法是：预取</p>
<ul>
<li>caching cannot help</li>
<li>我们将可能需要的数据提前放入cache中</li>
</ul>
<h3 id="conflict-miss">Conflict miss</h3>
<p>多个memory内存去了同一个的block，那么我们就尽量增加associativity</p>
<ul>
<li>More associativity</li>
<li>Victim cache</li>
</ul>
<h3 id="capacity-miss">Capacity miss</h3>
<p>Utilize cache space better：keep blocks that will be referenced</p>
<h2 id="cache-in-multi-core-cpu">Cache in Multi-core CPU</h2>
<h3 id="recallmulti-core-over-large-superscalar">Recall：Multi-Core over
Large Superscalar</h3>
<ul>
<li>Technology push
<ul>
<li>指令发布队列的大小限制了超标量的循环时间，OoO处理器→降低了性能</li>
<li>复杂度随问题宽度的二次增长</li>
<li>支持大指令窗口和问题宽度的大型多端口寄存器文件→更多的资源，降低频率或更长时间的RF访问，降低性能</li>
</ul></li>
<li>Application push
<ul>
<li>很多app需要在你的cpu上跑</li>
</ul></li>
</ul>
<p>Challenge from Multi-core CPU：</p>
<ul>
<li>需要缓存来缓解<strong>长内存延迟</strong>的负面影响</li>
<li>多核CPU也要通过cache改进内存访问延时长的问题</li>
<li>如何设计一个cache给多核用？核心需要一个一致的内存视图。A consistent
view of memory</li>
</ul>
<h2 id="caches-in-multi-core-cpu">Caches in Multi-Core CPU</h2>
<p>Cache的效率在Multi-Core/Multi-Thread系统中十分重要</p>
<ul>
<li>内存带宽非常宝贵，读取的数据一定要物尽其用</li>
<li>缓存空间是跨内核/线程的有限资源</li>
</ul>
<p>多个决定：</p>
<ul>
<li>Shared vs private Cache</li>
<li>How to preserve coherence and consistence</li>
</ul>
<h2 id="private-shared-caches">Private &amp; Shared Caches</h2>
<p>Private Cache</p>
<ul>
<li>这个私有的cache属于一个core</li>
</ul>
<p>Shared Cache</p>
<ul>
<li>共享的cache是被多个core共享的</li>
</ul>
<p>资源共享的好处</p>
<ul>
<li><p>通信延时降低</p></li>
<li><p>资源利用率效率提升</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-11_11.00.19.png" alt="截屏2023-05-11 11.00.19.png">
<figcaption aria-hidden="true">截屏2023-05-11 11.00.19.png</figcaption>
</figure></li>
</ul>
<h3 id="resource-sharing-concept-and-advantages">Resource Sharing
Concept and Advantages</h3>
<ul>
<li>Idea：与其将一个硬件资源专用于一个硬件上下文，不如允许多个上下文使用它
<ul>
<li>比如：functional units、pipeline、caches、buses</li>
</ul></li>
<li>Advantages
<ul>
<li>资源共享提高了利用率/效率，增加了throughput
<ul>
<li><strong>当资源被一个线程闲置时，另一个线程可以使用它;无需复制共享数据</strong></li>
</ul></li>
<li>减少了通信延迟
<ul>
<li>例如，多个线程之间共享的数据可以保存在多线程处理器的同一缓存中</li>
</ul></li>
<li>兼容共享内存编程模型</li>
</ul></li>
<li>Disadvantages
<ul>
<li><strong>共享资源会导致资源争夺</strong>
<ul>
<li>当资源不是空闲时，其他线程不能使用它。</li>
<li>如果空间被一个线程占用，则需要另一个线程重新占用它</li>
</ul></li>
<li>有时候会降低某些线程的表现
<ul>
<li>线程的performance会比单独跑要低</li>
</ul></li>
<li>消除了性能的隔离性，运行时性能不一致</li>
<li>Uncontrolled（free-for-all）sharing degrades OoS</li>
</ul></li>
</ul>
<p>那么说回多核之间的共享Caches的优缺点</p>
<ul>
<li><p>Advantage</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2085.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
<li><p>Disadvantage</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2086.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<h2 id="cache-coherence-memory-consistency">Cache Coherence &amp; Memory
Consistency</h2>
<ul>
<li>都是一致性，一个是不同处理器对于cache的block的访问的排序，一个不同处理器是对内存操作的排序</li>
<li>Coherence：Coherence是关于从不同处理器到<strong>相同内存位置</strong>的操作的<strong>排序</strong>
<ul>
<li>对每个缓存块的访问进行本地排序</li>
</ul></li>
<li>Consistency：Consistency是指对来自不同处理器(对不同内存位置)的所有内存操作进行排序
<ul>
<li>对所有内存位置的访问进行全局排序</li>
</ul></li>
</ul>
<h2 id="cache-coherence">Cache Coherence</h2>
<ul>
<li><p>什么是缓存一致性？【<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/417487200">https://zhuanlan.zhihu.com/p/417487200</a>】</p>
<p>当程序在运行过程中，会将运算需要的数据从主存复制一份到<strong>CPU高速缓存</strong>中，那么CPU进行计算时就可以从它的高速缓存读取数据和向其中写入数据，当运算结束后，再将高速缓存中的数据刷新到主存当中。</p>
<p>当线程执行这个语句时，<strong>会先从主存当中读取i的值</strong>，然后<strong>复制一份到高速缓存</strong>当中，然后<strong>CPU执行指令对i指令进行加1操作</strong>，然后将<strong>数据写入高速缓存</strong>，最后将<strong>高速缓存中i最新的值刷新到主存</strong>当中。</p>
<p>这个代码在<strong>单线程</strong>中运行时没有任何问题的，但是在多线程中运行就会有问题了。在多核CPU中，每条线程可能运行于不同的CPU中，因此每个线程运行时有自己的高速缓存（对单核CPU来说，其实也会出现这种问题，只不过是以线程调度的形式来分别执行的）。<strong>我们以多核CPU为例。</strong></p>
<p>比如同时有两个线程执行这段代码，假如初始时i的值为0，那么我们希望两个线程执行完之后i的值变为2。但事实会是这样吗？</p>
<p>可能存在下面一种情况：初始时，两个线程分别读取i的值存入各自所在的CPU的高速缓存当中，然后线程1进行加1操作，然后把i的最新值1写入到内存。此时线程2的高速缓存当中i的值还是0，进行加1操作后，i的值为1，然后线程2把i的值写入内存。</p>
<p>最终结果i的值是1，而不是2。这就是著名的<strong>缓存一致性问题</strong>。通常称这种被多个线程访问的变量为共享变量。</p>
<p>也就是说，<strong>如果一个变量在多个CPU中都存在缓存（一般在多线程编程时才会出现），那么就可能存在缓存不一致的问题。</strong></p></li>
</ul>
<p><strong>现代处理器的并行代码或线程共享内存，需要保证数据一致性</strong>。从软件、硬件两个角度考虑解决cache一致性。现代处理器cache一般对程序员透明，ISA一般只提供cache
flush命令。如果靠软件解决，对程序员负担太大。</p>
<p>缓存一致性：多核具有从任何核到内存地址的最后写入值的<strong>一致状态</strong></p>
<ul>
<li>Program order
preservation：程序的基本的顺序需要保持，处理器P写入地址，然后从同一地址读取，P得到写入的值。</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2087.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Coherent memory
view：连贯的内存视图，当P1进行了操作<code>mem[X]=1</code>在一段时间后，P2将可以<code>mem[X]</code>中读取出1</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2088.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Write
serialization：写序列化，不同处理器对同一地址的写操作在<strong>所有处理器</strong>中都以相同的顺序显示。所有的处理器看到的顺序必须是一样的。</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2089.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="hardware-architecture-for-cache-coherence">Hardware Architecture
for Cache Coherence</h2>
<p>从cores的角度，Cores、caches、interconnect、memory work一起实现cache
coherence</p>
<ul>
<li>Interconnect：Snoop/Directory</li>
<li>Cache updating：invl./update</li>
<li>Cache Tags：MESI</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2090.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2091.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="cache-coherence-protocols">Cache Coherence Protocols</h3>
<ul>
<li>Snoop：
<ul>
<li>基于总线，每个总线动作在总线上广播，一次一个动作。</li>
<li>Cache需要对共享总线进行侦测，如果侦测到总线上的操作与自己cache中的某个cache
block相符合(tag一致)，则采取某种动作（具体动作由cache一致性协议定义，比如MSI），这种系统需要支持广播功能的总线，此外这种方案比较<strong>适用于小规模的系统</strong></li>
<li>所有的Memory Request都要单点序列化</li>
</ul></li>
<li>Directory
<ul>
<li></li>
</ul></li>
</ul>
<h3 id="updating-policy">Updating Policy</h3>
<ul>
<li>Update Protocol
<ul>
<li><p>更新协议，只要有一个核中写了cache line，别的核对应一样tag的cache
line也要更新，但是这样子开销大，比如别的核不读这个cache
line只写，那么就会很浪费。</p></li>
<li><p>Write-update（或者叫write broadcast）:
当一个处理器更新某个数据时，其不往总线上发送invalidate消息，而是往总线上发送一条update
A消息，直接告知该变量的最新值，其它处理器上的cache侦听到这个操作后更新本地的变量为这个广播的最新值。其它处理器在下一次使用该变量时，直接cache
命中，可以直接使用最新数据，而不产生read miss操作。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/v2-413465b33d2d0258bec87584b1db1306_720w.jpg" alt="v2-413465b33d2d0258bec87584b1db1306_720w.jpg">
<figcaption aria-hidden="true">v2-413465b33d2d0258bec87584b1db1306_720w.jpg</figcaption>
</figure></li>
</ul></li>
<li>Invalidate Protocol
<ul>
<li><p>无效协议，写的时候，我们把别的核的cache无效掉，别的核要写要读需要重新发起请求。</p></li>
<li><p>Write-invalidate:
一个处理器需要更新某个数据时，其先往总线上发送一条Invalidate请求，系统中其它处理器上的cache控制器监控到这条消息，把自己的状态设置为invalid。如果这些invalidated
cache block在下一次需要访问该数据时，则会重新进行一个总线的read
miss操作。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/v2-3c7255aff9e10b4f50a28fe25551c7ce_720w.jpg" alt="v2-3c7255aff9e10b4f50a28fe25551c7ce_720w.jpg">
<figcaption aria-hidden="true">v2-3c7255aff9e10b4f50a28fe25551c7ce_720w.jpg</figcaption>
</figure></li>
</ul></li>
</ul>
<h3 id="msi协议httpszhuanlan.zhihu.comp417949142">MSI协议【<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/417949142">https://zhuanlan.zhihu.com/p/417949142</a>】</h3>
<p>实现内存一致性的协议可以细分为很多种，但最基本的还是MSI协议，MSI代表了
cache block的三种不同状态 I,S,M，分别是invalid,
Shared和Modified。任何时刻Cache
block必处于这三种状态之中的一种状态。</p>
<ul>
<li>I（Invalid）状态：该cache
block在当前cache中不存在或者被总线上的invalidate操作设置为无效，处于该状态的cache
block需要从Memory或者其它cache中获取，在访问该cache
block时，cache控制器需要往总线上产生一个read miss或者write
miss操作。</li>
<li>S（Shared）状态：该Cache
Block的内容没有被修改并且处于只读状态，cache
block存在于至少一处cache中和memory中，该状态下CPU能够直接读取该cache
block数据而无需与其它cache进行通信，总线上没有操作。</li>
<li>M（Modify）状态：该状态的Cache
Block只能存在一处cache中，该状态下cache
block，CPU能够直接读写而不需要知会其它CPU上的Cache。<strong>该状态的cache
block负责为其它cache节点提供最新的数据</strong>，<strong>同时也负责把最新数据写回到memory中</strong>。</li>
</ul>
<p>【从CPU的角度来看MSI协议】本CPU的命令</p>
<ol type="1">
<li>I状态：CPU中cache
block的状态总是从I状态开始的，因为一开始的时候cache中并没有缓存某个内存地址的数据。
<ol type="1">
<li>当CPU需要<strong>读取</strong>某个内存数据时，此时发生cache
miss，于是cache控制器对总线发送一个read
miss操作。Memory或者其它远端cache控制器(处于M状态)对该请求进行响应，本地cache拿到数据，进入shared状态</li>
<li>当CPU发起一个<strong>写操作</strong>时，Cache控制器往总线上发送一条write
miss命令，然后进入Modified状态。（问题：CPU为什么可以对处于I状态的cache
block 发起写操作？）</li>
</ol></li>
<li>S状态：
<ol type="1">
<li>CPU发起Read，由于Shared态可读，那么直接hit，仍为S态</li>
<li>CPU发起Write，则cache控制器往共享总线上放置一个invalidate命令，该cache
block切换到Modified状态</li>
</ol></li>
<li>M状态：
<ol type="1">
<li>无论CPU发起读还是写操作，由于直接cache命中，状态都不发生改变；</li>
</ol></li>
</ol>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2092.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>【从总线的角度来看MSI协议】来自其他CPU的命令</p>
<ol type="1">
<li>I状态：
<ol type="1">
<li>表明本CPU对于该cache
block数据不关心，所以也就不关心总线上的任何消息，也不会对总线上的任何消息产生动作。</li>
</ol></li>
<li>S状态：
<ol type="1">
<li>如果侦测到总线上发生了invalidate命令或者write
miss命令，则表明<strong>其它CPU需要对该cache
block进行修改</strong>，所以把本地cache block设置为无效，进入I状态</li>
<li>如果侦测到总线上发生了read
miss操作，表明其他CPU要读数据，则memory会提供有效响应，而本cache
block由于不是写的，仅仅是可读，不是数据来源，不做动作，维持在S状态</li>
</ol></li>
<li>M状态：
<ol type="1">
<li>如果侦测到总线上发生read miss操作，则本cache需要为请求方提供该cache
block,同时会利用这个机会更新memory。此时该cache
block进入共享状态S。</li>
<li>如果侦测到总线上发生write
miss操作，则本cache需要先更新自己的最新数据给请求方提方,同时会利用这个机会更新memory。但此时该cache
block进入无效状态I。</li>
</ol></li>
</ol>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2093.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="mesi协议">MESI协议</h3>
<p>查看这个例子，这两里的A和B只有一份内存的拷贝，即只在一个Cache
中，这里的两次的Invalidate真的有必要吗？没有必要，因此我们增强了MSI协议</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2094.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2095.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>引入新状态E（Exclusive）：处于独占状态（E）的cache
block只出现在<strong>一个处理器节点</strong>上，并且是干净的（表示cache中的数据与memory的数据一致）。处于E状态的cache
block, <strong>修改时候不需要进行总线上的invalidate操作</strong>。</p>
<ul>
<li>Key Differences
<ul>
<li>Local Core reads block in state E，state hold。读E状态的Cache
block，状态不变</li>
<li>Local Core writes block in state E，state M，without bus action</li>
<li>当总线上有Read Miss的指令，说明别的CPU有读这个Cache
Block了，E状态要改成S状态</li>
<li>当总线上有Write Miss的指令，说明别的CPU有写这个Cache
Block了，E状态要改成I状态</li>
</ul></li>
</ul>
<h2 id="cache-consistency">Cache Consistency</h2>
<h3 id="ordering-of-operations">Ordering of Operations</h3>
<p>如果说有ABCD四个操作，硬件执行的顺序是怎么样的？</p>
<p>Consistency：一个编程者和微架构之间的协议</p>
<ul>
<li>保持“预期的”(更准确地说，“商定的”)顺序简化了程序员的工作易于调试</li>
<li>保持“预期”的顺序通常会使硬件设计师的生活变得困难</li>
</ul>
<h3 id="single-processor的memory-ordering">Single Processor的Memory
Ordering</h3>
<p>被冯诺依曼模型确定了，顺序的执行程序。</p>
<p>OoO执行不会改变语义，即OoO执行时指令 retire顺序是程序的顺序。</p>
<p>优点</p>
<ul>
<li>架构状态在执行中是精确的</li>
<li>架构状态在程序的不同运行中是一致的→更容易调试程序</li>
</ul>
<p>缺点</p>
<ul>
<li>保持顺序会增加开销，降低性能，增加复杂性，降低可伸缩性</li>
</ul>
<h3 id="mimd-processor的memory-ordering">MIMD Processor的Memory
Ordering</h3>
<p>每个处理器的内存操作都是按照该处理器上运行的“线程”的顺序进行的(假设每个处理器都遵循冯·诺伊曼模型)。</p>
<ul>
<li>多个处理器<strong>并发</strong>地执行内存操作内存</li>
</ul>
<p>如何看到来自所有处理器的操作顺序?换句话说，不同处理器之间的操作顺序是什么?</p>
<h3 id="challenge">Challenge</h3>
<p>两个Processor不能看到相同的内存操作顺序。</p>
<p>多个内存更新之间的“发生在之前”关系在两个处理器的观点中是不一致的</p>
<h3 id="types-of-memory-barrier">4 Types of Memory Barrier</h3>
<ul>
<li>Load-Load:Effectively prevents ordering of loads performed before
the barrier with loads performed after the barrier</li>
<li>Load-Store:Effectively prevents ordering of loads performed before
the barrier with writes performed after the barrier</li>
<li>Store-Store:Effectively prevents ordering of stores performed before
the barrier with stores performed after the barrier</li>
<li>Store-Load:Effectively prevents ordering of stores performed before
the barrier with loads performed after the barrier</li>
</ul>
<p>多处理器系统中的顺序一致性，如果</p>
<ul>
<li>在单处理机中：每个单独处理机的操作按其程序指定的顺序出现在这个序列中和</li>
<li>在多处理器中：任何执行的结果都是相同的，就好像所有处理器的操作都按照某种顺序执行，就好像它们在操作单个共享内存一样</li>
</ul>
<h3 id="sequential-consistency">Sequential Consistency</h3>
<p>顺序一致性：内存是一个开关，每次服务于来自任何处理器的一个加载或存储</p>
<ul>
<li>所有处理器同时看到当前服务的load或store</li>
<li>每个处理器的操作都按程序顺序进行</li>
</ul>
<p>问题是性能极低，都是顺序执行的</p>
<h3 id="total-store-order">Total Store Order</h3>
<p>Total Store Order == SC + Store Buffer</p>
<ul>
<li>提交存储指令意味着数据存储在存储缓冲区中，而不是缓存层次结构中。</li>
<li>存储指令写入本地存储缓冲区，然后立即执行下一条指令(例如，加载)。</li>
<li>当准备好时，缓存将从存储缓冲区中取出写操作。</li>
<li>不保留存储加载顺序。</li>
</ul>
<p>存储缓冲的思想：将内存访问与其他访问和计算重叠。隐藏内核中较长的写延迟，重新排序先读后存储。</p>
<h1 id="lecture-11-深度学习处理器">Lecture 11 深度学习处理器</h1>
<h2 id="为什么需要ai加速器">为什么需要AI加速器</h2>
<p>深度学习应用广泛，市场大</p>
<ul>
<li>AI for X：图像识别、语音处理、自然语言处理</li>
<li>平台：渗透到了云服务器和智能手机</li>
</ul>
<p>通用CPU/GPU处理人工神经网络效率低下（费电）</p>
<h3 id="处理器性能指标">处理器&amp;性能指标</h3>
<ul>
<li>CPU：Central Processing Unit（一个大学生）</li>
<li>GPU：Graphics Processing Unit（100个小学生）</li>
<li>DL Accelerator：Deep Learning Accelerator（偏科生）</li>
</ul>
<p><strong>指标</strong></p>
<ul>
<li>延时：输入数据到输出的时间</li>
<li>通用性：适合运行的应用程序范围</li>
<li>能效：单位计算量所消耗的能量</li>
<li>可迭代性：AI模型变化时的硬件适应能力</li>
</ul>
<h3 id="卷积层计算和访存特性">卷积层计算和访存特性</h3>
<p>计算他性：矩阵乘向量</p>
<p>计算特性：矩阵乘矩阵</p>
<p>访存特性：时空局部性、一维局部性</p>
<h3 id="激活函数计算和访存特性">激活函数计算和访存特性</h3>
<p>计算特性：向量运算</p>
<p>访存特性：向量顺序访问</p>
<h3 id="全连接层计算和访存特性">全连接层计算和访存特性</h3>
<p>计算特性：矩阵相乘</p>
<p>访存特性：顺序访问</p>
<h2 id="深度学习加速器设计思路">深度学习加速器设计思路</h2>
<p>怎么设计深度学习加速器</p>
<p>矩阵乘法计算量的占比高于90%</p>
<ul>
<li>支持矩阵、向量乘法</li>
<li>固定的内存访问模式</li>
</ul>
<h2 id="设计思路">设计思路</h2>
<ul>
<li>并行计算模块：使用能够符合特定领域加速需求最简单的并行形式，例如，对于矩阵运算的加速，单条指令直接支持小矩阵运算</li>
<li>Global
Buffer：使用专有的存储器来减少数据搬运的距离和开销，将复杂的cache设计替换成scratchpad
memory</li>
<li>简化控制模块：将缩减的高级微架构特性而节省出的面积，用于增加更多的运算单元或者片上的存储</li>
<li>量化：减少数据尺寸与类型来符合特定领域性能需求，例如，深度学习中，推理采用int8的量化方式</li>
<li>专用编程语言：使用DSA专用编程语言</li>
</ul>
<p>与CPU的设计相反，CPU花了很多的资源在辅助功能上。CPU的五级流水线只有EXE在计算。</p>
<h3 id="global-buffer">Global Buffer</h3>
<p>复杂的Cache，strided内存访问容易竞争同一个cache set</p>
<p>使用专有的存储器来减少数据搬运的距离和开销，将复杂的cache设计替换成scratchpad
memory</p>
<p>Cache：能耗高，芯片面积大，管理自动</p>
<p>Buffer：能耗低，芯片面积小，手动管理</p>
<h3 id="简化控制模块">简化控制模块</h3>
<p>将缩减的高级微架构特性而节省出的面积，用于增加更多的运算单元或者片上的存储。减少复杂的控制逻辑。与CPU相反，只需要简单的控制模块即可。</p>
<h3 id="量化">量化</h3>
<p>Why Low Precision Works for ML</p>
<ul>
<li>低精度很多时候不会影响准确性</li>
<li>不同的任务可能需要不同的精度</li>
</ul>
<h3 id="并行计算模块">并行计算模块</h3>
<p>使用能够符合特定领域加速需求最简单的并行形式，例如，对于矩阵运算的加速，单条指令直接支持小矩阵运算。指令内部的并行，指令之间并行少（因为控制模块简化），指令内部并行多。</p>
<h3 id="专用编程语言">专用编程语言</h3>
<p>用DSA专用语言进行编程</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_10.09.28.png" alt="截屏2023-05-25 10.09.28.png">
<figcaption aria-hidden="true">截屏2023-05-25 10.09.28.png</figcaption>
</figure>
<h1 id="lecture-12-ai-processors">Lecture 12 AI processors</h1>
<h2 id="深度学习加速器设计目标">深度学习加速器设计目标</h2>
<p>计算：很多的矩阵、向量的计算</p>
<p>访存：很对的外存访问，访问DRAM的能耗很大</p>
<p>目前的主要挑战：不足的算力，访存代价太大</p>
<h3 id="挑战">挑战</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2096.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>能耗分析：32bit的DRAM读比32bit的浮点乘法能耗高出两个数量级</p>
<p>目标任务：减少能耗高的操作，DRAM/SRAM Read、32bit Multiply</p>
<h2 id="减少内存访问">减少内存访问</h2>
<h3 id="为什么需要on-chip-buffer">为什么需要On-chip Buffer</h3>
<p>使用片上内存，将一些数据存在片上，减少外部内存访问。</p>
<p>假设我们直接访问外部内存，计算能耗不多，<strong>在内存的存取上使用了很多的能耗</strong>。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2097.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>最差的情况是所有的内存读写都是访问外部内存</p>
<ul>
<li>AlexNet：需要724M的MAC操作和2896M次外部内存访问</li>
</ul>
<h3 id="cache-or-buffer">Cache or Buffer</h3>
<p>AI加速器的主要目标：提高算力、降低功耗</p>
<p>隐含的意思：可以牺牲可编程性</p>
<p>Buffer的能耗低，芯片面积小，因为无需额外的控制逻辑，但是管理方式是手动的。</p>
<p>Cache的能耗高，芯片面积大，需要有tag、比较等管理逻辑，但是管理方式是自动的。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2098.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h3 id="programming-modelcache-vs-buffer">Programming Model：Cache vs
Buffer</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%2099.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>Cache编程简单，自动管理；Buffer编程需要人工管理buffer。</p>
<h3 id="how-to-use-buffer">How to use buffer</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20100.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>有三种buffer：</p>
<ul>
<li>L1：for MTE module，转运数据</li>
<li>UB：for Vector module，向量</li>
<li>L0 A/B：for Cube module，矩阵乘法</li>
</ul>
<p>层级越低，能耗越小</p>
<p><strong>编程时需要注意数据到底在哪个Buffer中</strong></p>
<h2 id="减少global-buffer访问">减少Global Buffer访问</h2>
<ul>
<li><p>recall：FF vs SRAM vs DRAM vs Flash</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20101.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure></li>
</ul>
<p>Problem：Global Buffer access is expensive</p>
<p>Solution：增加寄存器文件利用</p>
<p><strong>AI芯片里面用的Buffer是SRAM</strong></p>
<h3 id="weight-stationary">Weight Stationary</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20102.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Key idea
<ul>
<li>最大程度减少从Global Buffer读取weight，将weight放进寄存器中</li>
<li>广播Activations和沿着PE方向上累加Psum</li>
</ul></li>
</ul>
<h3 id="output-stationary">Output Stationary</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20103.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Key idea
<ul>
<li>最大程度减少从GB中读取和存储Psum，尽量把Psum留在PE内部</li>
<li>广播weight和沿着PE方向上复用Activation</li>
</ul></li>
</ul>
<h3 id="input-stationary">Input Stationary</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20104.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Key idea
<ul>
<li>最大程度减少从GB中读取Activation，尽量把Activation留在PE内部</li>
<li>并行读取weight和沿着PE方向上累加Activation</li>
</ul></li>
</ul>
<h3 id="row-stationary">Row Stationary</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20105.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Key idea
<ul>
<li>从GB读出Filter中的一行和Activation的一个滑窗，留在PE内部</li>
<li>尽量减少从GB的整体读出量，而不是一个维度的</li>
</ul></li>
</ul>
<h2 id="增加计算">增加计算</h2>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20106.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>AI相关计算量里，矩阵乘法计算量的占比高于90%。尽可能使用定制计算单元，提升计算密度。</p>
<h3 id="矩阵乘法单元">矩阵乘法单元</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20107.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Scalar：标量实现
<ul>
<li>周期数：16<em>16</em>16</li>
<li>每周期内存访问量：2 rd，1/16 wr</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20108.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Vector：向量操作
<ul>
<li>周期数：16*16</li>
<li>每周期内存访问量：2*16 rd，1 wr</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20109.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Matrix：矩阵操作
<ul>
<li>周期数：1</li>
<li>每周期内存访问量：2<em>16</em>16 rd，16*16 wr</li>
</ul></li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20110.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p><strong>算力密度逐渐变高，但是灵活度逐渐变低</strong></p>
<h3 id="计算模块">计算模块</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20111.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Cube模块（算力核心）
<ul>
<li>单指令处理小矩阵乘法</li>
</ul></li>
<li>Vector模块（算力核心）
<ul>
<li>单指令处理向量操作，如activation激活</li>
</ul></li>
</ul>
<h2 id="常见ai加速器分析比较">常见AI加速器分析比较</h2>
<h3 id="华为ascend">华为Ascend</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_10.58.31.png" alt="截屏2023-05-25 10.58.31.png">
<figcaption aria-hidden="true">截屏2023-05-25 10.58.31.png</figcaption>
</figure>
<h4 id="ai-core内部的结构">AI Core内部的结构</h4>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20112.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h4 id="cube模块矩阵运算算力担当">Cube模块（矩阵运算，算力担当）</h4>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.05.00.png" alt="截屏2023-05-25 11.05.00.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.05.00.png</figcaption>
</figure>
<ul>
<li>矩阵乘运算单元Cube：一拍完成一个fp的2个16<em>16矩阵相乘。C=A</em>B，如果是int8输入，则一拍可以完成16<em>32与32</em>16的矩阵运算</li>
<li>累加器Accumulator：把当前矩阵乘结果和前次计算的中间结果相加（C=A*B+C），可以用于完成卷集中的加bias的操作</li>
<li>L0A/L0B/L0C
Buffer：L0A存储矩阵乘的左矩阵数据，L0B存储矩阵乘的右矩阵数据，L0C存储矩阵乘的结果和中间结果</li>
<li>A/B DFF：数据寄存器，缓存当前计算的16*16的左/右矩阵</li>
<li>Accum DF F：数据寄存器，缓存当前计算的16*16结果矩阵</li>
</ul>
<h3 id="vector模块向量运算多面手">Vector模块（向量运算，多面手）</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.12.28.png" alt="截屏2023-05-25 11.12.28.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.12.28.png</figcaption>
</figure>
<ul>
<li>向量运算单元Vector
Unit：覆盖各种基本的计算类型和许多定制的计算类型，主要包括FP16/FP32/int32/int8等数据类型的计算，支持连续或者固定间隔寻址；或者VA寄存器寻址（不规则向量运算）</li>
<li>SIMD长度：一条Vector指令可以完成两个128长度fp16类型的向量相加/乘，
或者64个fp32/int32类型的向量相加/乘</li>
<li>Unified Buffer(UB)： 保存Vector运算的源操作数和目的操作数；
一般要求32Byte对齐</li>
<li>数据从L0C-&gt;UB，需要以Vector
Unit作为中转，并可以随数据搬运完成一些RELU/数据格式转换等操作</li>
</ul>
<h3 id="scalar模块标量运算司令部">Scalar模块（标量运算，司令部）</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.19.23.png" alt="截屏2023-05-25 11.19.23.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.19.23.png</figcaption>
</figure>
<ul>
<li>Scalar Unit：
负责完成AICore中的标量运算，功能上可以看做一个小CPU；完成整个程序的循环控制、分支判断、CUBE/Vector等指令的地址和参数计算以及基本的算术运算等‘</li>
<li>Unified Buffer or Scalar Buffer: 晟腾310/910 Scalar
Unit不能直接访问外面的DDR/HBM,
需要预留UB的一部分(310)或者使用专门的Scalar Buffer(910)用作Scalar
Unit的堆栈空间</li>
<li>GPR：通用寄存器，目前包含32个通用寄存器</li>
<li>SPR:
专用寄存器，为了支持指令集一些指令的特殊需要，Davinci设计了许多专用寄存器，比如CoreID,
BLOCKID, VA, STATUS, CTRL等寄存器</li>
</ul>
<h3 id="mtebiu和片上高速存储buffer">MTE/BIU和片上高速存储Buffer</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.20.57.png" alt="截屏2023-05-25 11.20.57.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.20.57.png</figcaption>
</figure>
<ul>
<li>BIU (Bus Interface Unit): AICore
的“大门”，与总线交互的接口。AICore从外部（L2/DDR/HBM）读取、写入数据的出入口。负责把AICore的读写请求转换为总线上的请求并完成协议交互等工作。</li>
<li>MTE (Memory Transfer Unit): 也被称作 LSU (Load Store Unit),
负责AICore内部数据在不同Buffer之间的读写管理，以及完成一些格式转换的操作，比如padding,
转置, Img2Col, 解压等</li>
<li>L1 Buffer:
AICore内最大的一块数据中转区(1MB)，可以用来暂存AICore需要反复使用的一些数据从而减少从总线读写；
Img2col操作等MTE的数据格式转换功能需源数据必须位于L1 Buffer</li>
<li>L0A/L0B/L0C/UB/Scalar Buffer: 前面已介绍</li>
</ul>
<h3 id="指令和控制系统">指令和控制系统</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.24.03.png" alt="截屏2023-05-25 11.24.03.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.24.03.png</figcaption>
</figure>
<p>指令从BIU进来，到I
cache中再进入指令处理队列进行分布，针对不同指令的计算属性，分发到不同的对应的Queue中</p>
<ul>
<li>I Cache: AICore内部的指令Cache, 具有指令预取功能</li>
<li>Scalar PSQ: Scalar 指令处理队列</li>
<li>Instr Dispatch: 指令分发模块, CUBE/Vector/MTE 指令经过Scalar
PSQ处理之后，地址、参数等要素都已经配置好，之后Instr
Dispatch单元根据指令的类型，将CUBE/Vector/MTE指令分别分发到对应的指令队列等待相应的执行单元调度执行</li>
<li>Cube/Vector/MTE1/MTE2/MTE3 Queue:
Cube/Vector/MTE1/MTE2/MTE3指令队列；同一个队列里的指令顺序执行；不同队列之间，可以并行执行</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20113.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<h2 id="google-tpu">Google TPU</h2>
<h3 id="systolic-arrays">Systolic Arrays</h3>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20114.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<ul>
<li>Goal：设计一个加速器
<ul>
<li>简单，规则的设计(保持#独特的部分小而规则)</li>
<li>高并发→高性能</li>
<li>均衡的计算和I/O(内存)带宽</li>
</ul></li>
<li>Idea：用一个Processing Element PE的regular
Array替换PE，并仔细编排PEs之间的数据流
<ul>
<li>这样，它们在将输入数据输出到存储器之前共同对其进行转换</li>
</ul></li>
<li>Benefit：最大限度地提高从内存中取出的单个数据元素的计算量</li>
</ul>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_11.57.35.png" alt="截屏2023-05-25 11.57.35.png">
<figcaption aria-hidden="true">截屏2023-05-25 11.57.35.png</figcaption>
</figure>
<h3 id="ai加速器中的systolic-arrays">AI加速器中的Systolic Arrays。</h3>
<p><strong>二维的Systolic array</strong></p>
<p>每一个PE是一个cell，接受来自左方和上方的输入，右方和下方传递左方和上方的值</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/Untitled%20115.png" alt="Untitled">
<figcaption aria-hidden="true">Untitled</figcaption>
</figure>
<p>矩阵的数据传输时，是有时间顺序的，类似于一个平行四边形的方式传入</p>
<p><strong>Example</strong></p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.03.53.png" alt="截屏2023-05-25 12.03.53.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.03.53.png</figcaption>
</figure>
<ul>
<li><p>各个周期的计算情况</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.05.29.png" alt="截屏2023-05-25 12.05.29.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.05.29.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.05.39.png" alt="截屏2023-05-25 12.05.39.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.05.39.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.05.57.png" alt="截屏2023-05-25 12.05.57.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.05.57.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.06.15.png" alt="截屏2023-05-25 12.06.15.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.06.15.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.06.26.png" alt="截屏2023-05-25 12.06.26.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.06.26.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.06.36.png" alt="截屏2023-05-25 12.06.36.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.06.36.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.06.46.png" alt="截屏2023-05-25 12.06.46.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.06.46.png</figcaption>
</figure>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/%25E6%2588%25AA%25E5%25B1%258F2023-05-25_12.06.59.png" alt="截屏2023-05-25 12.06.59.png">
<figcaption aria-hidden="true">截屏2023-05-25 12.06.59.png</figcaption>
</figure></li>
</ul>
<h2 id="cambricon寒武纪">Cambricon寒武纪</h2>
<ul>
<li>Cambricon尝试解决两个问题
<ul>
<li>How to increase performance/power ratio？如何提升性能/功耗比例</li>
<li>How to increase programmability？如何提升可编程性</li>
</ul></li>
<li>目标：
<ul>
<li>设计一个高性能比，高可编程性的深度学习加速器</li>
</ul></li>
<li>DLP-S AI加速器</li>
</ul>
<h3 id="overall-architecture-of-dlp-s">Overall Architecture of
DLP-S</h3>
<ul>
<li>Control Module
<ul>
<li>IFU：Instruction Fetch Unit</li>
<li>IDU：Instruction Decode Unit</li>
</ul></li>
<li>Compute Unit
<ul>
<li>VFU：Vector Function Unit</li>
<li>MFU：Matrix Function Unit</li>
</ul></li>
<li>SRAM Unit
<ul>
<li>WRAM：Weight RAM</li>
<li>NRAM：Neuron RAM</li>
<li>DMA：Direct Memory Access</li>
</ul></li>
</ul>
<p>控制模块</p>
<ul>
<li>一些比较简单的控制流</li>
<li>IF、ID、Issue模块</li>
<li>IFU
<ul>
<li>地址生成模块，生成PC</li>
<li>指令Cache，Hit了直接把指令放队列</li>
<li>refill buffer，cache miss了要用这个从DRAM中读取指令</li>
<li>指令Queue</li>
</ul></li>
<li>IDU
<ul>
<li>Decoder，解码指令，并且根据指令内容</li>
<li>ALU</li>
<li>Issue Queue
<ul>
<li>Control Inst</li>
<li>Compute Inst</li>
<li>Memory Access Inst</li>
</ul></li>
</ul></li>
<li>Instruction Issue Queue
<ul>
<li>执行顺序：
<ul>
<li>控制指令队列</li>
<li>运算指令队列</li>
<li>访存指令队列</li>
</ul></li>
<li>Between Queue：是乱序的，inserting SYNC instruction between
instruction queues</li>
<li>In Queue：是顺序执行的</li>
</ul></li>
</ul>
<p>计算模块</p>
<ul>
<li>Matrix Inst</li>
<li>Vector Inst</li>
</ul>
<p>SRAM模块</p>
<ul>
<li>权重SRAM</li>
<li>激活SRAM</li>
<li>分开管理更加高效</li>
<li>SRAM都是用DMA操作</li>
</ul>
<h3 id="数据流">数据流</h3>
<p>推理的过程：</p>
<ul>
<li>神经元流
<ul>
<li>DRAM-NRAM-VPU-（MFU-VFU-）NRAM-DRAM</li>
<li></li>
</ul></li>
<li>权重tensor数据流</li>
<li>Execution Flow
<ul>
<li>Step1：IFU通过DMA从DRAM中</li>
</ul></li>
</ul>
<h3 id="dlp-isa">DLP ISA</h3>
<ul>
<li>控制指令：跳转、条件分支</li>
<li>数据转移指令：矩阵、向量、标量</li>
<li>计算指令：矩阵、向量、标量</li>
<li>逻辑运算指令：向量、标量</li>
</ul>
<h1 id="lecture-13-parallel-training">Lecture 13 Parallel Training</h1>
<ul>
<li><p>overview of Architecture of DLP-S 寒武纪芯片的架构</p>
<ul>
<li>控制单元
<ul>
<li>IF、ID单元</li>
</ul></li>
<li>计算单元
<ul>
<li>向量、矩阵计算单元</li>
</ul></li>
<li>SRAM单元
<ul>
<li>权重SRAM</li>
<li>Neuron RAM</li>
<li>DMA，都是Direct Memory Access</li>
</ul></li>
</ul></li>
<li><p>AI Architecture</p>
<ul>
<li>Parallel Training</li>
<li>AI Framework
<ul>
<li>mindspore、pytorch、tensorflow</li>
</ul></li>
<li>AI Runtime
<ul>
<li>计算加速库CANN</li>
</ul></li>
<li>AI Chip</li>
</ul></li>
<li><p>AI+科学计算</p>
<p>科学计算的核心问题是微分方程求解，算力消耗巨大，大规模求解器垄断</p></li>
</ul>
<h3 id="parallel-training">Parallel Training</h3>
<ul>
<li>模型训练的例子
<ul>
<li>比如说我们训练一个三层的全连接网络</li>
<li>随机初始化权重</li>
<li>mini-batch的迭代训练
<ul>
<li>前向</li>
<li>反向</li>
<li>权重更新</li>
<li>minibatch为1
<ul>
<li>每一层输入：向量</li>
<li>输出：向量</li>
<li>权重参数：矩阵</li>
<li>操作：向量乘以矩阵再进行激活如ReLU</li>
</ul></li>
<li>minibatch不为1
<ul>
<li>每一层输入：矩阵</li>
<li>输出：矩阵</li>
<li>权重参数：矩阵</li>
<li>操作：矩阵乘以矩阵再进行激活如ReLU</li>
</ul></li>
</ul></li>
<li>Loss function
<ul>
<li>计算误差，我们的目标是减小误差</li>
<li>误差反向传播，计算权重梯度和激活函数的梯度</li>
</ul></li>
<li>权重更新
<ul>
<li>SGD
<ul>
<li>W=W-lr*dW</li>
</ul></li>
<li>Momentum
<ul>
<li>v=μv-lr*dW</li>
<li>W=W+v</li>
</ul></li>
<li>AdamW</li>
</ul></li>
</ul></li>
</ul>
<h2 id="parallelism-taxonomy">Parallelism Taxonomy</h2>
<ul>
<li>Parallel Training
<ul>
<li>Data Parallel</li>
<li>Model Parallel
<ul>
<li>Intra Layer</li>
<li>Inter-layer</li>
</ul></li>
</ul></li>
</ul>
<h3 id="data-parallel-training">Data Parallel Training</h3>
<p>有很多的worker（GPU），每一个worker都是一个神经网络的一个copy</p>
<p>数据部分则是worker平分数据</p>
<p>比如有4个worker的情况：</p>
<ul>
<li>stronge
scaling：minibatch不变的，原来一个X是4<em>16的，现在每个worker只用算1</em>16的矩阵</li>
<li>weak scaling：minibatch原来是8的，现在每个worker的minibatch是2</li>
</ul>
<ol type="1">
<li>反向传播时需要聚合每个worker的dW</li>
<li>梯度更新：
<ul>
<li>N个worker加和梯度：算术平均</li>
<li>每一个worker更新权重</li>
</ul>
每一个worker的梯度进行加和平均，再对每一个worker的权重矩阵进行更新</li>
</ol>
<h3 id="allreduce-implementation-choices">AllReduce Implementation
Choices</h3>
<p>[<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/69797852">https://zhuanlan.zhihu.com/p/69797852</a>]</p>
<p>同步更新模式下，所有GPU在同一时间点与参数服务器交换、融合梯度；</p>
<p>异步更新模式下，所有GPU各自独立与参数服务器通信，交换、融合梯度。</p>
<ul>
<li>异步更新通信效率高速度快，但往往收敛不佳，因为一些速度慢的节点总会提供过时、错误的梯度方向。可通过上一篇介绍的Stale
Synchronous Parallel Parameter Server方法缓解该问题。</li>
<li>同步更新通信效率低，通常训练慢，但训练收敛稳定，因为同步更新基本等同于单卡调大
的batch size 训练。</li>
</ul>
<p>但是传统的同步更新方法（各个gpu卡算好梯度，求和算平均的方式），在融合梯度时，会产生巨大的通信数据量，这种通信压力往往在模型参数量很大时，显得很明显。因此我们需要找到一种方法，来解决同步更新的网络瓶颈问题。其中最具代表性的一种方法就是：ring
all-reduce。</p>
<ul>
<li><p>Ring AllReduce</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/v2-d67dc7234289db3aab052c34476775d5_720w.webp" alt="v2-d67dc7234289db3aab052c34476775d5_720w.webp">
<figcaption aria-hidden="true">v2-d67dc7234289db3aab052c34476775d5_720w.webp</figcaption>
</figure>
<ul>
<li>每个 GPU 只从左邻居接受数据、并发送数据给右邻居。</li>
</ul>
<p>算法主要分两步：</p>
<ol type="1">
<li><p>scatter-reduce：会逐步交换彼此的梯度并融合，最后每个 GPU
都会包含完整融合梯度的一部分。即每一个GPU都会向下一个GPU发送梯度信息，下一个GPU会进行融合，再发给一下个，形成一个环。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/v2-6210e72683d08a2410fa8d279fca7e2e_720w.webp" alt="v2-6210e72683d08a2410fa8d279fca7e2e_720w.webp">
<figcaption aria-hidden="true">v2-6210e72683d08a2410fa8d279fca7e2e_720w.webp</figcaption>
</figure></li>
<li><p>allgather：GPU会逐步交换彼此不完整的融合梯度，最后所有 GPU
都会得到完整的融合梯度。GPU传递已经融合梯度信息。</p>
<figure>
<img src="/Learning/2023-Spring-%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%8A%AF%E7%89%87%E4%B8%8E%E7%B3%BB%E7%BB%9F%2075738bcee30b4ee0b546d5e60d37dfbc/v2-f177475af4abea7ba36a8cc5b84684ba_720w.webp" alt="v2-f177475af4abea7ba36a8cc5b84684ba_720w.webp">
<figcaption aria-hidden="true">v2-f177475af4abea7ba36a8cc5b84684ba_720w.webp</figcaption>
</figure></li>
</ol>
<ul>
<li>通信代价分析：每个 GPU 在Scatter Reduce 阶段，接收 N-1 次数据，N 是
GPU 数量；每个 GPU 在allgather 阶段，接收 N-1 次 数据；每个 GPU 每次发送
K/N 大小数据块，K 是总数据大小；所以，Data Transferred=2(N−1)*K/N ，随着
GPU 数量 N 增加，总传输量恒定。也就是理论上，随着gpu数量的增加，ring
all-reduce有线性加速能力。
<ul>
<li>每个2(N-1)步，每一步都要一次同步syncs</li>
</ul></li>
</ul></li>
</ul>
<h3 id="model-parallel-training">Model Parallel Training</h3>
<ul>
<li>Intra Layer
<ul>
<li>每一个worker训练几个层</li>
</ul></li>
<li>Inter Layer
<ul>
<li>每一个worker训练层的一部分</li>
</ul></li>
</ul>
<h3 id="pipeline-parallel-training">Pipeline Parallel Training</h3>
<h3 id="inter-layer-parallel">Inter Layer Parallel</h3>
<ul>
<li>按行划分</li>
<li>按列划分</li>
<li>交替使用行列划分，可以减少通信次数</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>Data Parallel
<ul>
<li>Allreduce of weights</li>
<li>Can be overlapped with computation</li>
</ul></li>
<li>Pipeline Parallel
<ul>
<li>Point-wise communication of activations and activation
gradients</li>
<li>Hard to overlap with computation</li>
<li>Hard to load-balance</li>
</ul></li>
<li>Intra-Layer Parallel
<ul>
<li>Allgather，reduce scatter of activations and activation
gradients</li>
</ul></li>
</ul>
<h3 id="others">Others</h3>
<ul>
<li>Memory Size for a Huge Model
<ul>
<li>GPT3-175B
<ul>
<li>Optimizer优化器：3259GB</li>
<li>Activation激活，没有checkpoint：360GB</li>
<li>Activation激活，有checkpoint：3.75GB</li>
</ul></li>
</ul></li>
</ul>


        <hr>
        <!-- Pager -->
        <ul class="pager">
          
          <li class="previous">
            <a href="/Pytoch学习/2.Tensor/" data-toggle="tooltip" data-placement="top" title="【Pytorch学习2】Tensor">&larr; Previous Post</a>
          </li>
          
          
          <li class="next">
            <a href="/Learning/操作系统/" data-toggle="tooltip" data-placement="top" title="操作系统">Next Post &rarr;</a>
          </li>
          
        </ul>

        
        <!-- tip start -->
        <!-- tip -->
<!-- tip start -->
<div class="tip">
  <p>
    
      如果您喜欢此博客或发现它对您有用，则欢迎对此发表评论。 也欢迎您共享此博客，以便更多人可以参与。 如果博客中使用的图像侵犯了您的版权，请与作者联系以将其删除。 谢谢 ！
    
  </p>
</div>
<!-- tip end -->

        <!-- tip end -->
        

        
        <!-- Sharing Srtart -->
        <!-- Social Social Share Post -->
<!-- Docs:https://github.com/overtrue/share.js -->

<div class="social-share" data-initialized="true" data-disabled="tencent ,douban ,qzone ,linkedin ,facebook ,google ,diandian" data-wechat-qrcode-helper="" align="center">
  <ul class="list-inline text-center social-share-ul">
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-twitter">
        <i class="fa fa-twitter fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a class="social-share-icon icon-wechat">
        <i class="fa fa-weixin fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-weibo">
        <i class="fa fa-weibo fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon icon-qq">
        <i class="fa fa-qq fa-1x" aria-hidden="true"></i>
      </a>
    </li>
    <li class="social-share-li">
      <a target="_blank" class="social-share-icon" href="mailto:?subject=AIChip&body=Hi,I found this website and thought you might like it http://Hualingz.cn/Learning/2023-Spring-人工智能芯片与系统 75738bcee30b4ee0b546d5e60d37dfbc/">
        <i class="fa fa-envelope fa-1x" aria-hidden="true"></i>
      </a>
    </li>
  </ul>
</div>

<!-- css & js -->
<!-- <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/css/share.min.css"> -->
<script defer="defer" async="true" src="https://cdnjs.cloudflare.com/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script>

        <!-- Sharing End -->
        
        <hr>

        <!-- comments start -->
        <!-- 1. gitalk comment -->

  <!-- gitalk start -->
  <!-- Docs:https://github.com/gitalk/gitalk/blob/master/readme-cn.md -->

  <div id="gitalk-container"></div>

  
    <!-- <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.js"></script> -->
    <script src="/js/comment/gitalk.js"></script>
  

  <script>
    var gitalk = new Gitalk({
      clientID: '',
      clientSecret: '',
      repo: '',
      owner: '',
      admin: '',
      id: 'Fri Jul 21 2023 16:28:59 GMT+0800', // Ensure uniqueness and length less than 50
      distractionFreeMode: false, // Facebook-like distraction free mode
      perPage: 10,
      pagerDirection: 'last',
      createIssueManually: false,
      language: 'en',
      proxy: 'https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token'
    });
    gitalk.render('gitalk-container');

    var gtFolded = () => {
      setTimeout(function () {
        let markdownBody = document.getElementsByClassName("markdown-body");
        let list = Array.from(markdownBody);
        list.forEach(item => {
          if (item.clientHeight > 250) {
            item.classList.add('gt-comment-body-folded');
            item.style.maxHeight = '250px';
            item.title = 'Click to Expand';
            item.onclick = function () {
              item.classList.remove('gt-comment-body-folded');
              item.style.maxHeight = '';
              item.title = '';
              item.onclick = null;
            };
          }
        })
      }, 800);
    }
  </script>

  <!-- gitalk end -->


<!-- 2. gitment comment -->


<!-- 3. disqus comment -->


        <!-- comments end -->
        <hr>

      </div>

      <!-- Catalog: Tabe of Content -->
      <!-- Table of Contents -->

    
      <aside id="sidebar">
        <div id="toc" class="toc-article">
        <strong class="toc-title">目录</strong>
        
          <ol class="toc-nav"><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-1-intro"><span class="toc-nav-number">1.</span> <span class="toc-nav-text">Lecture 1 Intro</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B8%80%E4%BA%9B%E5%AE%9A%E7%90%86"><span class="toc-nav-number">1.0.1.</span> <span class="toc-nav-text">一些定理</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#amdhal-law"><span class="toc-nav-number">1.0.2.</span> <span class="toc-nav-text">Amdhal Law</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#roofline-model"><span class="toc-nav-number">1.0.3.</span> <span class="toc-nav-text">Roofline Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#littles-lawlwbuffer-size-throughputlatency"><span class="toc-nav-number">1.0.4.</span> <span class="toc-nav-text">**Little‘s
Law：L&#x3D;W(buffer size &#x3D; throughput*latency)**</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-2-neuman-isa-cpu"><span class="toc-nav-number">2.</span> <span class="toc-nav-text">Lecture 2 Neuman &amp; ISA &amp;
CPU</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#von-neuman-model"><span class="toc-nav-number">2.1.</span> <span class="toc-nav-text">Von Neuman Model</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#memory"><span class="toc-nav-number">2.1.1.</span> <span class="toc-nav-text">Memory</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#processing-unitpu"><span class="toc-nav-number">2.1.2.</span> <span class="toc-nav-text">Processing Unit（PU）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#input-and-output"><span class="toc-nav-number">2.1.3.</span> <span class="toc-nav-text">Input and Output</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#control-unit"><span class="toc-nav-number">2.1.4.</span> <span class="toc-nav-text">Control Unit</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#instruction-set-architectureisa"><span class="toc-nav-number">2.2.</span> <span class="toc-nav-text">Instruction Set
Architecture(ISA)</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#operate-instructions"><span class="toc-nav-number">2.2.1.</span> <span class="toc-nav-text">Operate Instructions</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#datamovement-instructions"><span class="toc-nav-number">2.2.2.</span> <span class="toc-nav-text">DataMovement Instructions</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#control-flow-instructions"><span class="toc-nav-number">2.2.3.</span> <span class="toc-nav-text">Control Flow Instructions</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#instructionprocessingcycle"><span class="toc-nav-number">2.3.</span> <span class="toc-nav-text">Instruction（Processing）Cycle</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#microarchitecture"><span class="toc-nav-number">2.3.1.</span> <span class="toc-nav-text">Microarchitecture</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#single-cycle-cpu%E5%8D%95%E5%91%A8%E6%9C%9Fcpu"><span class="toc-nav-number">2.3.2.</span> <span class="toc-nav-text">Single-Cycle CPU：单周期CPU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%8D%95%E5%91%A8%E6%9C%9F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-nav-number">2.3.3.</span> <span class="toc-nav-text">单周期的执行步骤</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#multi-cycle-cpu%E5%A4%9A%E5%91%A8%E6%9C%9Fcpu"><span class="toc-nav-number">2.3.4.</span> <span class="toc-nav-text">Multi-Cycle CPU：多周期CPU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#pipeline-cpu%E6%B5%81%E6%B0%B4%E7%BA%BFcpu"><span class="toc-nav-number">2.3.5.</span> <span class="toc-nav-text">Pipeline CPU：流水线CPU</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-3-pipeline-hazard-record-buffer"><span class="toc-nav-number">3.</span> <span class="toc-nav-text">Lecture 3 Pipeline
Hazard &amp; Record Buffer</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#pipeline-hazard"><span class="toc-nav-number">3.1.</span> <span class="toc-nav-text">Pipeline Hazard</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#structural-hazard"><span class="toc-nav-number">3.2.</span> <span class="toc-nav-text">Structural Hazard</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#data-hazard"><span class="toc-nav-number">3.2.1.</span> <span class="toc-nav-text">Data Hazard</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#control-hazard"><span class="toc-nav-number">3.2.2.</span> <span class="toc-nav-text">Control Hazard</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#reorder-buffer"><span class="toc-nav-number">3.3.</span> <span class="toc-nav-text">Reorder Buffer</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#pipeline-cpuideal-vs-realistic"><span class="toc-nav-number">3.3.1.</span> <span class="toc-nav-text">Pipeline CPU：Ideal vs
Realistic</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#for-multi-cycle-execution"><span class="toc-nav-number">3.3.2.</span> <span class="toc-nav-text">For Multi-cycle Execution</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#for-exception-and-interrupt"><span class="toc-nav-number">3.3.3.</span> <span class="toc-nav-text">For Exception and Interrupt</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#for-false-dependenceswawwar"><span class="toc-nav-number">3.3.4.</span> <span class="toc-nav-text">For False
Dependences（WAW&amp;WAR）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E4%BF%9D%E7%95%99%E9%A1%BA%E5%BA%8F%E8%AF%AD%E4%B9%89"><span class="toc-nav-number">3.4.</span> <span class="toc-nav-text">保留顺序语义</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#reorder-bufferrob"><span class="toc-nav-number">3.4.1.</span> <span class="toc-nav-text">Reorder Buffer（ROB）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#rob%E7%9A%84entry"><span class="toc-nav-number">3.4.2.</span> <span class="toc-nav-text">ROB的entry</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#in-order-pipeline-with-reorder-buffer"><span class="toc-nav-number">3.5.</span> <span class="toc-nav-text">In-Order Pipeline with
Reorder Buffer</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-4-tomasulo-simd"><span class="toc-nav-number">4.</span> <span class="toc-nav-text">Lecture 4 Tomasulo &amp; SIMD</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#tomasulo%E7%AE%97%E6%B3%95"><span class="toc-nav-number">4.1.</span> <span class="toc-nav-text">Tomasulo算法</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#in-order-dispatch"><span class="toc-nav-number">4.1.1.</span> <span class="toc-nav-text">In-order Dispatch</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#reservation-station"><span class="toc-nav-number">4.2.</span> <span class="toc-nav-text">Reservation Station</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#tomaulo%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0"><span class="toc-nav-number">4.3.</span> <span class="toc-nav-text">Tomaulo算法实现</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#enabling-ooo-execution"><span class="toc-nav-number">4.3.1.</span> <span class="toc-nav-text">Enabling OoO Execution</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#register-rename"><span class="toc-nav-number">4.3.2.</span> <span class="toc-nav-text">Register Rename</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%89%A7%E8%A1%8C%E6%AD%A5%E9%AA%A4"><span class="toc-nav-number">4.3.3.</span> <span class="toc-nav-text">执行步骤</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#question"><span class="toc-nav-number">4.3.4.</span> <span class="toc-nav-text">Question</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#out-of-order-execution-with-precise-exception"><span class="toc-nav-number">4.4.</span> <span class="toc-nav-text">Out-of-Order
Execution with Precise Exception</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-5-superscalarsimdmultithread"><span class="toc-nav-number">5.</span> <span class="toc-nav-text">Lecture 5
Superscalar&amp;SIMD&amp;Multithread</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#recalltomasula-algorithm"><span class="toc-nav-number">5.0.1.</span> <span class="toc-nav-text">Recall：Tomasula Algorithm</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#superscalar"><span class="toc-nav-number">5.1.</span> <span class="toc-nav-text">Superscalar</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#vector-ins"><span class="toc-nav-number">5.2.</span> <span class="toc-nav-text">Vector Ins</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#sisd"><span class="toc-nav-number">5.3.</span> <span class="toc-nav-text">SISD</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#simd"><span class="toc-nav-number">5.4.</span> <span class="toc-nav-text">SIMD</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#mimd"><span class="toc-nav-number">5.5.</span> <span class="toc-nav-text">MIMD</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#misd"><span class="toc-nav-number">5.6.</span> <span class="toc-nav-text">MISD</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#support-vector-insns"><span class="toc-nav-number">5.7.</span> <span class="toc-nav-text">Support Vector Insns</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#programmer-visible-architectural-states"><span class="toc-nav-number">5.7.1.</span> <span class="toc-nav-text">Programmer Visible
Architectural States</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#roofline-model-for-simd-cpu"><span class="toc-nav-number">5.8.</span> <span class="toc-nav-text">Roofline Model for SIMD CPU</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#fine-grained-multithreading"><span class="toc-nav-number">5.9.</span> <span class="toc-nav-text">Fine-Grained Multithreading</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#advantages"><span class="toc-nav-number">5.9.1.</span> <span class="toc-nav-text">Advantages</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#disadvantages"><span class="toc-nav-number">5.9.2.</span> <span class="toc-nav-text">Disadvantages</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#multi-core"><span class="toc-nav-number">5.10.</span> <span class="toc-nav-text">Multi-Core</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#why-multi-core"><span class="toc-nav-number">5.11.</span> <span class="toc-nav-text">Why Multi-Core</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-6-memory"><span class="toc-nav-number">6.</span> <span class="toc-nav-text">Lecture 6 Memory</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#memory-overview"><span class="toc-nav-number">6.1.</span> <span class="toc-nav-text">Memory Overview</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#sram"><span class="toc-nav-number">6.2.</span> <span class="toc-nav-text">SRAM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#memory-arrays"><span class="toc-nav-number">6.2.1.</span> <span class="toc-nav-text">Memory Arrays</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#bit-line-word-line"><span class="toc-nav-number">6.2.2.</span> <span class="toc-nav-text">Bit Line &amp; Word Line</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#memory-banking"><span class="toc-nav-number">6.2.3.</span> <span class="toc-nav-text">Memory Banking</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#hbmdram"><span class="toc-nav-number">6.3.</span> <span class="toc-nav-text">HBM&amp;DRAM</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#main-memory-system"><span class="toc-nav-number">6.3.1.</span> <span class="toc-nav-text">Main Memory System</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#dram"><span class="toc-nav-number">6.3.2.</span> <span class="toc-nav-text">DRAM</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#bank"><span class="toc-nav-number">6.3.3.</span> <span class="toc-nav-text">Bank</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#architecture-of-dram"><span class="toc-nav-number">6.3.4.</span> <span class="toc-nav-text">Architecture of DRAM</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#transferring-a-cache-block"><span class="toc-nav-number">6.3.5.</span> <span class="toc-nav-text">Transferring a cache block</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#address-bits-of-memory"><span class="toc-nav-number">6.3.6.</span> <span class="toc-nav-text">Address Bits of Memory</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#ssd"><span class="toc-nav-number">6.4.</span> <span class="toc-nav-text">SSD</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#disk"><span class="toc-nav-number">6.5.</span> <span class="toc-nav-text">Disk</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-7-graphics-processing-units"><span class="toc-nav-number">7.</span> <span class="toc-nav-text">Lecture 7 Graphics
Processing Units</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#gpu"><span class="toc-nav-number">7.1.</span> <span class="toc-nav-text">GPU</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#keymessages"><span class="toc-nav-number">7.1.1.</span> <span class="toc-nav-text">KeyMessages</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cpu-vs-gpu"><span class="toc-nav-number">7.1.2.</span> <span class="toc-nav-text">CPU vs GPU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#programming-model"><span class="toc-nav-number">7.1.3.</span> <span class="toc-nav-text">Programming Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#gpus-are-simd-engines-underneath"><span class="toc-nav-number">7.1.4.</span> <span class="toc-nav-text">GPUs are SIMD Engines
Underneath</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#nvidia-geforce-gtx-285"><span class="toc-nav-number">7.1.5.</span> <span class="toc-nav-text">NVIDIA GeForce GTX 285</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#programming-model-1"><span class="toc-nav-number">7.2.</span> <span class="toc-nav-text">Programming Model</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#sisd-1"><span class="toc-nav-number">7.2.1.</span> <span class="toc-nav-text">SISD</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#simd-1"><span class="toc-nav-number">7.2.2.</span> <span class="toc-nav-text">SIMD</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#multithreadspmd"><span class="toc-nav-number">7.2.3.</span> <span class="toc-nav-text">Multithread：SPMD</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#gpu-programming-example"><span class="toc-nav-number">7.3.</span> <span class="toc-nav-text">GPU Programming Example</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#traditional-program-structure"><span class="toc-nav-number">7.3.1.</span> <span class="toc-nav-text">Traditional Program
Structure</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#sample-vector-add"><span class="toc-nav-number">7.3.2.</span> <span class="toc-nav-text">Sample: Vector Add</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#gpu%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-nav-number">7.3.3.</span> <span class="toc-nav-text">GPU的架构</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-8-gpu-optimization"><span class="toc-nav-number">8.</span> <span class="toc-nav-text">Lecture 8 GPU optimization</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#recall"><span class="toc-nav-number">8.0.1.</span> <span class="toc-nav-text">Recall</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#simt-warp"><span class="toc-nav-number">8.1.</span> <span class="toc-nav-text">SIMT &amp; Warp</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#warp"><span class="toc-nav-number">8.1.1.</span> <span class="toc-nav-text">Warp</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#optimization"><span class="toc-nav-number">8.2.</span> <span class="toc-nav-text">Optimization</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%87%8F%E5%B0%91%E5%85%A8%E5%B1%80%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="toc-nav-number">8.2.1.</span> <span class="toc-nav-text">减少全局内存访问</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#multithreading"><span class="toc-nav-number">8.2.2.</span> <span class="toc-nav-text">Multithreading</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#memory-coalescing"><span class="toc-nav-number">8.2.3.</span> <span class="toc-nav-text">Memory Coalescing</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#shared-memory"><span class="toc-nav-number">8.2.4.</span> <span class="toc-nav-text">Shared Memory</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#simt-efficiency"><span class="toc-nav-number">8.3.</span> <span class="toc-nav-text">SIMT Efficiency</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#divergency"><span class="toc-nav-number">8.3.1.</span> <span class="toc-nav-text">Divergency</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#atomic-operations"><span class="toc-nav-number">8.3.2.</span> <span class="toc-nav-text">Atomic Operations</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#image-histogram"><span class="toc-nav-number">8.3.3.</span> <span class="toc-nav-text">Image Histogram</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cpu%E5%92%8Cgpu%E7%9A%84%E9%80%9A%E4%BF%A1"><span class="toc-nav-number">8.4.</span> <span class="toc-nav-text">CPU和GPU的通信</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cuda-streams"><span class="toc-nav-number">8.5.</span> <span class="toc-nav-text">CUDA Streams</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#gpu-limitation"><span class="toc-nav-number">8.6.</span> <span class="toc-nav-text">GPU Limitation</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-9-cache"><span class="toc-nav-number">9.</span> <span class="toc-nav-text">Lecture 9 Cache</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#recall-1"><span class="toc-nav-number">9.1.</span> <span class="toc-nav-text">Recall</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#gpu-programming-model-cuda-programming-model"><span class="toc-nav-number">9.1.1.</span> <span class="toc-nav-text">GPU Programming
Model &amp; CUDA Programming Model</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#data-movement-computation"><span class="toc-nav-number">9.1.2.</span> <span class="toc-nav-text">Data Movement &amp; Computation</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#dram-1"><span class="toc-nav-number">9.1.3.</span> <span class="toc-nav-text">DRAM</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#memory-hierarchy-and-caches"><span class="toc-nav-number">9.2.</span> <span class="toc-nav-text">Memory Hierarchy and Caches</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#memory-hierarchy-example"><span class="toc-nav-number">9.2.1.</span> <span class="toc-nav-text">Memory Hierarchy Example</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#why-cache-works"><span class="toc-nav-number">9.2.2.</span> <span class="toc-nav-text">Why Cache Works？</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#caching-in-a-pipeline-design"><span class="toc-nav-number">9.3.</span> <span class="toc-nav-text">Caching in a pipeline Design</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#hierarchy-latency-analysis"><span class="toc-nav-number">9.4.</span> <span class="toc-nav-text">Hierarchy Latency Analysis</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache"><span class="toc-nav-number">9.5.</span> <span class="toc-nav-text">Cache</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#blocks"><span class="toc-nav-number">9.5.1.</span> <span class="toc-nav-text">Blocks</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#toy-example"><span class="toc-nav-number">9.5.2.</span> <span class="toc-nav-text">Toy Example</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#caching-basics"><span class="toc-nav-number">9.6.</span> <span class="toc-nav-text">Caching Basics</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#placement"><span class="toc-nav-number">9.6.1.</span> <span class="toc-nav-text">Placement</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-10-cache-coherence"><span class="toc-nav-number">10.</span> <span class="toc-nav-text">Lecture 10 Cache Coherence</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#basics"><span class="toc-nav-number">10.1.</span> <span class="toc-nav-text">Basics</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-block%E6%9B%BF%E6%8D%A2%E7%AD%96%E7%95%A5"><span class="toc-nav-number">10.2.</span> <span class="toc-nav-text">Cache Block替换策略</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#lru"><span class="toc-nav-number">10.2.1.</span> <span class="toc-nav-text">LRU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#plru"><span class="toc-nav-number">10.2.2.</span> <span class="toc-nav-text">PLRU</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#random"><span class="toc-nav-number">10.2.3.</span> <span class="toc-nav-text">Random</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#optimal-replacement-policy"><span class="toc-nav-number">10.2.4.</span> <span class="toc-nav-text">Optimal Replacement Policy</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#beladys-opt"><span class="toc-nav-number">10.2.4.1.</span> <span class="toc-nav-text">Belady‘s OPT</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#write-policy"><span class="toc-nav-number">10.3.</span> <span class="toc-nav-text">Write Policy</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cache-policieshandling-memory-write"><span class="toc-nav-number">10.3.1.</span> <span class="toc-nav-text">Cache Policies：Handling
Memory Write</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#instructions-vs-data-caches"><span class="toc-nav-number">10.4.</span> <span class="toc-nav-text">Instructions vs Data Caches</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-performance"><span class="toc-nav-number">10.5.</span> <span class="toc-nav-text">Cache Performance</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cache-size"><span class="toc-nav-number">10.5.1.</span> <span class="toc-nav-text">Cache Size：</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#block-size"><span class="toc-nav-number">10.5.2.</span> <span class="toc-nav-text">Block Size：</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#associativity"><span class="toc-nav-number">10.5.3.</span> <span class="toc-nav-text">Associativity</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-miss"><span class="toc-nav-number">10.6.</span> <span class="toc-nav-text">Cache Miss</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#compulsory-miss"><span class="toc-nav-number">10.6.1.</span> <span class="toc-nav-text">Compulsory miss</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#conflict-miss"><span class="toc-nav-number">10.6.2.</span> <span class="toc-nav-text">Conflict miss</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#capacity-miss"><span class="toc-nav-number">10.6.3.</span> <span class="toc-nav-text">Capacity miss</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-in-multi-core-cpu"><span class="toc-nav-number">10.7.</span> <span class="toc-nav-text">Cache in Multi-core CPU</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#recallmulti-core-over-large-superscalar"><span class="toc-nav-number">10.7.1.</span> <span class="toc-nav-text">Recall：Multi-Core over
Large Superscalar</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#caches-in-multi-core-cpu"><span class="toc-nav-number">10.8.</span> <span class="toc-nav-text">Caches in Multi-Core CPU</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#private-shared-caches"><span class="toc-nav-number">10.9.</span> <span class="toc-nav-text">Private &amp; Shared Caches</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#resource-sharing-concept-and-advantages"><span class="toc-nav-number">10.9.1.</span> <span class="toc-nav-text">Resource Sharing
Concept and Advantages</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-coherence-memory-consistency"><span class="toc-nav-number">10.10.</span> <span class="toc-nav-text">Cache Coherence &amp; Memory
Consistency</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-coherence"><span class="toc-nav-number">10.11.</span> <span class="toc-nav-text">Cache Coherence</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#hardware-architecture-for-cache-coherence"><span class="toc-nav-number">10.12.</span> <span class="toc-nav-text">Hardware Architecture
for Cache Coherence</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cache-coherence-protocols"><span class="toc-nav-number">10.12.1.</span> <span class="toc-nav-text">Cache Coherence Protocols</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#updating-policy"><span class="toc-nav-number">10.12.2.</span> <span class="toc-nav-text">Updating Policy</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#msi%E5%8D%8F%E8%AE%AEhttpszhuanlan.zhihu.comp417949142"><span class="toc-nav-number">10.12.3.</span> <span class="toc-nav-text">MSI协议【https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;417949142】</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mesi%E5%8D%8F%E8%AE%AE"><span class="toc-nav-number">10.12.4.</span> <span class="toc-nav-text">MESI协议</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cache-consistency"><span class="toc-nav-number">10.13.</span> <span class="toc-nav-text">Cache Consistency</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ordering-of-operations"><span class="toc-nav-number">10.13.1.</span> <span class="toc-nav-text">Ordering of Operations</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#single-processor%E7%9A%84memory-ordering"><span class="toc-nav-number">10.13.2.</span> <span class="toc-nav-text">Single Processor的Memory
Ordering</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mimd-processor%E7%9A%84memory-ordering"><span class="toc-nav-number">10.13.3.</span> <span class="toc-nav-text">MIMD Processor的Memory
Ordering</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#challenge"><span class="toc-nav-number">10.13.4.</span> <span class="toc-nav-text">Challenge</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#types-of-memory-barrier"><span class="toc-nav-number">10.13.5.</span> <span class="toc-nav-text">4 Types of Memory Barrier</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#sequential-consistency"><span class="toc-nav-number">10.13.6.</span> <span class="toc-nav-text">Sequential Consistency</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#total-store-order"><span class="toc-nav-number">10.13.7.</span> <span class="toc-nav-text">Total Store Order</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-11-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-nav-number">11.</span> <span class="toc-nav-text">Lecture 11 深度学习处理器</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81ai%E5%8A%A0%E9%80%9F%E5%99%A8"><span class="toc-nav-number">11.1.</span> <span class="toc-nav-text">为什么需要AI加速器</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%A4%84%E7%90%86%E5%99%A8%E6%80%A7%E8%83%BD%E6%8C%87%E6%A0%87"><span class="toc-nav-number">11.1.1.</span> <span class="toc-nav-text">处理器&amp;性能指标</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%8D%B7%E7%A7%AF%E5%B1%82%E8%AE%A1%E7%AE%97%E5%92%8C%E8%AE%BF%E5%AD%98%E7%89%B9%E6%80%A7"><span class="toc-nav-number">11.1.2.</span> <span class="toc-nav-text">卷积层计算和访存特性</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E8%AE%A1%E7%AE%97%E5%92%8C%E8%AE%BF%E5%AD%98%E7%89%B9%E6%80%A7"><span class="toc-nav-number">11.1.3.</span> <span class="toc-nav-text">激活函数计算和访存特性</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E8%AE%A1%E7%AE%97%E5%92%8C%E8%AE%BF%E5%AD%98%E7%89%B9%E6%80%A7"><span class="toc-nav-number">11.1.4.</span> <span class="toc-nav-text">全连接层计算和访存特性</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E5%99%A8%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="toc-nav-number">11.2.</span> <span class="toc-nav-text">深度学习加速器设计思路</span></a></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E8%B7%AF"><span class="toc-nav-number">11.3.</span> <span class="toc-nav-text">设计思路</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#global-buffer"><span class="toc-nav-number">11.3.1.</span> <span class="toc-nav-text">Global Buffer</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%AE%80%E5%8C%96%E6%8E%A7%E5%88%B6%E6%A8%A1%E5%9D%97"><span class="toc-nav-number">11.3.2.</span> <span class="toc-nav-text">简化控制模块</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E9%87%8F%E5%8C%96"><span class="toc-nav-number">11.3.3.</span> <span class="toc-nav-text">量化</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9D%97"><span class="toc-nav-number">11.3.4.</span> <span class="toc-nav-text">并行计算模块</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B8%93%E7%94%A8%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80"><span class="toc-nav-number">11.3.5.</span> <span class="toc-nav-text">专用编程语言</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-12-ai-processors"><span class="toc-nav-number">12.</span> <span class="toc-nav-text">Lecture 12 AI processors</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%8A%A0%E9%80%9F%E5%99%A8%E8%AE%BE%E8%AE%A1%E7%9B%AE%E6%A0%87"><span class="toc-nav-number">12.1.</span> <span class="toc-nav-text">深度学习加速器设计目标</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%8C%91%E6%88%98"><span class="toc-nav-number">12.1.1.</span> <span class="toc-nav-text">挑战</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%87%8F%E5%B0%91%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE"><span class="toc-nav-number">12.2.</span> <span class="toc-nav-text">减少内存访问</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81on-chip-buffer"><span class="toc-nav-number">12.2.1.</span> <span class="toc-nav-text">为什么需要On-chip Buffer</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#cache-or-buffer"><span class="toc-nav-number">12.2.2.</span> <span class="toc-nav-text">Cache or Buffer</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#programming-modelcache-vs-buffer"><span class="toc-nav-number">12.2.3.</span> <span class="toc-nav-text">Programming Model：Cache vs
Buffer</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#how-to-use-buffer"><span class="toc-nav-number">12.2.4.</span> <span class="toc-nav-text">How to use buffer</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%87%8F%E5%B0%91global-buffer%E8%AE%BF%E9%97%AE"><span class="toc-nav-number">12.3.</span> <span class="toc-nav-text">减少Global Buffer访问</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#weight-stationary"><span class="toc-nav-number">12.3.1.</span> <span class="toc-nav-text">Weight Stationary</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#output-stationary"><span class="toc-nav-number">12.3.2.</span> <span class="toc-nav-text">Output Stationary</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#input-stationary"><span class="toc-nav-number">12.3.3.</span> <span class="toc-nav-text">Input Stationary</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#row-stationary"><span class="toc-nav-number">12.3.4.</span> <span class="toc-nav-text">Row Stationary</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%A2%9E%E5%8A%A0%E8%AE%A1%E7%AE%97"><span class="toc-nav-number">12.4.</span> <span class="toc-nav-text">增加计算</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E5%8D%95%E5%85%83"><span class="toc-nav-number">12.4.1.</span> <span class="toc-nav-text">矩阵乘法单元</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E8%AE%A1%E7%AE%97%E6%A8%A1%E5%9D%97"><span class="toc-nav-number">12.4.2.</span> <span class="toc-nav-text">计算模块</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#%E5%B8%B8%E8%A7%81ai%E5%8A%A0%E9%80%9F%E5%99%A8%E5%88%86%E6%9E%90%E6%AF%94%E8%BE%83"><span class="toc-nav-number">12.5.</span> <span class="toc-nav-text">常见AI加速器分析比较</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E5%8D%8E%E4%B8%BAascend"><span class="toc-nav-number">12.5.1.</span> <span class="toc-nav-text">华为Ascend</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#ai-core%E5%86%85%E9%83%A8%E7%9A%84%E7%BB%93%E6%9E%84"><span class="toc-nav-number">12.5.1.1.</span> <span class="toc-nav-text">AI Core内部的结构</span></a></li><li class="toc-nav-item toc-nav-level-4"><a class="toc-nav-link" href="#cube%E6%A8%A1%E5%9D%97%E7%9F%A9%E9%98%B5%E8%BF%90%E7%AE%97%E7%AE%97%E5%8A%9B%E6%8B%85%E5%BD%93"><span class="toc-nav-number">12.5.1.2.</span> <span class="toc-nav-text">Cube模块（矩阵运算，算力担当）</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#vector%E6%A8%A1%E5%9D%97%E5%90%91%E9%87%8F%E8%BF%90%E7%AE%97%E5%A4%9A%E9%9D%A2%E6%89%8B"><span class="toc-nav-number">12.5.2.</span> <span class="toc-nav-text">Vector模块（向量运算，多面手）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#scalar%E6%A8%A1%E5%9D%97%E6%A0%87%E9%87%8F%E8%BF%90%E7%AE%97%E5%8F%B8%E4%BB%A4%E9%83%A8"><span class="toc-nav-number">12.5.3.</span> <span class="toc-nav-text">Scalar模块（标量运算，司令部）</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#mtebiu%E5%92%8C%E7%89%87%E4%B8%8A%E9%AB%98%E9%80%9F%E5%AD%98%E5%82%A8buffer"><span class="toc-nav-number">12.5.4.</span> <span class="toc-nav-text">MTE&#x2F;BIU和片上高速存储Buffer</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%8C%87%E4%BB%A4%E5%92%8C%E6%8E%A7%E5%88%B6%E7%B3%BB%E7%BB%9F"><span class="toc-nav-number">12.5.5.</span> <span class="toc-nav-text">指令和控制系统</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#google-tpu"><span class="toc-nav-number">12.6.</span> <span class="toc-nav-text">Google TPU</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#systolic-arrays"><span class="toc-nav-number">12.6.1.</span> <span class="toc-nav-text">Systolic Arrays</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#ai%E5%8A%A0%E9%80%9F%E5%99%A8%E4%B8%AD%E7%9A%84systolic-arrays"><span class="toc-nav-number">12.6.2.</span> <span class="toc-nav-text">AI加速器中的Systolic Arrays。</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#cambricon%E5%AF%92%E6%AD%A6%E7%BA%AA"><span class="toc-nav-number">12.7.</span> <span class="toc-nav-text">Cambricon寒武纪</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#overall-architecture-of-dlp-s"><span class="toc-nav-number">12.7.1.</span> <span class="toc-nav-text">Overall Architecture of
DLP-S</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="toc-nav-number">12.7.2.</span> <span class="toc-nav-text">数据流</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#dlp-isa"><span class="toc-nav-number">12.7.3.</span> <span class="toc-nav-text">DLP ISA</span></a></li></ol></li></ol></li><li class="toc-nav-item toc-nav-level-1"><a class="toc-nav-link" href="#lecture-13-parallel-training"><span class="toc-nav-number">13.</span> <span class="toc-nav-text">Lecture 13 Parallel Training</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#parallel-training"><span class="toc-nav-number">13.0.1.</span> <span class="toc-nav-text">Parallel Training</span></a></li></ol></li><li class="toc-nav-item toc-nav-level-2"><a class="toc-nav-link" href="#parallelism-taxonomy"><span class="toc-nav-number">13.1.</span> <span class="toc-nav-text">Parallelism Taxonomy</span></a><ol class="toc-nav-child"><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#data-parallel-training"><span class="toc-nav-number">13.1.1.</span> <span class="toc-nav-text">Data Parallel Training</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#allreduce-implementation-choices"><span class="toc-nav-number">13.1.2.</span> <span class="toc-nav-text">AllReduce Implementation
Choices</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#model-parallel-training"><span class="toc-nav-number">13.1.3.</span> <span class="toc-nav-text">Model Parallel Training</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#pipeline-parallel-training"><span class="toc-nav-number">13.1.4.</span> <span class="toc-nav-text">Pipeline Parallel Training</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#inter-layer-parallel"><span class="toc-nav-number">13.1.5.</span> <span class="toc-nav-text">Inter Layer Parallel</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#summary"><span class="toc-nav-number">13.1.6.</span> <span class="toc-nav-text">Summary</span></a></li><li class="toc-nav-item toc-nav-level-3"><a class="toc-nav-link" href="#others"><span class="toc-nav-number">13.1.7.</span> <span class="toc-nav-text">Others</span></a></li></ol></li></ol></li></ol>
        
        </div>
      </aside>
    



      <!-- Sidebar Container -->
      <div class="
                col-lg-8 col-lg-offset-1
                col-md-10 col-md-offset-1
                sidebar-container">

        <!-- Featured Tags -->
        
        <section>
          <!-- no hr -->
          <h5>
            <a href="/tags/">特色标签</a>
          </h5>
          <div class="tags">
            
            <a class="tag" href="/tags/#浙江大学" title="浙江大学">浙江大学</a>
            
            <a class="tag" href="/tags/#AI chip" title="AI chip">AI chip</a>
            
            <a class="tag" href="/tags/#AI芯片系统" title="AI芯片系统">AI芯片系统</a>
            
          </div>
        </section>
        

        <!-- Friends Blog -->
        
        <hr>
        <h5>链友</h5>
        <ul class="list-inline">

          
        </ul>
        
      </div>
    </div>
  </div>
</article>



<!-- anchorjs start -->
<!-- async load function -->
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script type="text/javascript">
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function(e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  };
</script>
<script type="text/javascript">
  //anchor-js, Doc:http://bryanbraun.github.io/anchorjs/
  async ("https://cdn.bootcss.com/anchor-js/1.1.1/anchor.min.js", function() {
    anchors.options = {
      visible: 'hover',
      placement: 'left',
      // icon: 'ℬ'
      icon: '❡'
    };
    anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
  });
</script>
<style>
  /* place left on bigger screen */
  @media all and (min-width: 800px) {
    .anchorjs-link {
      position: absolute;
      left: -0.75em;
      font-size: 1.1em;
      margin-top: -0.1em;
    }
  }
</style>

<!-- anchorjs end -->



		<!-- Footer (contains ThemeColor、viewer) -->
		<!-- Footer -->
<footer>
  <div class="container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <ul class="list-inline text-center">
          

          
            <li>
              <a target="_blank" href="https://github.com/Hualeez">
                <span class="fa-stack fa-lg">
                  <i class="fa fa-circle fa-stack-2x"></i>
                  <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                </span>
              </a>
            </li>
          

          

          

          

          

          

          

        </ul>
        <p class="copyright text-muted">
          Copyright &copy;
          Hualingz
          2023
          <br>
          Theme by
          <a target="_blank" rel="noopener" href="http://beantech.org">BeanTech</a>
          <span style="display: inline-block; margin: 0 5px;">
            <i class="fa fa-heart"></i>
          </span>
          re-Ported by
          <a target="_blank" rel="noopener" href="https://v-vincen.life/">Live My Life</a>
          |
          <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="91px" height="20px" src="https://ghbtns.com/github-btn.html?user=V-Vincen&repo=V-Vincen.github.io&type=star&count=true"></iframe>
        </p>
      </div>
    </div>
  </div>
</footer>

<a id="rocket" href="#top" class=""></a>


  <!-- jQuery -->
  <script type="text/javascript" src="/js/jquery.min.js"></script>
  <!-- Bootstrap Core JavaScript -->
  <script type="text/javascript" src="/js/bootstrap.min.js"></script>
  <!-- Custom Theme JavaScript -->
  <script type="text/javascript" src="/js/hux-blog.min.js"></script>
  <!-- catalog -->
  <script async="true" type="text/javascript" src="/js/catalog.js"></script>
  <!-- totop(rocket) -->
  <script async="true" type="text/javascript" src="/js/totop.js"></script>

  
    <!-- Busuanzi JavaScript -->
    <script async="async" src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  

  
    <!-- Scroll start -->
    <script async="async" type="text/javascript" src="/js/scroll.js"></script>
    <!-- Scroll end -->
  

  
    <!-- LangSelect start -->
    <script type="text/javascript" src="/js/langselect.js"></script>
    <!-- LangSelect end -->
  

  
    <!-- Mouseclick -->
    <script type="text/javascript" src="/js/mouseclick.js" content='The first step is as good as half over...,Laugh and grow fat...,Man proposes God disposes...,When all else is lost the future still remains...,Wasting time is robbing oneself...,Sharp tools make good work...,Cease to struggle and you cease to live...,A friend in need is a friend indeed...,Faith can move mountains...' color='#9933CC,#339933,#66CCCC,#FF99CC,#CCCCFF,#6666CC,#663399,#66CC99,#FF0033'></script>
  

  
    <!-- ribbon -->
    <script type="text/javascript" src="/js/ribbonDynamic.js"></script>
  

  






  <!-- viewer start -->
  <!-- viewer start (Picture preview) -->
  
    <script async="async" type="text/javascript" src="/js/viewer/viewer.min.js"></script>
    <script async="async" type="text/javascript" src="/js/viewer/pic-viewer.js"></script>
  

  <!-- viewer end -->


<script>
  // async load function
  function async (u, c) {
    var d = document,
      t = 'script',
      o = d.createElement(t),
      s = d.getElementsByTagName(t)[0];
    o.src = u;
    if (c) {
      o.addEventListener('load', function (e) {
        c(null, e);
      }, false);
    }
    s.parentNode.insertBefore(o, s);
  }

  // fastClick.js
  async ("https://cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function () {
    var $nav = document.querySelector("nav");
    if ($nav)
      FastClick.attach($nav);
    }
  )
</script>

<!-- Because of the native support for backtick-style fenced code blocks right within the Markdown is landed in Github Pages, From V1.6, There is no need for Highlight.js, so Huxblog drops it officially. -
https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0 - https://help.github.com/articles/creating-and-highlighting-code-blocks/ -->
<!-- <script> async ("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function () { hljs.initHighlightingOnLoad(); }) </script> <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet"> -->

<!-- jquery.tagcloud.js -->
<!-- <script> // only load tagcloud.js in tag.html if ($('#tag_cloud').length !== 0) { async ("http://Hualingz.cn/js/jquery.tagcloud.js", function () { $.fn.tagcloud.defaults = { // size: { start: 1, end: 1, unit: 'em' }, color: {
start: '#bbbbee', end: '#0085a1' } }; $('#tag_cloud a').tagcloud(); }) } </script> -->


		<!-- Search -->
		
		<div class="popup search-popup local-search-popup">
  <span class="popup-btn-close">
    ESC
  </span>
  <div class="container">
    <div class="row">
      <!-- <div class="col-md-9 col-md-offset-1"> -->
      <div class="col-lg-9 col-lg-offset-1 col-md-10 col-md-offset-1 local-search-content">

        <div class="local-search-header clearfix">

          <div class="local-search-input-wrapper">
            <span class="search-icon">
              <i class="fa fa-search fa-lg" style="margin: 25px 10px 25px 20px;"></i>
            </span>
            <input autocomplete="off" placeholder="搜索..." type="text" id="local-search-input">
          </div>
        </div>
        <div id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>


  
    <script src="/js/ziploader.js"></script>
  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.json";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    // monitor main search box;
    var onPopupClose = function (e) {
      $('.popup').fadeOut(300);
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $('.popup').fadeIn(300);
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }
    // get search zip version
    $.get('/searchVersion.json?t=' + (+new Date()), function (res) {
      if (localStorage.getItem('searchVersion') !== res) {
        localStorage.setItem('searchVersion', res);
        initSearchJson();
      }
    });

    function initSearchJson() {
      initLoad(['/search.flv'], {
        loadOptions: {
          success: function (obj) {
            localStorage.setItem('searchJson', obj['search.json'])
          },
          error: function (e) {
            return console.log(e)
          }
        },
        returnOptions: {
          'json': TYPE_TEXT
        },
        mimeOptions: {
          'json': 'application/json'
        }
      })
    }
    // search function;
    var searchFunc = function (search_id, content_id) {
      'use strict';
      isfetched = true;
      var datas = JSON.parse(localStorage.getItem('searchJson'));
      // console.log(search_id)
      var input = document.getElementById(search_id);
      var resultContent = document.getElementById(content_id);
      var inputEventFunction = function () {
        var searchText = input.value.trim().toLowerCase();
        var keywords = searchText.split(/[\s\-]+/);
        if (keywords.length > 1) {
          keywords.push(searchText);
        }
        var resultItems = [];
        if (searchText.length > 0) {
          // perform local searching
          datas.forEach(function (data) {
            var isMatch = false;
            var hitCount = 0;
            var searchTextCount = 0;
            var title = data.title
              ? data.title.trim()
              : '';
            var titleInLowerCase = title.toLowerCase();
            var content = data.content
              ? data.content.trim().replace(/<[^>]+>/g, "")
              : '';
            var contentInLowerCase = content.toLowerCase();
            var articleUrl = decodeURIComponent(data.url);

            var date = data.date;
            var dateTime = date.replace(/T/, " ").replace(/.000Z/, "");
            var imgUrl = data.header_img;
            


            var indexOfTitle = [];
            var indexOfContent = [];
            // only match articles with not empty titles
            keywords.forEach(function (keyword) {
              function getIndexByWord(word, text, caseSensitive) {
                var wordLen = word.length;
                if (wordLen === 0) {
                  return [];
                }
                var startPosition = 0,
                  position = [],
                  index = [];
                if (!caseSensitive) {
                  text = text.toLowerCase();
                  word = word.toLowerCase();
                }
                while ((position = text.indexOf(word, startPosition)) > -1) {
                  index.push({position: position, word: word});
                  startPosition = position + wordLen;
                }
                return index;
              }
              indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
              indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
            });
            if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
              isMatch = true;
              hitCount = indexOfTitle.length + indexOfContent.length;
            }
            // show search results
            if (isMatch) {
              // sort index by position of keyword
              [indexOfTitle, indexOfContent].forEach(function (index) {
                index.sort(function (itemLeft, itemRight) {
                  if (itemRight.position !== itemLeft.position) {
                    return itemRight.position - itemLeft.position;
                  } else {
                    return itemLeft.word.length - itemRight.word.length;
                  }
                });
              });
              // merge hits into slices
              function mergeIntoSlice(text, start, end, index) {
                var item = index[index.length - 1];
                var position = item.position;
                var word = item.word;
                var hits = [];
                var searchTextCountInSlice = 0;
                while (position + word.length <= end && index.length != 0) {
                  if (word === searchText) {
                    searchTextCountInSlice++;
                  }
                  hits.push({position: position, length: word.length});
                  var wordEnd = position + word.length;
                  // move to next position of hit
                  index.pop();
                  while (index.length != 0) {
                    item = index[index.length - 1];
                    position = item.position;
                    word = item.word;
                    if (wordEnd > position) {
                      index.pop();
                    } else {
                      break;
                    }
                  }
                }
                searchTextCount += searchTextCountInSlice;
                return {hits: hits, start: start, end: end, searchTextCount: searchTextCountInSlice};
              }
              var slicesOfTitle = [];
              if (indexOfTitle.length != 0) {
                slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
              }
              var slicesOfContent = [];
              while (indexOfContent.length != 0) {
                var item = indexOfContent[indexOfContent.length - 1];
                var position = item.position;
                var word = item.word;
                // cut out 100 characters
                var start = position - 20;
                var end = position + 80;
                if (start < 0) {
                  start = 0;
                }
                if (end < position + word.length) {
                  end = position + word.length;
                }
                if (end > content.length) {
                  end = content.length;
                }
                slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
              }
              // sort slices in content by search text's count and hits' count
              slicesOfContent.sort(function (sliceLeft, sliceRight) {
                if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                  return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                  return sliceRight.hits.length - sliceLeft.hits.length;
                } else {
                  return sliceLeft.start - sliceRight.start;
                }
              });
              // select top N slices in content
              var upperBound = parseInt('1');
              if (upperBound >= 0) {
                slicesOfContent = slicesOfContent.slice(0, upperBound);
              }
              // highlight title and content
              function highlightKeyword(text, slice) {
                var result = '';
                var prevEnd = slice.start;
                slice.hits.forEach(function (hit) {
                  result += text.substring(prevEnd, hit.position);
                  var end = hit.position + hit.length;
                  result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                  prevEnd = end;
                });
                result += text.substring(prevEnd, slice.end);
                return result;
              }
              var resultItem = '';

              // if (slicesOfTitle.length != 0) {   resultItem += "<li><a target='_blank' href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>"; } else {   resultItem += "<li><a target='_blank' href='" +
              // articleUrl + "' class='search-result-title'>" + title + "</a>"; } slicesOfContent.forEach(function (slice) {   resultItem += "<a target='_blank' href='" + articleUrl + "'><p class=\"search-result\">" + highlightKeyword(content, slice) +
              // "...</p></a>"; }); resultItem += "</li>";

              if (slicesOfTitle.length != 0) {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</div><time class='search-result-date'>" + dateTime + "</time>";
              } else {
                resultItem += "<a target='_blank' href='" + articleUrl + "' class='search-result'><div class='search-result-left'><div class='search-result-title'>" + title + "</div><time class='search-result-date'>" + dateTime + "</time>";
              }
              slicesOfContent.forEach(function (slice) {
                resultItem += "<p class=\"search-result-content\">" + highlightKeyword(content, slice) + "...</p>";
              });
              resultItem += "</div><div class='search-result-right'><img class='media-image' src='" + imgUrl + "' width='64px' height='48px'></img></div></a>";

              resultItems.push({item: resultItem, searchTextCount: searchTextCount, hitCount: hitCount, id: resultItems.length});
            }
          })
        };

        if (keywords.length === 1 && keywords[0] === "") {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else if (resultItems.length === 0) {
          resultContent.innerHTML = '<div id="no-result"></div>'
        } else {
          resultItems.sort(function (resultLeft, resultRight) {
            if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
              return resultRight.searchTextCount - resultLeft.searchTextCount;
            } else if (resultLeft.hitCount !== resultRight.hitCount) {
              return resultRight.hitCount - resultLeft.hitCount;
            } else {
              return resultRight.id - resultLeft.id;
            }
          });
          var searchResultList = '<div class=\"search-result-list\">';
          resultItems.forEach(function (result) {
            searchResultList += result.item;
          })
          searchResultList += "</div>";
          resultContent.innerHTML = searchResultList;
        }
      }
      if ('auto' === 'auto') {
        input.addEventListener('input', inputEventFunction);
      } else {
        $('.search-icon').click(inputEventFunction);
        input.addEventListener('keypress', function (event) {
          if (event.keyCode === 13) {
            inputEventFunction();
          }
        });
      }
      // remove loading animation
      $('body').css('overflow', '');
      proceedsearch();
    }
    // handle and trigger popup window;
    $('.popup-trigger').click(function (e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc('local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });
    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function (e) {
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 && $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });

    document.addEventListener('mouseup', (e) => {
      var _con = document.querySelector(".local-search-content");
      if (_con) {
        if (!_con.contains(e.target)) {
          onPopupClose();
        }
      }
    });
  </script>


		
	<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
